[
  {
    "objectID": "setup/index.html",
    "href": "setup/index.html",
    "title": "Setting up",
    "section": "",
    "text": "Please follow CASA0013 Set-up page to set up the CASA Computing Environment."
  },
  {
    "objectID": "setup/index.html#set-up",
    "href": "setup/index.html#set-up",
    "title": "Setting up",
    "section": "",
    "text": "Please follow CASA0013 Set-up page to set up the CASA Computing Environment."
  },
  {
    "objectID": "sessions/weekX_lecture.html#understanding-and-describing-data",
    "href": "sessions/weekX_lecture.html#understanding-and-describing-data",
    "title": "XXX",
    "section": "Understanding and describing data",
    "text": "Understanding and describing data\n\nQuantitative research is the process of collecting and analysing numerical data to describe, model, and predict variables of interest.\nGarbage in, garbage out.\n\n\nThis lecture focuses on understanding and describing data."
  },
  {
    "objectID": "sessions/weekX_lecture.html#learning-objectives",
    "href": "sessions/weekX_lecture.html#learning-objectives",
    "title": "XXX",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand basic data types;\nConsider how to summarise and represent data."
  },
  {
    "objectID": "sessions/week9.html",
    "href": "sessions/week9.html",
    "title": "Week 9",
    "section": "",
    "text": "This week will introduce how to simplify complex datasets by reducing the number of variables while preserving important information.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#introduction",
    "href": "sessions/week9.html#introduction",
    "title": "Week 9",
    "section": "",
    "text": "This week will introduce how to simplify complex datasets by reducing the number of variables while preserving important information.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#learning-objectives",
    "href": "sessions/week9.html#learning-objectives",
    "title": "Week 9",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the principle of dimensionality reduction.\nUnderstand the method of principle component analysis.\nVisualise and describe the results of PCA.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#lecture",
    "href": "sessions/week9.html#lecture",
    "title": "Week 9",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#quiz",
    "href": "sessions/week9.html#quiz",
    "title": "Week 9",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#practical",
    "href": "sessions/week9.html#practical",
    "title": "Week 9",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#further-resources",
    "href": "sessions/week9.html#further-resources",
    "title": "Week 9",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week8_lecture.html#understanding-and-describing-data",
    "href": "sessions/week8_lecture.html#understanding-and-describing-data",
    "title": "XXX",
    "section": "Understanding and describing data",
    "text": "Understanding and describing data\n\nQuantitative research is the process of collecting and analysing numerical data to describe, model, and predict variables of interest.\nGarbage in, garbage out.\n\n\nThis lecture focuses on understanding and describing data."
  },
  {
    "objectID": "sessions/week8_lecture.html#learning-objectives",
    "href": "sessions/week8_lecture.html#learning-objectives",
    "title": "XXX",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand basic data types;\nConsider how to summarise and represent data."
  },
  {
    "objectID": "sessions/week7_practical.html#preamble",
    "href": "sessions/week7_practical.html#preamble",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Preamble",
    "text": "Preamble\nWe’re going even deeper this week in Volume 2 of the Regression Sessions, so to help you along on your journey we have got Volume 2 of the Progression Sessions with Blame and DRS - Enjoy!"
  },
  {
    "objectID": "sessions/week7_practical.html#introduction",
    "href": "sessions/week7_practical.html#introduction",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Introduction",
    "text": "Introduction\nSimilar to last week’s practical, we will continue our investigation into the factors that affect school-level attainment figures, following the lecture you have just seen.\nLast week, you created a data subset for England of some 30-odd variables related to different measures of attainment, and a selection of continuous and categorical variables which might help explain those attainment levels. This week we will use more of those variables to build a multiple regression model and evaluate its outputs.\nBuilding a good regression model can be as much art as it is science! Back to our cake baking analogy last week - think of it a bit like the bake-off ‘technical challenge’ - same recipe, same ingredients, potentially some very different outcomes!\n\nIt takes a lot of practice, iteration and understanding of the various dimensions in your model to build a good regression model. It is much more than just a good R-squared and some ‘statistically significant’ coefficients!"
  },
  {
    "objectID": "sessions/week7_practical.html#aims",
    "href": "sessions/week7_practical.html#aims",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Aims",
    "text": "Aims\nBy the end of this week’s regression session you should:\n\nConsolidate your knowledge of using R or Python to process data in order to carry out a scientific investigation\nBuild on the skills learned last week to practice further plotting and visualising of data to assess relationships between multiple x variables and a selected y variable\nRefresh your knowledge of using built-in statistical software functions in R and Python to run some more sophisticated regression models and produce statistical outputs from those models\nPractice interpreting the outputs of those models thinking in particular about issues of confounding, mediating, multicollinearity and the independence of residuals\nPractice experimenting with interaction effects in your model and the interpretation of those outputs\n\n\n\n\n\n\n\nNote\n\n\n\nAs with last week’s practical you will find code-blocks that will allow you to run your analysis in either R or Python. Again, it’s up to you which you decide to use."
  },
  {
    "objectID": "sessions/week7_practical.html#tasks",
    "href": "sessions/week7_practical.html#tasks",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Tasks",
    "text": "Tasks\nThis week we won’t look at individual local authorities, but will focus on the whole of England.\n1. Baseline Model\n\nRun your baseline bivariate, whole of England, regression model from last week\n2. Data Prep and Exploratory Data Analysis\n\nPrepare your data and carry out some exploratory data analysis to understand what you are working with\n3. Multiple Regression Model\n\nExperiment with adding additional explanatory variables one-by-one into your model - both continuous and categorical. You might even experiment with reclassifying variables to reduce any noise that might exist with excessive categories unclear continuous relationships\nTry to find the model that best explains your attainment variable. One that strikes a good balance between:\n\nexplanatory power (a good \\(R^2\\), significant explanatory variables) - best doesn’t necessarily mean the highest \\(R^2\\), if a variable with more nuance allows you to say something more interesting about a relationship.\nparsimony (the principle of simplicity - fewest variables, simplest possible explanation)\n\n\n4. Evaluation\n\nWhen you have your ‘best’ model, how do you interpret the coefficients?\n\nWhich variable(s) has(have) the most explanatory power (check t-values for this)?\nHow do you interpret the combined explanatory power of variables in your model?\nWhat kind of confounding do you observe as you add more variables (if any)?\nDo you have any issues of multicollinearity or residual independence? Does your model pass the standard tests?\n\n\n5. Interaction Effects\n\nExperiment with interacting some of the variables in your best multiple regression model and see if this adds any more explanatory nuance to your main analysis"
  },
  {
    "objectID": "sessions/week7_practical.html#task-1---baseline-model",
    "href": "sessions/week7_practical.html#task-1---baseline-model",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Task 1 - Baseline Model",
    "text": "Task 1 - Baseline Model\n\nFirst read in your data file from last week and run your final baseline model from last week\nThen run a basic bivariate regression model for your attainment variable of choice (I am using raw Attainment 8 scores - ATT8SCR - , but last week you should have chosen one of the other different Attainment scores to try and model - therefore you will need to adjust your code accordingly)\nYour choice of independent variable can be anything, but you might want to start with PTFSM6CLA1A (% of pupils at the end of key stage 4 who are disadvantaged) as I am.\nDon’t forget, you probably will want to log-transform both of your variables\n\n\n\n\n\n\n\nNote\n\n\n\nThe paths in the code below are specific to my home computer - you’ll need to adapt this code to read the csv from where it is on your computer.\nAnd one more reminder - you will also need to change the variables to the ones you used last week - don’t just copy mine!\n\n\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\nimport numpy as np\nimport janitor\nfrom pathlib import Path\nimport statsmodels.api as sm\n\n# little function to define the file root on different machines\ndef find_qm_root(start_path: Path = Path.cwd(), anchor: str = \"QM\") -&gt; Path:\n    \"\"\"\n    Traverse up from the start_path until the anchor folder (e.g. 'QM' or 'QM_Fork')      is found. Returns the path to the anchor folder.\n    \"\"\"\n    for parent in [start_path] + list(start_path.parents):\n        if parent.name == anchor:\n            return parent\n    raise FileNotFoundError(f\"Anchor folder '{anchor}' not found in path      hierarchy.\")\n  \nqm_root = find_qm_root()\nbase_path = qm_root / \"sessions\" / \"L6_data\" / \"Performancetables_130242\" / \"2022-2023\"\nna_all = [\"\", \"NA\", \"SUPP\", \"NP\", \"NE\", \"SP\", \"SN\", \"LOWCOV\", \"NEW\", \"SUPPMAT\", \"NaN\"]\n\nengland_filtered = pd.read_csv(base_path / \"england_filtered.csv\", na_values=na_all, dtype={\"URN\": str})\n\n# Log-transform safely: replace non-positive values with NaN\nengland_filtered['log_ATT8SCR'] = np.where(england_filtered['ATT8SCR'] &gt; 0, np.log(england_filtered['ATT8SCR']), np.nan)\nengland_filtered['log_PTFSM6CLA1A'] = np.where(england_filtered['PTFSM6CLA1A'] &gt; 0, np.log(england_filtered['PTFSM6CLA1A']), np.nan)\n\n# Drop rows with NaNs in either column\nengland_filtered_clean = england_filtered.dropna(subset=['log_ATT8SCR', 'log_PTFSM6CLA1A'])\n\n# Define independent and dependent variables\nX = sm.add_constant(england_filtered_clean['log_PTFSM6CLA1A'])  # adds intercept\ny = england_filtered_clean['log_ATT8SCR']\n\n# Fit the model\nengland_model1 = sm.OLS(y, X).fit()\n#england_summary = extract_model_summary(england_model1, 'England Model')\n\n# Print summary\nprint(england_model1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.468\nModel:                            OLS   Adj. R-squared:                  0.468\nMethod:                 Least Squares   F-statistic:                     2611.\nDate:                Sat, 25 Oct 2025   Prob (F-statistic):               0.00\nTime:                        15:21:52   Log-Likelihood:                 1668.0\nNo. Observations:                2968   AIC:                            -3332.\nDf Residuals:                    2966   BIC:                            -3320.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               4.4778      0.013    349.542      0.000       4.453       4.503\nlog_PTFSM6CLA1A    -0.2071      0.004    -51.095      0.000      -0.215      -0.199\n==============================================================================\nOmnibus:                      105.763   Durbin-Watson:                   1.313\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              199.209\nSkew:                           0.267   Prob(JB):                     5.53e-44\nKurtosis:                       4.151   Cond. No.                         17.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nCodelibrary(tidyverse)\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(here)\n\nbase_path &lt;- here(\"sessions\", \"L6_data\", \"Performancetables_130242\", \"2022-2023\")\nna_all &lt;- c(\"\", \"NA\", \"SUPP\", \"NP\", \"NE\", \"SP\", \"SN\", \"LOWCOV\", \"NEW\", \"SUPPMAT\")\n\nengland_filtered &lt;- read_csv(file.path(base_path, \"england_filtered.csv\"), na = na_all) |&gt; mutate(URN = as.character(URN))\n\n#str(england_filtered)\n\nengland_filtered_clean &lt;- england_filtered[\n  !is.na(england_filtered$ATT8SCR) & \n  !is.na(england_filtered$PTFSM6CLA1A) &\n  england_filtered$ATT8SCR &gt; 0 &\n  england_filtered$PTFSM6CLA1A &gt; 0, \n]\n\n# Fit linear model and get predicted values\nengland_model1 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered_clean)\nsummary(england_model1)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65843 -0.08607 -0.00897  0.07800  0.55670 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.477823   0.012811  349.54   &lt;2e-16 ***\nlog(PTFSM6CLA1A) -0.207090   0.004053  -51.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.138 on 2966 degrees of freedom\nMultiple R-squared:  0.4681,    Adjusted R-squared:  0.468 \nF-statistic:  2611 on 1 and 2966 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "sessions/week7_practical.html#task-2---exploratory-analysis-and-data-preparation",
    "href": "sessions/week7_practical.html#task-2---exploratory-analysis-and-data-preparation",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Task 2 - Exploratory Analysis and Data Preparation",
    "text": "Task 2 - Exploratory Analysis and Data Preparation\n\nRight, we that was a nice and simple starter. We now have a baseline and something to compare your subsequent more sophisticated models to.\nThese next steps will be a little trickier and I will be expecting you to use some of the knowledge gained last week to complete the tasks, rather than me giving you all of the right code explicitly.\n\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Drop unwanted numeric columns\nnumeric_cols = england_filtered_clean.drop(columns=['easting', 'northing', 'LEA'], errors='ignore').select_dtypes(include='number')\n\n# Convert to long format\nnumeric_long = numeric_cols.melt(var_name='variable', value_name='value')\n\n# Set up the figure\nvariables = numeric_long['variable'].unique()\nn_vars = len(variables)\ncols = 6\nrows = (n_vars + cols - 1) // cols\n\nfig, axes = plt.subplots(rows, cols, figsize=(18, 3 * rows))\naxes = axes.flatten()\n\nfor i, var in enumerate(variables):\n    sns.histplot(data=numeric_long[numeric_long['variable'] == var], x='value', bins=30, color='steelblue', ax=axes[i])\n    axes[i].set_title(var)\n    axes[i].set_xlabel(\"Value\")\n    axes[i].set_ylabel(\"Count\")\n\n# Hide unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.suptitle(\"Histograms of Numerical Variables\", y=1.02)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Log-transform relevant variables\nengland_filtered_clean['log_ATT8SCR'] = np.log(england_filtered_clean['ATT8SCR'])\nengland_filtered_clean['log_PTFSM6CLA1A'] = np.log(england_filtered_clean['PTFSM6CLA1A'])\nengland_filtered_clean['log_PNUMEAL'] = np.log(england_filtered_clean['PNUMEAL'])\nengland_filtered_clean['log_PERCTOT'] = np.log(england_filtered_clean['PERCTOT'])\n\n# Prepare long format data\nlong_df = pd.melt(\n    england_filtered_clean,\n    id_vars=['log_ATT8SCR'],\n    value_vars=['log_PTFSM6CLA1A', 'log_PNUMEAL', 'log_PERCTOT', 'PTPRIORLO'],\n    var_name='predictor',\n    value_name='x_value'\n)\n\n# Custom axis labels\naxis_labels = {\n    'log_PTFSM6CLA1A': 'log(PTFSM6CLA1A)',\n    'log_PNUMEAL': 'log(PNUMEAL)',\n    'log_PERCTOT': 'log(PERCTOT)',\n    'PTPRIORLO': 'PTPRIORLO'\n}\n\n# Set up the figure manually\npredictors = long_df['predictor'].unique()\ncols = 2\nrows = (len(predictors) + cols - 1) // cols\nfig, axes = plt.subplots(rows, cols, figsize=(12, 4 * rows))\naxes = axes.flatten()\n\nfor i, predictor in enumerate(predictors):\n    subset = long_df[long_df['predictor'] == predictor]\n    sns.regplot(\n        data=subset,\n        x='x_value',\n        y='log_ATT8SCR',\n        scatter_kws={'alpha': 0.5, 'color': 'steelblue'},\n        line_kws={'color': 'black'},\n        ax=axes[i]\n    )\n    axes[i].set_title(f\"log(ATT8SCR) vs {axis_labels[predictor]}\")\n    axes[i].set_xlabel(axis_labels[predictor])\n    axes[i].set_ylabel(\"log(ATT8SCR)\")\n\n# Hide unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.suptitle(\"Scatter Plots of log(ATT8SCR) vs Predictors\", y=1.02)\nsns.despine()\nplt.show()\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Select categorical columns and drop unwanted ones\ncategorical_cols = england_filtered_clean.select_dtypes(include='object').drop(\n    columns=['URN', 'SCHNAME.x', 'LANAME', 'TOWN.x', 'SCHOOLTYPE.x'], errors='ignore'\n)\n\n# Determine layout\nn_cols = 2\nn_rows = (len(categorical_cols.columns) + n_cols - 1) // n_cols\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\naxes = axes.flatten()\n\n# Generate bar plots\nfor i, colname in enumerate(categorical_cols.columns):\n    sns.countplot(data=categorical_cols, x=colname, ax=axes[i], color='darkorange')\n    axes[i].set_title(f\"Distribution of {colname}\")\n    axes[i].set_xlabel(colname)\n    axes[i].set_ylabel(\"Count\")\n    axes[i].tick_params(axis='x', rotation=45)\n\n# Hide unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCodelibrary(tidyverse)\n\n# Select only numeric columns\nnumeric_cols &lt;- england_filtered_clean %&gt;% \n  select(where(is.numeric)) %&gt;% \n  select(-easting, -northing, -LEA)\n\n# Convert to long format for faceting\nnumeric_long &lt;- numeric_cols %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\")\n\n# Plot faceted histograms\nggplot(numeric_long, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  facet_wrap(~ variable, scales = \"free\", ncol = 6) +\n  theme_minimal() +\n  labs(title = \"Histograms of Numerical Variables\")\n\n\n\n\n\n\n\n\nCodelibrary(tidyverse)\n\n# Prepare the data\nscatter_data &lt;- england_filtered_clean %&gt;%\n  select(ATT8SCR, PTFSM6CLA1A, PNUMEAL, PERCTOT, PTPRIORLO) %&gt;%\n  mutate(\n    log_ATT8SCR = log(ATT8SCR),\n    log_PTFSM6CLA1A = log(PTFSM6CLA1A),\n    log_PNUMEAL = log(PNUMEAL),\n    log_PERCTOT = log(PERCTOT)\n  ) %&gt;%\n  pivot_longer(\n    cols = c(log_PTFSM6CLA1A, log_PNUMEAL, log_PERCTOT, PTPRIORLO),\n    names_to = \"predictor\",\n    values_to = \"x_value\"\n  )\n\n# Create faceted scatter plots\nggplot(scatter_data, aes(x = x_value, y = log_ATT8SCR)) +\n  geom_point(alpha = 0.5, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"black\") +\n  facet_wrap(~ predictor, scales = \"free_x\") +\n  theme_minimal() +\n  labs(\n    title = \"Scatter Plots of log(ATT8SCR) vs Predictors\",\n    x = \"Predictor (log-transformed where applicable)\",\n    y = \"log(ATT8SCR)\"\n  )\n\n\n\n\n\n\n\n\nCodelibrary(tidyverse)\nlibrary(cowplot)\n\n# Drop unwanted columns\ncategorical_cols &lt;- england_filtered_clean %&gt;%\n  select(where(is.character)) %&gt;%\n  select(-URN, -SCHNAME.x, -LANAME, -TOWN.x, -SCHOOLTYPE.x)\n\n# Create a list to store plots (unnamed)\nplot_list &lt;- list()\n\n# Loop through each categorical column and generate a bar plot\nfor (colname in names(categorical_cols)) {\n  p &lt;- ggplot(categorical_cols, aes_string(x = colname)) +\n    geom_bar(fill = \"darkorange\") +\n    theme_minimal() +\n    labs(title = paste(\"Distribution of\", colname), x = colname, y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  plot_list &lt;- append(plot_list, list(p))  # Append without naming\n}\n\n# Combine all plots into a single figure\ncombined_plot &lt;- cowplot::plot_grid(plotlist = plot_list, ncol = 2)\nprint(combined_plot)\n\n\n\n\n\n\n\n\n\n\nChanging your dummy reference category\nYour dummy variable reference category is the category within the variable that all other categories will be compared against in your model.\nWhile the reference category has no effect on the model itself, it does make a difference for how you interpret your model.\nFor example, if you are using Regions in England as a dummy, setting your reference region as London will mean all other regions are compared to it and they might naturally be lower or higher.\nA good strategy is to select the most or least numerous or important category in your variable, rather than something in the middle. Of course, you may not know which is most important until you run your model, so you may need to go back and reset the reference variable and run the model again.\nHere is some code in R and Python to help you carry out the setting of the reference level (Yes, it’s more straightforward in R!)\n\n\n\n\n\n\n\n\n\n\n\nCode## In R, you can change the reference category using the relevel() function\n\nengland_filtered_clean$gor_name &lt;- relevel(factor(england_filtered_clean$gor_name), ref = \"South East\")\nengland_filtered_clean$ofsted_rating_name &lt;- relevel(factor(england_filtered_clean$OFSTEDRATING), ref = \"Good\")\nengland_filtered_clean$ADMPOL_PT &lt;- relevel(factor(england_filtered_clean$ADMPOL_PT), ref = \"OTHER NON SEL\")\n\n\n\n\n\nReclassifying a variable into fewer categories\nAs mentioned in the lecture, it can sometimes be useful to reclassify a variable into fewer categories to see if a signal appears.\nLet’s have a look at the religious character of a school variable and test it out as dummy in a basic model:\n\nNote the Python code is far more complicated than the R code, but should prodice similar outputs\n\n\n\n\n\n\n\n\n\nCodeimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Drop rows with missing values in relevant columns\nmodel_df = england_filtered_clean[['ATT8SCR', 'PTFSM6CLA1A', 'RELCHAR']].dropna().copy()\n\n# Log-transform numeric variables\nmodel_df['log_ATT8SCR'] = np.log(model_df['ATT8SCR'].astype(float))\nmodel_df['log_PTFSM6CLA1A'] = np.log(model_df['PTFSM6CLA1A'].astype(float))\n\n# Ensure RELCHAR is treated as categorical\nmodel_df['RELCHAR'] = model_df['RELCHAR'].astype('category')\n\n# Create design matrix with dummy variables for RELCHAR\nX = pd.get_dummies(model_df[['log_PTFSM6CLA1A', 'RELCHAR']], drop_first=True)\n\n# Add constant\nX = sm.add_constant(X)\n\n# Response variable\ny = model_df['log_ATT8SCR']\n\n# Ensure all columns are float64 to avoid dtype issues\nX = X.astype('float64')\ny = y.astype('float64')\n\n# Fit linear model using Pandas DataFrame directly\nmodel = sm.OLS(y, X).fit()\n\n# Display summary with proper variable names\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.429\nModel:                            OLS   Adj. R-squared:                  0.424\nMethod:                 Least Squares   F-statistic:                     99.29\nDate:                Sat, 25 Oct 2025   Prob (F-statistic):          3.44e-213\nTime:                        15:22:06   Log-Likelihood:                 1138.8\nNo. Observations:                1868   AIC:                            -2248.\nDf Residuals:                    1853   BIC:                            -2165.\nDf Model:                          14                                         \nCovariance Type:            nonrobust                                         \n============================================================================================================\n                                               coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------------------\nconst                                        4.2730      0.134     31.939      0.000       4.011       4.535\nlog_PTFSM6CLA1A                             -0.1860      0.005    -34.224      0.000      -0.197      -0.175\nRELCHAR_Catholic                             0.1769      0.153      1.159      0.246      -0.122       0.476\nRELCHAR_Christian                            0.2358      0.134      1.763      0.078      -0.027       0.498\nRELCHAR_Church of England                    0.1498      0.133      1.130      0.259      -0.110       0.410\nRELCHAR_Church of England/Christian          0.1546      0.187      0.827      0.408      -0.212       0.521\nRELCHAR_Church of England/Roman Catholic     0.0989      0.141      0.700      0.484      -0.178       0.376\nRELCHAR_Does not apply                       0.1255      0.132      0.950      0.342      -0.134       0.385\nRELCHAR_Greek Orthodox                       0.1586      0.187      0.849      0.396      -0.208       0.525\nRELCHAR_Hindu                                0.2145      0.187      1.146      0.252      -0.152       0.581\nRELCHAR_Jewish                               0.1289      0.138      0.935      0.350      -0.141       0.399\nRELCHAR_Muslim                               0.4221      0.136      3.106      0.002       0.156       0.689\nRELCHAR_Roman Catholic                       0.1755      0.132      1.325      0.185      -0.084       0.435\nRELCHAR_Roman Catholic/Church of England     0.0789      0.145      0.546      0.585      -0.205       0.363\nRELCHAR_Sikh                                 0.1980      0.162      1.223      0.221      -0.119       0.515\n==============================================================================\nOmnibus:                       54.961   Durbin-Watson:                   1.338\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              135.365\nSkew:                           0.041   Prob(JB):                     4.04e-30\nKurtosis:                       4.316   Cond. No.                         561.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nCode# Fit linear model and get predicted values\nengland_model2 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + RELCHAR, data = england_filtered_clean)\nsummary(england_model2)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + RELCHAR, data = england_filtered_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.64638 -0.08524 -0.00935  0.07833  0.55537 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                              4.353703   0.136250  31.954  &lt; 2e-16\nlog(PTFSM6CLA1A)                        -0.206428   0.004055 -50.910  &lt; 2e-16\nRELCHARCatholic                          0.157465   0.156284   1.008  0.31375\nRELCHARChristian                         0.218783   0.136987   1.597  0.11035\nRELCHARChurch of England                 0.131462   0.135766   0.968  0.33298\nRELCHARChurch of England/Christian       0.122838   0.191454   0.642  0.52118\nRELCHARChurch of England/Roman Catholic  0.080702   0.144692   0.558  0.57706\nRELCHARDoes not apply                    0.109962   0.135391   0.812  0.41675\nRELCHARGreek Orthodox                    0.133210   0.191417   0.696  0.48654\nRELCHARHindu                             0.173507   0.191523   0.906  0.36504\nRELCHARJewish                            0.089840   0.141044   0.637  0.52420\nRELCHARMuslim                            0.410154   0.139248   2.945  0.00325\nRELCHARNone                              0.120208   0.135422   0.888  0.37480\nRELCHARRoman Catholic                    0.156817   0.135599   1.156  0.24758\nRELCHARRoman Catholic/Church of England  0.065413   0.148244   0.441  0.65906\nRELCHARSikh                              0.177106   0.165766   1.068  0.28542\n                                           \n(Intercept)                             ***\nlog(PTFSM6CLA1A)                        ***\nRELCHARCatholic                            \nRELCHARChristian                           \nRELCHARChurch of England                   \nRELCHARChurch of England/Christian         \nRELCHARChurch of England/Roman Catholic    \nRELCHARDoes not apply                      \nRELCHARGreek Orthodox                      \nRELCHARHindu                               \nRELCHARJewish                              \nRELCHARMuslim                           ** \nRELCHARNone                                \nRELCHARRoman Catholic                      \nRELCHARRoman Catholic/Church of England    \nRELCHARSikh                                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1353 on 2935 degrees of freedom\n  (17 observations deleted due to missingness)\nMultiple R-squared:  0.4904,    Adjusted R-squared:  0.4878 \nF-statistic: 188.3 on 15 and 2935 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nYou will notice that the religious character variables nearly all insignificant. As such, lets try collapsing into three groups - “None”, “Christian” and “Non-Christian”\n\n\n\n\n\n\n\n\nCode# Define mapping function\ndef classify_relchar(value):\n    if pd.isna(value):\n        return \"None\"\n    elif value in [\"Does not apply\", \"None\"]:\n        return \"None\"\n    elif value in [\n        \"Roman Catholic\", \"Greek Orthodox\", \"Church of England\", \"Catholic\", \"Christian\",\n        \"Church of England/Roman Catholic\", \"Roman Catholic/Church of England\",\n        \"Church of England/Christian\", \"Anglican/Church of England\"\n    ]:\n        return \"Christian\"\n    else:\n        return \"Non-Christian\"\n\n# Apply classification\nengland_filtered_clean['RELCHAR_Grp'] = england_filtered_clean['RELCHAR'].apply(classify_relchar)\n\n\n\n\n\nCodeengland_filtered_clean &lt;- england_filtered_clean %&gt;%\n  mutate(RELCHAR_Grp = case_when(\n    RELCHAR %in% c(\"Does not apply\", \"None\") ~ \"None\",\n    RELCHAR %in% c(\n      \"Roman Catholic\", \"Greek Orthodox\", \"Church of England\", \"Catholic\", \"Christian\",\n      \"Church of England/Roman Catholic\", \"Roman Catholic/Church of England\",\n      \"Church of England/Christian\", \"Anglican/Church of England\"\n    ) ~ \"Christian\",\n    TRUE ~ \"Non-Christian\"\n  ))\n\n\n\n\n\nNow re-run your model\n\n\n\n\n\n\n\n\nCodeimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Convert RELCHAR_Grp to a categorical variable with \"None\" as the reference level\nengland_filtered_clean['RELCHAR_Grp'] = pd.Categorical(\n    england_filtered_clean['RELCHAR_Grp'],\n    categories=[\"None\", \"Christian\", \"Non-Christian\"],\n    ordered=False\n)\n\n# Drop rows with missing values in relevant columns\nmodel_df = england_filtered_clean[['ATT8SCR', 'PTFSM6CLA1A', 'RELCHAR_Grp']].dropna().copy()\n\n# Log-transform numeric variables\nmodel_df['log_ATT8SCR'] = np.log(model_df['ATT8SCR'].astype(float))\nmodel_df['log_PTFSM6CLA1A'] = np.log(model_df['PTFSM6CLA1A'].astype(float))\n\n# Ensure RELCHAR is treated as categorical\nmodel_df['RELCHAR_Grp'] = model_df['RELCHAR_Grp'].astype('category')\n\n# Create design matrix with dummy variables for RELCHAR\nX = pd.get_dummies(model_df[['log_PTFSM6CLA1A', 'RELCHAR_Grp']], drop_first=True)\n\n# Add constant\nX = sm.add_constant(X)\n\n# Response variable\ny = model_df['log_ATT8SCR']\n\n# Ensure all columns are float64 to avoid dtype issues\nX = X.astype('float64')\ny = y.astype('float64')\n\n# Fit linear model using Pandas DataFrame directly\nmodel = sm.OLS(y, X).fit()\n\n# Display summary with proper variable names\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.480\nModel:                            OLS   Adj. R-squared:                  0.479\nMethod:                 Least Squares   F-statistic:                     912.1\nDate:                Sat, 25 Oct 2025   Prob (F-statistic):               0.00\nTime:                        15:22:07   Log-Likelihood:                 1701.5\nNo. Observations:                2968   AIC:                            -3395.\nDf Residuals:                    2964   BIC:                            -3371.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         4.4636      0.013    348.686      0.000       4.438       4.489\nlog_PTFSM6CLA1A              -0.2051      0.004    -51.053      0.000      -0.213      -0.197\nRELCHAR_Grp_Christian         0.0371      0.007      5.539      0.000       0.024       0.050\nRELCHAR_Grp_Non-Christian     0.1544      0.024      6.347      0.000       0.107       0.202\n==============================================================================\nOmnibus:                       91.156   Durbin-Watson:                   1.356\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              175.557\nSkew:                           0.219   Prob(JB):                     7.55e-39\nKurtosis:                       4.108   Cond. No.                         32.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nCode# Fit linear model and get predicted values\n# Ensure RELCHAR_Grp is a factor with \"None\" as the reference level\nengland_filtered_clean &lt;- england_filtered_clean %&gt;%\n  mutate(RELCHAR_Grp = factor(RELCHAR_Grp, levels = c(\"None\", \"Christian\", \"Non-Christian\")))\n\nengland_model2 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + RELCHAR_Grp, data = england_filtered_clean)\nsummary(england_model2)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + RELCHAR_Grp, data = england_filtered_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65098 -0.08639 -0.00989  0.07850  0.56101 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.467675   0.012825 348.354  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)         -0.206242   0.004027 -51.213  &lt; 2e-16 ***\nRELCHAR_GrpChristian      0.036648   0.006730   5.445 5.59e-08 ***\nRELCHAR_GrpNon-Christian  0.080653   0.019773   4.079 4.64e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.137 on 2964 degrees of freedom\nMultiple R-squared:  0.4759,    Adjusted R-squared:  0.4754 \nF-statistic: 897.1 on 3 and 2964 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nHas this made a difference? What’s happened to the significance of the religious character variable?\n\n\n\n\n\n\nTip\n\n\n\nYou can use this method to reclassify any different variable.\nThink about how you might reclassify a continuous variable. You might think about converting something like ‘total number of pupils on roll’ into ‘small’, ‘medium’ and ‘large’ schools, for example, based on certain thresholds. How might you choose these thresholds? If you are struggling to think of how you might code this kind of reclassification up, this is exactly where AI can be very helpful in assisting you - although while it might be good at the code to do it, I can guarantee it will likely be pretty bad at deciding on useful breaks in your data, so this is where you might need to intervene."
  },
  {
    "objectID": "sessions/week7_practical.html#task-3---building-your-optimum-multiple-regression-model",
    "href": "sessions/week7_practical.html#task-3---building-your-optimum-multiple-regression-model",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Task 3 - Building your optimum multiple regression model",
    "text": "Task 3 - Building your optimum multiple regression model\nRecipe Steps\n\nUsing the steps you learned last week and the information from this week’s lecture, I would like you to find the best possible 7-dependent variable model for your chosen attainment variable (without interaction terms). These can be continuous or categorical variables or any combination of them. Remember:\n\nUse exploratory analysis to check the distributions of your variables using histograms, box plots etc. or binary scatter plots with your dependent variable, before putting them into your model (you’ve already done some of this, but you may need to do some more)\nyou might run into issues with logging some variables that have real 0s in them - this might cause your model to break. You might need to filter these variables out of your dataset before running your model. Some of the code above will help with this - if you get stuck, ask a friendly AI for help\nwith your dummy variables, you might want to experiment with changing your reference category\nyou might also want to reclassify a variable if you suspect it is important, but that in its present form is coming out as insignificant\ncheck you regression assumptions - linearity, homoscedasticity, normality of residuals, multicollinearity, independence of residuals - does your model pass?\nwhich are the most important variables in your model in terms of t-values?\n\n\nYou should try and build your model step by step, a variable at a time. Each time you run the model, check what is happening to the coefficients\n\nWhat do you notice about confounding or mediation as you go?\nDo any of your variables become insignificant? For example, what happens to religious character groups in the presence of regions, for example? If a variable becomes insignificant, you might we wise to drop it from the analysis (but maybe not until the end in case another variable makes it significant again)\n\n\n\nON YOUR MARKS, GET SET - GO!!!\nWhen you think you have your best model, move on to Task 4 below\n\n\n\n\n\n\n\n\n\n\n\nCodelibrary(performance)\nlibrary(jtools)\n\nengland_filtered_clean &lt;- england_filtered_clean %&gt;%\n  filter(PTFSM6CLA1A &gt; 0, PERCTOT &gt; 0, PNUMEAL &gt; 0)\n\nengland_model3 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + RELCHAR_Grp, data = england_filtered_clean)\nsummary(england_model3)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + \n    RELCHAR_Grp, data = england_filtered_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37259 -0.06589 -0.00213  0.06103  0.76031 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               5.058526   0.015456 327.295   &lt;2e-16 ***\nlog(PTFSM6CLA1A)         -0.112548   0.003571 -31.516   &lt;2e-16 ***\nlog(PERCTOT)             -0.400243   0.008238 -48.585   &lt;2e-16 ***\nRELCHAR_GrpChristian     -0.004545   0.005082  -0.894    0.371    \nRELCHAR_GrpNon-Christian -0.013948   0.014850  -0.939    0.348    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.102 on 2956 degrees of freedom\nMultiple R-squared:  0.7092,    Adjusted R-squared:  0.7088 \nF-statistic:  1802 on 4 and 2956 DF,  p-value: &lt; 2.2e-16\n\nCodeengland_model4 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + gor_name, data = england_filtered_clean, na.action = na.exclude)\nsummary(england_model4)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + \n    gor_name, data = england_filtered_clean, na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39840 -0.06023 -0.00054  0.05878  0.71757 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       5.0107282  0.0153085 327.317  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)                 -0.1318266  0.0037583 -35.076  &lt; 2e-16 ***\nlog(PERCTOT)                     -0.3615186  0.0083599 -43.244  &lt; 2e-16 ***\ngor_nameEast Midlands            -0.0101682  0.0076764  -1.325 0.185402    \ngor_nameEast of England          -0.0032498  0.0070509  -0.461 0.644903    \ngor_nameLondon                    0.0885951  0.0071258  12.433  &lt; 2e-16 ***\ngor_nameNorth East                0.0716603  0.0097712   7.334 2.88e-13 ***\ngor_nameNorth West               -0.0002658  0.0068006  -0.039 0.968826    \ngor_nameSouth West                0.0206193  0.0073599   2.802 0.005118 ** \ngor_nameWest Midlands             0.0216499  0.0071195   3.041 0.002379 ** \ngor_nameYorkshire and the Humber  0.0272177  0.0074654   3.646 0.000271 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09737 on 2950 degrees of freedom\nMultiple R-squared:  0.7357,    Adjusted R-squared:  0.7348 \nF-statistic: 821.1 on 10 and 2950 DF,  p-value: &lt; 2.2e-16\n\nCodeengland_model5 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + log(PNUMEAL) + gor_name, data = england_filtered_clean, na.action = na.exclude)\nsummary(england_model5)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + \n    log(PNUMEAL) + gor_name, data = england_filtered_clean, na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40707 -0.05726  0.00153  0.05906  0.66397 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       4.9559677  0.0158886 311.920  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)                 -0.1463138  0.0039338 -37.194  &lt; 2e-16 ***\nlog(PERCTOT)                     -0.3397722  0.0084583 -40.170  &lt; 2e-16 ***\nlog(PNUMEAL)                      0.0206186  0.0019433  10.610  &lt; 2e-16 ***\ngor_nameEast Midlands            -0.0004914  0.0075902  -0.065 0.948380    \ngor_nameEast of England           0.0007515  0.0069315   0.108 0.913676    \ngor_nameLondon                    0.0745376  0.0071191  10.470  &lt; 2e-16 ***\ngor_nameNorth East                0.0963456  0.0098697   9.762  &lt; 2e-16 ***\ngor_nameNorth West                0.0098559  0.0067434   1.462 0.143966    \ngor_nameSouth West                0.0343067  0.0073388   4.675 3.08e-06 ***\ngor_nameWest Midlands             0.0256262  0.0069986   3.662 0.000255 ***\ngor_nameYorkshire and the Humber  0.0359445  0.0073741   4.874 1.15e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09558 on 2949 degrees of freedom\nMultiple R-squared:  0.7454,    Adjusted R-squared:  0.7445 \nF-statistic: 784.9 on 11 and 2949 DF,  p-value: &lt; 2.2e-16\n\nCodeengland_model6 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + log(PNUMEAL) + OFSTEDRATING + gor_name, data = england_filtered_clean, na.action = na.exclude)\nsummary(england_model6)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + \n    log(PNUMEAL) + OFSTEDRATING + gor_name, data = england_filtered_clean, \n    na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39799 -0.05644  0.00150  0.05800  0.54719 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       4.841810   0.017103 283.105  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)                 -0.140453   0.003814 -36.829  &lt; 2e-16 ***\nlog(PERCTOT)                     -0.293625   0.008762 -33.510  &lt; 2e-16 ***\nlog(PNUMEAL)                      0.017114   0.001899   9.011  &lt; 2e-16 ***\nOFSTEDRATINGOutstanding           0.067858   0.005310  12.779  &lt; 2e-16 ***\nOFSTEDRATINGRequires improvement -0.051879   0.005492  -9.445  &lt; 2e-16 ***\nOFSTEDRATINGSerious Weaknesses   -0.039225   0.019715  -1.990   0.0467 *  \nOFSTEDRATINGSpecial Measures     -0.090715   0.023054  -3.935 8.52e-05 ***\ngor_nameEast Midlands             0.004759   0.007336   0.649   0.5166    \ngor_nameEast of England           0.001422   0.006674   0.213   0.8313    \ngor_nameLondon                    0.071049   0.006851  10.371  &lt; 2e-16 ***\ngor_nameNorth East                0.085795   0.009534   8.999  &lt; 2e-16 ***\ngor_nameNorth West                0.016287   0.006545   2.489   0.0129 *  \ngor_nameSouth West                0.033397   0.007066   4.727 2.39e-06 ***\ngor_nameWest Midlands             0.027829   0.006750   4.123 3.85e-05 ***\ngor_nameYorkshire and the Humber  0.033224   0.007109   4.673 3.10e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09112 on 2886 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.7694,    Adjusted R-squared:  0.7682 \nF-statistic: 642.1 on 15 and 2886 DF,  p-value: &lt; 2.2e-16\n\nCodeengland_model7 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + log(PNUMEAL) + OFSTEDRATING + gor_name + PTPRIORLO + ADMPOL_PT, data = england_filtered_clean, na.action = na.exclude)\nsummary(england_model7)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + \n    log(PNUMEAL) + OFSTEDRATING + gor_name + PTPRIORLO + ADMPOL_PT, \n    data = england_filtered_clean, na.action = na.exclude)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37723 -0.04528  0.00333  0.04912  0.30081 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          4.6296089  0.0154652 299.356  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)                    -0.0699264  0.0036900 -18.950  &lt; 2e-16 ***\nlog(PERCTOT)                        -0.2244960  0.0074455 -30.152  &lt; 2e-16 ***\nlog(PNUMEAL)                         0.0179676  0.0016174  11.109  &lt; 2e-16 ***\nOFSTEDRATINGOutstanding              0.0551690  0.0043794  12.597  &lt; 2e-16 ***\nOFSTEDRATINGRequires improvement    -0.0516629  0.0045211 -11.427  &lt; 2e-16 ***\nOFSTEDRATINGSerious Weaknesses      -0.0602663  0.0162207  -3.715 0.000207 ***\nOFSTEDRATINGSpecial Measures        -0.0951661  0.0189679  -5.017 5.56e-07 ***\ngor_nameEast Midlands                0.0128394  0.0060587   2.119 0.034161 *  \ngor_nameEast of England              0.0185393  0.0056555   3.278 0.001058 ** \ngor_nameLondon                       0.0395373  0.0058383   6.772 1.53e-11 ***\ngor_nameNorth East                   0.0331207  0.0080787   4.100 4.25e-05 ***\ngor_nameNorth West                  -0.0010765  0.0054833  -0.196 0.844374    \ngor_nameSouth West                   0.0382890  0.0059572   6.427 1.51e-10 ***\ngor_nameWest Midlands                0.0187024  0.0057100   3.275 0.001068 ** \ngor_nameYorkshire and the Humber     0.0328390  0.0060105   5.464 5.06e-08 ***\nPTPRIORLO                           -0.0069371  0.0002201 -31.525  &lt; 2e-16 ***\nADMPOL_PTNON SEL IN HIGHLY SEL AREA -0.0154098  0.0060930  -2.529 0.011488 *  \nADMPOL_PTSEL                         0.0652773  0.0077220   8.453  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07492 on 2883 degrees of freedom\n  (59 observations deleted due to missingness)\nMultiple R-squared:  0.8443,    Adjusted R-squared:  0.8433 \nF-statistic: 868.5 on 18 and 2883 DF,  p-value: &lt; 2.2e-16\n\nCode#Get fitted values with NA for excluded rows\nfitted_vals &lt;- fitted(england_model7)\n\n# Add fitted values to the dataframe\nengland_filtered_clean$fitted7 &lt;- fitted_vals"
  },
  {
    "objectID": "sessions/week7_practical.html#task-4---evaluation",
    "href": "sessions/week7_practical.html#task-4---evaluation",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Task 4 - Evaluation",
    "text": "Task 4 - Evaluation\n\nWhen you have your ‘best’ model, how do you interpret the coefficients?\n\nWhich variable(s) has(have) the most explanatory power (check t-values for this)?\nHow do you interpret the combined explanatory power of variables in your model?\nWhat kind of confounding do you observe as you add more variables (if any)?\nDo you have any issues of multicollinearity or residual independence? Does your model pass the standard tests?\n\n\n\nHere’s my best model compared with some of the others I built as I went along\n\nCodenames(coef(england_model3))\n\n[1] \"(Intercept)\"              \"log(PTFSM6CLA1A)\"        \n[3] \"log(PERCTOT)\"             \"RELCHAR_GrpChristian\"    \n[5] \"RELCHAR_GrpNon-Christian\"\n\nCodecoef_names &lt;- c(\n  \"Constant\" = \"(Intercept)\",\n  #\"Religious Christian\" = \"RELCHAR_GrpChristian\",\n  #\"Religious not-Christian\",\"RELCHAR_GrpNon-Christian\",\n  \"% Disadvantaged end KS4 (log)\" = \"log(PTFSM6CLA1A)\",\n  \"% overall absence (log)\" = \"log(PERCTOT)\",\n  \"% English Not First Language (log)\" = \"log(PNUMEAL)\",\n  \"Ofsted: Outstanding\" = \"OFSTEDRATINGOutstanding\",\n  \"Ofsted: Requires Improvement\" = \"OFSTEDRATINGRequires improvement\",\n  \"Ofsted: Serious Weaknesses\" = \"OFSTEDRATINGSerious Weaknesses\",\n  \"Ofsted: Special Measures\" = \"OFSTEDRATINGSpecial Measures\",\n  \"Region: East Midlands\" = \"gor_nameEast Midlands\",\n  \"Region: East of England\" = \"gor_nameEast of England\",\n  \"Region: London\" = \"gor_nameLondon\",\n  \"Region: North East\" = \"gor_nameNorth East\",\n  \"Region: North West\" = \"gor_nameNorth West\",\n  \"Region: South West\" = \"gor_nameSouth West\",\n  \"Region: West Midlands\" = \"gor_nameWest Midlands\",\n  \"Region: Yorkshire and the Humber\" = \"gor_nameYorkshire and the Humber\",\n  \"% Low Prior Attainment\" = \"PTPRIORLO\",\n  \"Admissions: Non-selective in Highly Selective Area\" = \"ADMPOL_PTNON SEL IN HIGHLY SEL AREA\",\n  \"Admissions: Selective\" = \"ADMPOL_PTSEL\"\n)\n\n\n\nCodelibrary(jtools)\n\nexport_summs(\n  england_model3, england_model4, england_model5, england_model6, england_model7, \n  robust = \"HC3\",\n  coefs = coef_names\n)\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\n\n\n\nConstant\n5.06 ***\n5.01 ***\n4.96 ***\n4.84 ***\n4.63 ***\n\n\n\n(0.02)   \n(0.02)   \n(0.02)   \n(0.02)   \n(0.02)   \n\n\n% Disadvantaged end KS4 (log)\n-0.11 ***\n-0.13 ***\n-0.15 ***\n-0.14 ***\n-0.07 ***\n\n\n\n(0.00)   \n(0.00)   \n(0.00)   \n(0.00)   \n(0.00)   \n\n\n% overall absence (log)\n-0.40 ***\n-0.36 ***\n-0.34 ***\n-0.29 ***\n-0.22 ***\n\n\n\n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\n% English Not First Language (log)\n       \n       \n0.02 ***\n0.02 ***\n0.02 ***\n\n\n\n       \n       \n(0.00)   \n(0.00)   \n(0.00)   \n\n\nOfsted: Outstanding\n       \n       \n       \n0.07 ***\n0.06 ***\n\n\n\n       \n       \n       \n(0.01)   \n(0.00)   \n\n\nOfsted: Requires Improvement\n       \n       \n       \n-0.05 ***\n-0.05 ***\n\n\n\n       \n       \n       \n(0.01)   \n(0.01)   \n\n\nOfsted: Serious Weaknesses\n       \n       \n       \n-0.04    \n-0.06 ** \n\n\n\n       \n       \n       \n(0.02)   \n(0.02)   \n\n\nOfsted: Special Measures\n       \n       \n       \n-0.09 ***\n-0.10 ***\n\n\n\n       \n       \n       \n(0.02)   \n(0.02)   \n\n\nRegion: East Midlands\n       \n-0.01    \n-0.00    \n0.00    \n0.01 *  \n\n\n\n       \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\nRegion: East of England\n       \n-0.00    \n0.00    \n0.00    \n0.02 ***\n\n\n\n       \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\nRegion: London\n       \n0.09 ***\n0.07 ***\n0.07 ***\n0.04 ***\n\n\n\n       \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\nRegion: North East\n       \n0.07 ***\n0.10 ***\n0.09 ***\n0.03 ***\n\n\n\n       \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\nRegion: North West\n       \n-0.00    \n0.01    \n0.02 *  \n-0.00    \n\n\n\n       \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\nRegion: South West\n       \n0.02 ** \n0.03 ***\n0.03 ***\n0.04 ***\n\n\n\n       \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\nRegion: West Midlands\n       \n0.02 ** \n0.03 ***\n0.03 ***\n0.02 ** \n\n\n\n       \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\nRegion: Yorkshire and the Humber\n       \n0.03 ***\n0.04 ***\n0.03 ***\n0.03 ***\n\n\n\n       \n(0.01)   \n(0.01)   \n(0.01)   \n(0.01)   \n\n\n% Low Prior Attainment\n       \n       \n       \n       \n-0.01 ***\n\n\n\n       \n       \n       \n       \n(0.00)   \n\n\nAdmissions: Non-selective in Highly Selective Area\n       \n       \n       \n       \n-0.02 *  \n\n\n\n       \n       \n       \n       \n(0.01)   \n\n\nAdmissions: Selective\n       \n       \n       \n       \n0.07 ***\n\n\n\n       \n       \n       \n       \n(0.01)   \n\n\nN\n2961       \n2961       \n2961       \n2902       \n2902       \n\n\nR2\n0.71    \n0.74    \n0.75    \n0.77    \n0.84    \n\nStandard errors are heteroskedasticity robust. *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\nCodecheck_model(england_model7)\n\n\n\n\n\n\n\n\nCodelibrary(tidyverse)\nlibrary(ggrepel)\n\n# Convert fitted values back to original scale\nengland_filtered_clean &lt;- england_filtered_clean %&gt;%\n  mutate(\n    fitted_original = exp(fitted7),\n    highlight = LANAME == \"Brighton and Hove\",\n    label = if_else(highlight, SCHNAME.x, NA_character_)\n  )\n\n# Scatter plot with layered points and full-model fit line\nggplot(england_filtered_clean, aes(x = fitted_original, y = ATT8SCR)) +\n  # All schools in grey\n  geom_point(color = \"grey80\", alpha = 0.5, size = 2) +\n\n  # Brighton and Hove schools in orange\n  geom_point(data = filter(england_filtered_clean, highlight),\n             aes(x = fitted_original, y = ATT8SCR),\n             color = \"darkorange\", size = 2.5) +\n\n  # Labels for Brighton and Hove schools\n  geom_text_repel(data = filter(england_filtered_clean, highlight),\n                  aes(label = label),\n                  size = 3, max.overlaps = 20) +\n\n  # Line of best fit for all schools\n  geom_smooth(method = \"lm\", se = TRUE, color = \"black\") +\n  \n# Mirror x and y axes\n  coord_equal() +\n\n\n  theme_minimal() +\n  labs(\n    title = \"Observed vs Fitted ATT8SCR (Original Scale)\",\n    x = \"Modelled Attainment 8, 2022-23\",\n    y = \"Observed Attainment 8, 2022-23\"\n  )\n\n\n\n\n\n\n\n\nCode# Filter for Brighton and Hove only\nbrighton_data &lt;- england_filtered_clean %&gt;%\n  filter(LANAME == \"Brighton and Hove\")\n\n# Plot with regression line\nggplot(brighton_data, aes(x = fitted_original, y = ATT8SCR)) +\n  geom_point(color = \"darkorange\", size = 2) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"black\") +\n  geom_text_repel(aes(label = SCHNAME.x), max.overlaps = 20, size = 3) +\n  theme_minimal() +\n  labs(\n    title = \"Observed vs Fitted ATT8SCR for Brighton and Hove Schools\",\n    x = \"Modelled Attainment 8, 2022-23\",\n    y = \"Observed Attainment 8, 2022-23\"\n  )\n\n\n\n\n\n\n\n\nCodelibrary(tidyverse)\n\n# Calculate residuals and filter for Brighton and Hove\nbrighton_residuals &lt;- england_filtered_clean %&gt;%\n  filter(LANAME == \"Brighton and Hove\") %&gt;%\n  mutate(\n    fitted_original = exp(fitted7),\n    residual = ATT8SCR - fitted_original,\n    abs_residual = abs(residual)\n  )\n\n# Create a bar plot centered on zero\nggplot(brighton_residuals, aes(x = reorder(SCHNAME.x, residual), y = residual)) +\n  geom_col(fill = \"darkorange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey40\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Residuals of ATT8SCR for Brighton and Hove Schools\",\n    x = \"School\",\n    y = \"Residual (Observed - Fitted)\"\n  )"
  },
  {
    "objectID": "sessions/week7_practical.html#task-5---interacting-variables-extension-activity",
    "href": "sessions/week7_practical.html#task-5---interacting-variables-extension-activity",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Task 5 - Interacting Variables (Extension activity)",
    "text": "Task 5 - Interacting Variables (Extension activity)\nIn the lecture, you saw how we could look at how some of the continuous variables (like disadvantage or absence) vary by categorical variable levels (like region). Or indeed, whether continuous variables might interact with each other - for example do levels of unauthorised absence and disadvantage affect each other.\nAs you saw in the lecture, interpreting interactions can be complex, so here I would just like you to have a brief experiment with some of the variables in your best model.\nIf you are struggling to understand these effects, again, an AI helper like Gemini or ChatGPT might help you understand these interactions if you feed it your model outputs.\nMost of the time in most of the regression models you build, you might not need to interact variables, but it can be informative. Next week, however, we will look at other alternatives to interacting variables when we move onto linear mixed effects models. So at this point, just some experimentation is what I would like you to achieve."
  },
  {
    "objectID": "sessions/week7.html#introduction",
    "href": "sessions/week7.html#introduction",
    "title": "Week 7 - Prof D’s Regression Sessions Vol 2",
    "section": "Introduction",
    "text": "Introduction\nThis week will build on last week’s introduction to Linear Regression, extending the model into deep space and beyond. Sorry, that’s the Progression Sessions Vol 2 (this week’s recommended listening on the practical pages!). No, we won’t quite get that far, but we will be extending our regression model into multiple (near-space) dimensions to really start to unpick what might be influencing our attainment variables in English Schools.\nIn a multiple linear regression model, additional independent/explanatory variables are added to try and explain the unexplained variance that may remain in a bivariate regression model. In theory these dimensions are infinite, however in practice care needs to be taken to strike the correct balance between explanatory power and interpretability with it unlikely that you could select a large number of independent variables without running into issues of multicollinearity (more of which in the lecture).",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "7. Multiple Regression"
    ]
  },
  {
    "objectID": "sessions/week7.html#learning-objectives",
    "href": "sessions/week7.html#learning-objectives",
    "title": "Week 7 - Prof D’s Regression Sessions Vol 2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will:\n\nExtend your understanding of linear regression by learning how we might incorporate additional continuous and categorical independent variables\nUnderstand why multicollinearity is an issue and how to avoid it\nUnderstand how the spatial or temporal autocorrelation of residuals might indicate issues in your model\nUnderstand how to incorporate and interpret dummy (categorial) independent variables in your model\nUnderstand what confounding is and how you can use confounding to better understand the influence of variables in your model\nUnderstand how it is possible to interact some of the variables in your model to see how the effects of one variable change in the presence of another, or whether there is indeed no statistically significant interaction",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "7. Multiple Regression"
    ]
  },
  {
    "objectID": "sessions/week7.html#lecture",
    "href": "sessions/week7.html#lecture",
    "title": "Week 7 - Prof D’s Regression Sessions Vol 2",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "7. Multiple Regression"
    ]
  },
  {
    "objectID": "sessions/week7.html#quiz",
    "href": "sessions/week7.html#quiz",
    "title": "Week 7 - Prof D’s Regression Sessions Vol 2",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "7. Multiple Regression"
    ]
  },
  {
    "objectID": "sessions/week7.html#practical",
    "href": "sessions/week7.html#practical",
    "title": "Week 7 - Prof D’s Regression Sessions Vol 2",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nMain Practical Page\nDownload",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "7. Multiple Regression"
    ]
  },
  {
    "objectID": "sessions/week7.html#further-resources",
    "href": "sessions/week7.html#further-resources",
    "title": "Week 7 - Prof D’s Regression Sessions Vol 2",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "7. Multiple Regression"
    ]
  },
  {
    "objectID": "sessions/week6_lecture.html#this-weeks-session---foundations",
    "href": "sessions/week6_lecture.html#this-weeks-session---foundations",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "This week’s Session - Foundations",
    "text": "This week’s Session - Foundations\n\n\n\nMotivation:\n\nOver the years I’ve marked too many dissertations and examined too many PhDs where people still get the basics of regression wrong - this is my attempt to help fix this once and for all!\nI’ve been using this method for 20 years and I still get things wrong and I’m still learning new things about it!\nOnce you get the basics, it is perhaps THE most useful tool in your statistical toolbox\nForget about machine learning and pointlessly complicated models - a simple regression model is your statistical Swiss Army Knife!"
  },
  {
    "objectID": "sessions/week6_lecture.html#this-weeks-session---foundations-1",
    "href": "sessions/week6_lecture.html#this-weeks-session---foundations-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "This week’s Session - Foundations",
    "text": "This week’s Session - Foundations\n\n\n\nA Recipe for Success:\n\nunderstanding your ingredients (data)\nunderstanding the basic method (rules) - sample size, heteroscedascity, variance, degrees of freedom\nbaking your first cake (model)\njudging your efforts (interpreting the outputs)"
  },
  {
    "objectID": "sessions/week6_lecture.html#this-weeks-session---foundations-2",
    "href": "sessions/week6_lecture.html#this-weeks-session---foundations-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "This week’s Session - Foundations",
    "text": "This week’s Session - Foundations\n\nApplied Regression in the real world\n\nAny model is just a simplified version of reality\nA good model can offer you insights into the real world situation you are studying and can underpin good real world decisions\na good model interpreted badly can lead to poor policy decisions\na bad model, even if interpreted correctly, will probably also lead to poor policy decisions\n\nSo we need to get both the modelling and the interpretation correct!"
  },
  {
    "objectID": "sessions/week6_lecture.html#secondary-schools-and-attainment---gcses",
    "href": "sessions/week6_lecture.html#secondary-schools-and-attainment---gcses",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Secondary Schools and Attainment - GCSEs",
    "text": "Secondary Schools and Attainment - GCSEs\n\nSecondary Schools mainly teach children between the ages of 11-16 in England and Wales (some 11-18, some 13-18)\nThe examinations most children take at the end of Year 11 (age 16) are called GCSEs (General Certificate of Secondary Education)\nThe GCSEs are graded from 9 (highest) to 1 (lowest), with a grade of 4 considered a “standard pass” and a grade of 5 considered a “strong pass”"
  },
  {
    "objectID": "sessions/week6_lecture.html#secondary-schools-and-attainment---attainment-8",
    "href": "sessions/week6_lecture.html#secondary-schools-and-attainment---attainment-8",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Secondary Schools and Attainment - Attainment 8",
    "text": "Secondary Schools and Attainment - Attainment 8\n\nAttainment 8 is a measure that sums the grades for each pupil across 8 GCSEs (the standard number taken).\n\nMaths is always counted twice and English often counted twice where both language and literature are taken.\nThus a maximum Attainment 8 score of 90 can be achieved\n40 = Standard Pass, 50 = Strong Pass\n\nThe Attainment 8 scores for all year 11 students can be averaged for each school giving a school-level average Attainment 8 Score\nAttainment 8 is a raw score and doesn’t account for important variations in the cohorts of students each school admits or the types of school, so direct comparison between schools without accounting for these factors is risky"
  },
  {
    "objectID": "sessions/week6_lecture.html#secondary-schools-and-attainment---progress-8",
    "href": "sessions/week6_lecture.html#secondary-schools-and-attainment---progress-8",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Secondary Schools and Attainment - Progress 8",
    "text": "Secondary Schools and Attainment - Progress 8\n\nProgress 8 is an alternative attainment score which looks at the progress a student makes between arriving at a school in year 7 or 9 and leaving at age 16\nIt compares their levels of attainment at entry and exit with the progress made by similar students nationally\nProgress 8 is a ‘value-added’ ratio. A score of zero means students, on average, made expected progress, while a positive score means they made more progress than expected, and a negative score means they made less"
  },
  {
    "objectID": "sessions/week6_lecture.html#secondary-schools-attainment-and-urban-policy",
    "href": "sessions/week6_lecture.html#secondary-schools-attainment-and-urban-policy",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Secondary Schools, Attainment and Urban Policy",
    "text": "Secondary Schools, Attainment and Urban Policy\n\n\n\nSchool performance and pupil attainment can be a big urban policy issue - particularly where variations in access and perceived quality occur\nThese variations feed into broader socio-economic issues in cities\nIn the UK, schools are the responsibility of local government\nUnderstanding the drivers behind pupil attainment and school performance vital for effective resource allocation and good local policy"
  },
  {
    "objectID": "sessions/week6_lecture.html#secondary-schools-attainment-and-urban-policy-1",
    "href": "sessions/week6_lecture.html#secondary-schools-attainment-and-urban-policy-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Secondary Schools, Attainment and Urban Policy",
    "text": "Secondary Schools, Attainment and Urban Policy\n\n\n\nIn 2024, Brighton and Hove Council convinced pupil attainment mainly driven by Disadvantage Attainment Gap - socio-economically disadvantaged students perform worse than their more affluent peers\nWork of Professor Stephen Gorard, University of Durham, suggests mixing of disadvantage improves attainment\nSolution: create more socially mixed schools through a new controversial admissions policy\n\n\n\n\n\nThe Conversation: https://theconversation.com/poorer-pupils-do-worse-at-school-heres-how-to-reduce-the-attainment-gap-205535"
  },
  {
    "objectID": "sessions/week6_lecture.html#reserarch-questions",
    "href": "sessions/week6_lecture.html#reserarch-questions",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Reserarch Question(s)",
    "text": "Reserarch Question(s)\n\nWhat are the factors that affect school-level educational attainment in Brighton and Hove?\nTo what extent is attainment driven by social mixing?\nAre there any other factors that are relevant?\nWhat are the implications of this for local policy?\nCan regression help us and, if it can, how can we go about carefully building a regression model to help us answer these questions?"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1---ingredients",
    "href": "sessions/week6_lecture.html#step-1---ingredients",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1 - Ingredients",
    "text": "Step 1 - Ingredients\n\nThe Exploratory Data Analysis phase is the most important part of any modelling exercise (see Weeks 1 & 2 of this course)\nFailure to get to know your data properly means you might mis-specify your model by:\n\nusing the wrong explanatory variables or omitting some key ones\nmisunderstanding your data types - e.g. counts vs continuous\nmisunderstanding the relationships between your variables (linear vs logarithmic)\nnot accounting for important spatial or temporal patterns (autocorrelation) that might mean your observations are not independent"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1a---ingredients-gathering-and-preparation",
    "href": "sessions/week6_lecture.html#step-1a---ingredients-gathering-and-preparation",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1a - Ingredients (gathering and preparation)",
    "text": "Step 1a - Ingredients (gathering and preparation)\n\n\n\nThe data is collected by the Department for Education (DfE) - a full annual census of each school’s\nHundreds of variables collected relating to:\n\nattainment and progress\npupil characteristics\nschool characteristics\n\nSome data / variables (ingredients) will be more useful than others\n\n\n\n\n\nDfE Data: https://www.compare-school-performance.service.gov.uk/compare-schools"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1a---ingredients-gathering-and-preparation-1",
    "href": "sessions/week6_lecture.html#step-1a---ingredients-gathering-and-preparation-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1a - Ingredients (gathering and preparation)",
    "text": "Step 1a - Ingredients (gathering and preparation)"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1b---ingredients-familiarisation",
    "href": "sessions/week6_lecture.html#step-1b---ingredients-familiarisation",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1b - Ingredients (familiarisation)",
    "text": "Step 1b - Ingredients (familiarisation)\n\n\n\n\n\n\n\nVisualisation is perhaps the most important part of the EDA phase\nAlways map and graph your data so that you can spot potential issues before they ruin your model!"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1b---ingredients-familiarisation-1",
    "href": "sessions/week6_lecture.html#step-1b---ingredients-familiarisation-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1b - Ingredients (familiarisation)",
    "text": "Step 1b - Ingredients (familiarisation)\n\n\n\n\n\n\n\n\n\n\nHmmm? Are there any problems that could be indicated here?"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1b---ingredients-familiarisation-2",
    "href": "sessions/week6_lecture.html#step-1b---ingredients-familiarisation-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1b - Ingredients (familiarisation)",
    "text": "Step 1b - Ingredients (familiarisation)\n\n\n\n\n\n\n\n\n\n\nNotice anything now?\nWhat could/should we do about it? Any suggestions?"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1c---ingredients-selection",
    "href": "sessions/week6_lecture.html#step-1c---ingredients-selection",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1c - Ingredients (selection)",
    "text": "Step 1c - Ingredients (selection)"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1c---ingredients-selection-1",
    "href": "sessions/week6_lecture.html#step-1c---ingredients-selection-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1c - Ingredients (selection)",
    "text": "Step 1c - Ingredients (selection)"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1c---ingredients-selection-2",
    "href": "sessions/week6_lecture.html#step-1c---ingredients-selection-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1c - Ingredients (selection)",
    "text": "Step 1c - Ingredients (selection)"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1c---ingredients-selection-3",
    "href": "sessions/week6_lecture.html#step-1c---ingredients-selection-3",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1c - Ingredients (selection)",
    "text": "Step 1c - Ingredients (selection)\n\n\n\nThe process of exploring and interrogating your data should take much longer than the modelling at the end\nThe process is iterative - you might well be exploring your data and the theory at the same time to help you explore, filter, select and prepare for the modelling phase\n\n\n\n\n\n\n\nflowchart TD\n    A[\"&lt;img src='L6_images/Data.png'&gt;\"] --&gt; B[&lt;img src='L6_images/Book.png'&gt;]\n    B[&lt;img src='L6_images/Book.png'&gt;] --&gt; A[\"&lt;img src='L6_images/Data.png'&gt;\"]"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1c---ingredients-selection-4",
    "href": "sessions/week6_lecture.html#step-1c---ingredients-selection-4",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1c - Ingredients (selection)",
    "text": "Step 1c - Ingredients (selection)\n\n\n\nUnderstanding your ‘system’ is vital\nWhat is “in” and what is “out”?\nHow do you know?\n\nExperience (e.g. I used to be a school teacher so aware of differences in types of school)\nResearch! If you are unfamiliar with the system, do your research"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1c---ingredients-selection-5",
    "href": "sessions/week6_lecture.html#step-1c---ingredients-selection-5",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1c - Ingredients (selection)",
    "text": "Step 1c - Ingredients (selection)\n\nThe role of theory and wider research on your system is vital in helping you select your variables for investigation\nDO NOT JUST THROW EVERYTHING INTO YOUR MODEL JUST BECAUSE YOU HAVE SOME VARIABLES\n\nThis is a common mistake\nIt can also lead to spurious interpretation where correlation and causation are not the same thing\n\nCarrying out a thorough literature review will also give you context for interpreting your model results later on in the process\nALWAYS CARRY OUT A THOROUGH LITERATURE REVIEW TO HELP GUIDE YOUR DATA / VARIABLE SELECTION"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1c---ingredients-selection-6",
    "href": "sessions/week6_lecture.html#step-1c---ingredients-selection-6",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1c - Ingredients (selection)",
    "text": "Step 1c - Ingredients (selection)\n\n\n\nBrighton and Hove City Council relied heavily on the work of Gorard in building its policy\nPaper links social mixing to improved attainment for disadvantaged pupils\nIncludes variables such as:\n\nschool type (e.g. academy, maintained)\npupil characteristics (e.g. free school meals (FSM) eligibility, special educational needs, ethnicity)\nschool characteristics (e.g. size, location)\n\nThus all might be worth investigating\n\n\n\n\n\nhttps://journals.sagepub.com/doi/full/10.1177/2158244018825171"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-1c---ingredients-selection-7",
    "href": "sessions/week6_lecture.html#step-1c---ingredients-selection-7",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 1c - Ingredients (selection)",
    "text": "Step 1c - Ingredients (selection)\n\n\n\nHowever, wider reading also suggests that factors such as attendance (which Gordard does not include in his paper as a variable) may also play a big role in the attainment of disadvantaged pupils\nWork by Claymore suggests that a large proportion of the gap in attainment between disadvantaged pupils and more affluent peers can be explained by:\n\nthe differences in absence rates\nexclusion\nrates of moving between schools\n\n\n\n\n\n\nhttps://www.nfer.ac.uk/publications/being-present-the-power-of-attendance-and-stability-for-disadvantaged-pupils/"
  },
  {
    "objectID": "sessions/week6_lecture.html#step-2---method",
    "href": "sessions/week6_lecture.html#step-2---method",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Step 2 - Method",
    "text": "Step 2 - Method\n\n\n\n\n\nflowchart TD\n    I{Choosing a &lt;br/&gt;statistical test} --&gt;\n    A[How Many Variables?] --&gt;\n    C(2?) & G(More than 2?)\n    C --&gt;|Categorical?| D(Chi Squared &lt;br/&gt;or Similar e.g. T-test)\n    C --&gt;|Scale or Ratio?| E(Pearson Correlation &lt;br/&gt;or Spearman's Rank)\n    C --&gt;|Both?| F(Ask Google)\n    G --&gt; H(REGRESSION - everything else, &lt;br/&gt;get in the bin)"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---its-just-a-scatter-plot",
    "href": "sessions/week6_lecture.html#linear-regression---its-just-a-scatter-plot",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - It’s just a scatter plot!",
    "text": "Linear Regression - It’s just a scatter plot!\n\n\n\n\n\n\n\n\n\n\nA regression model is nothing more than a description of a scatter plot\nDependent variable = \\(Y\\)-axis\nIndependent variable = \\(X\\)-axis"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---line-of-best-fit",
    "href": "sessions/week6_lecture.html#linear-regression---line-of-best-fit",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Line of Best-fit",
    "text": "Linear Regression - Line of Best-fit\n\n\n\n\n\n\n\n\n\n\nThe linear regression model is the straight line of best-fit\nA linear model function lm() (in R) uses a method called Ordinary Least Squares (OLS) to find the line of best-fit.\nThe best line minimises the squared (so that negatives and positives don’t cancel) vertical distances between the line and the points - hence OLS"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---residuals-error",
    "href": "sessions/week6_lecture.html#linear-regression---residuals-error",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Residuals / Error",
    "text": "Linear Regression - Residuals / Error\n\n\n\n\n\n\n\n\n\n\nThe vertical distances between the points and the line of best fit are called the residuals - sometimes also referred to as the errors or \\(\\epsilon\\)\nThe closer the points are to the line, the better the fit of the model"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---r-squared",
    "href": "sessions/week6_lecture.html#linear-regression---r-squared",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - R-Squared",
    "text": "Linear Regression - R-Squared\n\n\n\n\n\n\n\n\n\n\nThe fit of the model is represented by the coefficient of determination or \\(R^2\\) value (0-1), which is calculated from the residuals\nIt describes how much of the variation in \\(Y\\) (Attainment 8 score) is explained by the variation in \\(X\\) (% Disadvantaged Students) - here 69%\nThe closer to 1 (100%), the better the fit of the model"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---r-squared-1",
    "href": "sessions/week6_lecture.html#linear-regression---r-squared-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - R-Squared",
    "text": "Linear Regression - R-Squared\n\n\nIt’s easy to visually estimate your \\(R^2\\) value from looking at how well correlated the points are. NB as squared, always +\n\n\nSource: https://work.thaslwanter.at/Stats/html/statsRelation.html"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---slope-and-intercept",
    "href": "sessions/week6_lecture.html#linear-regression---slope-and-intercept",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Slope and Intercept",
    "text": "Linear Regression - Slope and Intercept\n\n\n\n\n\n\n\n\n\n\nThe regression line itself can be described by an equation with two parameters / coefficients:\n\nThe intercept - \\(\\beta_0\\) - which is the value of \\(Y\\) when \\(X = 0\\)\nThe slope - \\(\\beta_1\\) - which the change in the value of \\(Y\\) for a 1 unit change in \\(X\\)"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---slope-and-intercept-1",
    "href": "sessions/week6_lecture.html#linear-regression---slope-and-intercept-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Slope and Intercept",
    "text": "Linear Regression - Slope and Intercept\n\nAnother way of thinking of your intercept is as your model baseline.\n\nIn our case: baseline attainment, controlling for disadvantage.\n\nHigher or lower intercept values for different local authorities relate to higher or lower baseline attainment.\nAnother way of thinking about your slope is the level of influence your \\(X\\) variable might be having on your \\(Y\\) variable\n\nHigher value = Steeper Slope = More influence\nLower value = more horizontal slope = Less influence"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---model-estimates",
    "href": "sessions/week6_lecture.html#linear-regression---model-estimates",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Model Estimates",
    "text": "Linear Regression - Model Estimates\n\n\n\n\n\n\n\n\n\n\\[Y = \\beta_0 + \\beta_1X_1 + \\epsilon\\] \\[\\hat{Y} = 62.35 + (-0.63 \\times X) + \\epsilon\\] \\[40.3 = 62.35 + (-0.63 \\times 35) + 0\\]"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---the-statistical-model",
    "href": "sessions/week6_lecture.html#linear-regression---the-statistical-model",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - The Statistical Model",
    "text": "Linear Regression - The Statistical Model\n\n\n\nScatter plots are an excellent intuitive way to understand the relationship between two continuous variables\nBut algorithms are required to generate the various plot statistics and other useful info\nLots of different statistical software packages will do this\nDoesn’t matter which you use, they all do pretty much the same thing under the hood"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---the-statistical-model-1",
    "href": "sessions/week6_lecture.html#linear-regression---the-statistical-model-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - The Statistical Model",
    "text": "Linear Regression - The Statistical Model\n\n\n\nCall:\nlm(formula = ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.300 -2.165  1.400  2.567  4.738 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  62.3457     3.7367  16.685 1.68e-07 ***\nPTFSM6CLA1A  -0.6292     0.1492  -4.217  0.00293 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.087 on 8 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.651 \nF-statistic: 17.78 on 1 and 8 DF,  p-value: 0.002927\n\n\n\nExample output from R"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---running-the-model",
    "href": "sessions/week6_lecture.html#linear-regression---running-the-model",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Running the Model",
    "text": "Linear Regression - Running the Model\n\n\nThis is the code representation of the model equation we saw earlier:\n\nlm() is the function that fits a linear model\nATT8SCR (Attainment 8 Score) is the dependent variable \\(Y\\)\n~ means “is modelled by”\nPTFSM6CLA1A (% Disadvantaged Students) is the independent variable \\(X\\)\ndata = bnt_sub is the dataset we are using which contains the variables"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---residuals-and-error",
    "href": "sessions/week6_lecture.html#linear-regression---residuals-and-error",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Residuals and Error",
    "text": "Linear Regression - Residuals and Error\n \n\nThe residual errors range from -8.3 (Longhill) to + 4.7 (Varndean)\nThe residual standard error of 4.087 = predictions for School’s Attainment 8 score are off by about 4.1 points\nResidual Standard Error (RSE) and \\(R^2\\) are inversely related - A smaller RSE and larger \\(R^2\\) both mean a more precise model\nThe F-Statistic is a ratio of the amount of variance in \\(Y\\) explained by the model to that not. x17.7 more - statistically significant &lt;0.005"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom",
    "href": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Degrees of Freedom",
    "text": "Linear Regression - Degrees of Freedom\n\n\nDF are the Degrees of Freedom in the model. DF very important for understanding how reliable your \\(R^2\\) value might be\n\nThe first number (1) relates to the number of variables\nThe second number (8) relates to the number of observations (cases) in the dataset, minus the number of parameters\nIn our example we have 10 observations and 2 parameters (intercept and slope) so \\(10 - 2 = 8\\) degrees of freedom"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-1",
    "href": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Degrees of Freedom",
    "text": "Linear Regression - Degrees of Freedom\n\nGeneral rule: more degrees of freedom = more reliable model\nA model with many parameters vs observations will bend to fit those observations and not be a good generalisation - this is called overfitting\nA model with little freedom might appear to have a high \\(R^2\\)\n\nbut it is likely to perform poorly on new, unseen data\nit has captured the characteristics of your specific dataset rather than an underlying truth\n\nGeneral Rule: the more observations in your dataset the better!\nHigh \\(R^2\\) with low DF = 💩"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-2",
    "href": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Degrees of Freedom",
    "text": "Linear Regression - Degrees of Freedom\n\n\n\n\n\n\n\n\n\n\n2 observations - 2 parameters = 0 degrees of freedom"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-3",
    "href": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-3",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Degrees of Freedom",
    "text": "Linear Regression - Degrees of Freedom\n\n\n\n\n\n\n\n\n\n\n3 observations - 2 parameters = 1 degree of freedom"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-4",
    "href": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-4",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Degrees of Freedom",
    "text": "Linear Regression - Degrees of Freedom\n\n\n\n\n\n\n\n\n\n\n4 observations - 2 parameters = 2 degrees of freedom"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-5",
    "href": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-5",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Degrees of Freedom",
    "text": "Linear Regression - Degrees of Freedom\n\n\n\n\n\n\n\n\n\n\n5 observations - 2 parameters = 3 degrees of freedom"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-6",
    "href": "sessions/week6_lecture.html#linear-regression---degrees-of-freedom-6",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Degrees of Freedom",
    "text": "Linear Regression - Degrees of Freedom\n\nHow many Degrees of Freedom are required for a reliable model?\nNo single, universally agreed-upon number - aim to maintain a healthy ratio of observations to the number of parameters\nA common rule of thumb is the 10:1 ratio - 10 observations for every 1 parameter - although this is a rough guide. If in doubt, even up to 20:1 to be safe\nIn our example, we have 10 observations and 2 parameters = 5:1 ratio = so a potentially unreliable model - proceed with extreme caution!"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---variance",
    "href": "sessions/week6_lecture.html#linear-regression---variance",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Variance",
    "text": "Linear Regression - Variance\n\nDegress of Freedom are also used to calculate the variance of the model\nIf you ran the same model on different samples of the same population - e.g instead of Brighton, you ran this model on schools in Leeds or Bristol or Liverpool - you would get different values for your coefficients each time\nHow similar these are to each other and more importantly the the whole population (all schools in England and Wales) is the variance - you want to minimise the variance in your coefficients if you want to generalise your model to the wider population\nWe will return to the idea of variance in the practical"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---coefficients-1",
    "href": "sessions/week6_lecture.html#linear-regression---coefficients-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Coefficients 1",
    "text": "Linear Regression - Coefficients 1\n\n\nThe Estimates (\\(\\beta\\))\n\nIntercept \\(\\beta_0\\) - estimated Attainment 8 value when % Disadvantaged Children in a school is zero\nPTFSM6CLA1A (slope \\(\\beta_1\\)) - effect of a one-unit change in the % Disadvantaged Children on Attainment 8 - therefore linked to the units of the independent variable and not directly comparable across different variables if measured on different scales"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---coefficients-2",
    "href": "sessions/week6_lecture.html#linear-regression---coefficients-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Coefficients 2",
    "text": "Linear Regression - Coefficients 2\n\n\nThe Standard Error (SE) - measure of the uncertainty or precision of the estimate (coefficient) - how much it might vary\n\nLarge SE relative to the Estimate = estimate unreliable / uncertain\nSE of 0.15 for the estimate of -0.63 for PTFSM6CLA1A means the change in the value of Attainment 8 for a 1% change in disadvantaged students could vary between -0.48 and -0.78"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---coefficients-3",
    "href": "sessions/week6_lecture.html#linear-regression---coefficients-3",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Coefficients 3",
    "text": "Linear Regression - Coefficients 3\n\n\nThe t-value - ratio of the \\(\\frac{Estimate}{SE}\\)\n\na large standard error will make the t-value small (close to zero)\nt-values can be thought of as a standardised coefficient - very useful for comparing the relative importance of different predictors in the model\nThe larger the t-value, the more important the predictor is in explaining the variation in the dependent variable - more important the variable"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---coefficients-4",
    "href": "sessions/week6_lecture.html#linear-regression---coefficients-4",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Coefficients 4",
    "text": "Linear Regression - Coefficients 4\n\n\nThe \\(P\\)-value - \\(P\\)-robability of observing a t-value as extreme as the one calculated if there were no relationship between the independent and the dependent variable\nA small \\(P\\)-value (typically \\(p\\) &lt; 0.05) indicates &lt;5% chance that \\(X\\) is not really explaining variation in \\(Y\\)\nThe Signif. codes (like *** and **) are just a quick visual guide to this p-value, showing you at a glance which variables are significant.\nAny parameter with 1, 2 or 3* is “statistically significant” at 5% or better"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---coefficients-4-1",
    "href": "sessions/week6_lecture.html#linear-regression---coefficients-4-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Coefficients 4",
    "text": "Linear Regression - Coefficients 4\n\n\nThe \\(P\\)-values relate to the null hypotheses that:\n\nThe true value of the intercept (baseline attainment) is zero when the independent variable (% disadvantage) is zero.\n\\(P\\) &lt;0.001 = &lt;0.1% \\(P\\)-robability that 62.3 occurred by random chance\n\nNote - It doesn’t tell us that when % Disadvantaged is zero, 62.3 will be a correct Attainment 8 score, just that real value is unlikely to be zero"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---coefficients-4-2",
    "href": "sessions/week6_lecture.html#linear-regression---coefficients-4-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Coefficients 4",
    "text": "Linear Regression - Coefficients 4\n\n\nThe \\(P\\)-values relate to the null hypotheses that:\n\nThe true value of the PTFSM6CLA1A slope (relationship between % Disadvantaged and Attainment 8) is zero. \\(P\\)- &lt; 0.01 = &lt;1% \\(P\\)-robability that the relationship observed is by chance. Relationship is likely to be real."
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---summary-so-far",
    "href": "sessions/week6_lecture.html#linear-regression---summary-so-far",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Summary so far",
    "text": "Linear Regression - Summary so far\n\n\n\n\n\n\n\n\n\n\nVariation in % disadvantaged students in schools in Brighton appears to explain about 65-68% of the variation in Attainment 8 at the school level\nThe % disadvantaged students is a statistically significant predictor and the relationship appears to be linear\nA 1% reduction in the number of disadvantaged students in a school appears to be associated with a 0.62 point increase in Attainment 8, and vice versa. So the council was right? Well, not quite…"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---summary-so-far-1",
    "href": "sessions/week6_lecture.html#linear-regression---summary-so-far-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Summary so far",
    "text": "Linear Regression - Summary so far\n\nThe degrees of freedom in the model are small, so model likely to be over-fitted and unreliable for generalisation\n\n\nSmall numbers of observations (schools in the city) mean that the model is highly sensitive to changes in the data\nIt’s also unclear whether there are any other factors that might be correlated with disadvantaged students that might also be influencing Attainment 8 scores and confounding (return to this later) the apparent relationship we observe\nWhat are the practical consequences of overfitting?"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---overfitting-outliers-and-high-leverage-points",
    "href": "sessions/week6_lecture.html#linear-regression---overfitting-outliers-and-high-leverage-points",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Overfitting, Outliers and High Leverage Points",
    "text": "Linear Regression - Overfitting, Outliers and High Leverage Points\n\nWhen we overfit, the model can be overly influenced by outliers (large residuals) and changes in key leverage points (points that are far from the mean of the independent variable)\nWhat happens to our model if:\n\na school improvement plan brings the outlier (Longhill) closer to attainment at Hove Park?\nCouncil makes Varndean increase its disadvantaged intake to 30%?\nAnd BACA improves its Attainment 8 to the city average following a huge cash injection from its academy trust?"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---overfitting-outliers-and-high-leverage-points-1",
    "href": "sessions/week6_lecture.html#linear-regression---overfitting-outliers-and-high-leverage-points-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Overfitting, Outliers and High Leverage Points",
    "text": "Linear Regression - Overfitting, Outliers and High Leverage Points\n\n\n\n\n\n\n\n\n\n\nPlausible changes to three schools have almost completely removed relationship between % Disadvantaged Students & Attainment 8\nOverfitting -&gt; parameters highly sensitive to minor changes to a few key data points\nThe closer the best-fit line gets to horizontal, the closer we get to NO RELATIONSHIP between the independent and dependent variables"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---overfitting-outliers-and-high-leverage-points-2",
    "href": "sessions/week6_lecture.html#linear-regression---overfitting-outliers-and-high-leverage-points-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Overfitting, Outliers and High Leverage Points",
    "text": "Linear Regression - Overfitting, Outliers and High Leverage Points\n\n\n\nCall:\nlm(formula = ATT8SCR ~ PTFSM6CLA1A, data = btn_edit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4563 -2.4489 -0.5964  2.4240  6.0536 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  54.5204     3.2593  16.728 1.65e-07 ***\nPTFSM6CLA1A  -0.2025     0.1252  -1.617    0.145    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.476 on 8 degrees of freedom\nMultiple R-squared:  0.2462,    Adjusted R-squared:  0.152 \nF-statistic: 2.614 on 1 and 8 DF,  p-value: 0.1446\n\n\n\nOverall \\(P\\)-value of model now statistically insignificant &gt;0.1\n% Disadvantage in Schools now statistically insignificant"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---more-degrees-of-freedom",
    "href": "sessions/week6_lecture.html#linear-regression---more-degrees-of-freedom",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - More Degrees of Freedom",
    "text": "Linear Regression - More Degrees of Freedom\n\n\n\n\n\n\n\n\n\n\nNow we have a model that includes all schools in England and Wales\nNegative association present - appears to confirm Gorard’s observation\nR-Squared is reasonable - 34% of the variation explained"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---more-degrees-of-freedom-1",
    "href": "sessions/week6_lecture.html#linear-regression---more-degrees-of-freedom-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - More Degrees of Freedom",
    "text": "Linear Regression - More Degrees of Freedom\n\n\n\nCall:\nlm(formula = ATT8SCR ~ PTFSM6CLA1A, data = england_filtered)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.268  -4.894  -1.345   3.662  33.140 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 56.955578   0.280566  203.00   &lt;2e-16 ***\nPTFSM6CLA1A -0.377548   0.009251  -40.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.59 on 3248 degrees of freedom\n  (101 observations deleted due to missingness)\nMultiple R-squared:  0.339, Adjusted R-squared:  0.3388 \nF-statistic:  1665 on 1 and 3248 DF,  p-value: &lt; 2.2e-16\n\n\n\nWe now have 3248 degrees of freedom\nIntercept and % Disadvantaged Students both highly significant\nLife is good? Wrong… Houston, we have some more problems!!"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---other-regression-assumptions",
    "href": "sessions/week6_lecture.html#linear-regression---other-regression-assumptions",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Other Regression Assumptions",
    "text": "Linear Regression - Other Regression Assumptions\n\nI forgot to mention - there are another set of rules to follow to ensure our linear regression is reliable:\n\nLinearity: The relationship between the independent and dependent variables is linear\nHomoscedasticity: Constant variance of residuals across all levels of the independent variable\nNormality of residuals: Residuals are normally distributed\nNo multicollinearity: Independent variables are not highly correlated\nIndependence of residuals: Errors are not related to each other with each other"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---linearity",
    "href": "sessions/week6_lecture.html#linear-regression---linearity",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Linearity?",
    "text": "Linear Regression - Linearity?\n\n\n\n\n\n\n\n\n\n\nLinearity - Does your line go through all the points nicely?\nHmmmm - suggestion of non-linearity with big up-tick at the low end of disadvantage"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---linearity-1",
    "href": "sessions/week6_lecture.html#linear-regression---linearity-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Linearity?",
    "text": "Linear Regression - Linearity?\n\n\n\n\n\n\n\n\n\n\nPloting the residuals against the fitted values is one check for linearity\nThe residuals should be randomly scattered around zero - if they are not, it suggests a non-linear relationship\nThe reference line should be horizontal at zero"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---homoscedasticity-constant-variance",
    "href": "sessions/week6_lecture.html#linear-regression---homoscedasticity-constant-variance",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Homoscedasticity (Constant variance)?",
    "text": "Linear Regression - Homoscedasticity (Constant variance)?\n\n\n\n\n\n\n\n\n\n\nIssues here too: the residuals are not randomly scattered around zero\nThey are instead displaying heteroscdasticity (different variance)"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---normality-of-residuals",
    "href": "sessions/week6_lecture.html#linear-regression---normality-of-residuals",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Normality of Residuals?",
    "text": "Linear Regression - Normality of Residuals?\n\n\n\n\n\n\n\n\n\n\nThis is known as a Q-Q plot (Quantile-Quantile plot)\nA number of points not on the line - suggesting the residuals are not normally distributed\nMany points off line could mean untrustworthy p-values"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---outliers",
    "href": "sessions/week6_lecture.html#linear-regression---outliers",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Outliers?",
    "text": "Linear Regression - Outliers?\n\n\n\n\n\n\n\n\n\n\nAt least we don’t have any problems with outliers!"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---diagnostics",
    "href": "sessions/week6_lecture.html#linear-regression---diagnostics",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Diagnostics",
    "text": "Linear Regression - Diagnostics\n\nLinearity: 😭\nHomoscedasticity: 😭\nNormality of residuals: 😭\nNo multicollinearity: 🕦\nIndependence of residuals: 🕦\nPresently our model violates most of the key assumptions that underpin linear regression models. What this means is that in its current form, the relationship between % Disadvantaged Students and Attainment 8 described by the model is not reliable"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---non-linearity",
    "href": "sessions/week6_lecture.html#linear-regression---non-linearity",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Non-Linearity",
    "text": "Linear Regression - Non-Linearity\n\\[log(Y) = \\beta_0 + \\beta_1log(X_1) + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\nThe log(\\(Y\\)) log(\\(X\\)) model is a standard transformation of these variables that can help to linearise relationships\nSometimes referred to as an elasticity model - a % (rather than constant) change in \\(X\\) leads to a % (not constant) change in \\(Y\\)"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation",
    "href": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - the log-log transformation",
    "text": "Linear Regression - the log-log transformation"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-1",
    "href": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - the log-log transformation",
    "text": "Linear Regression - the log-log transformation"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-2",
    "href": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - the log-log transformation",
    "text": "Linear Regression - the log-log transformation\n\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.20789 -0.08730 -0.00789  0.08197  0.56024 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.475612   0.012893  347.14   &lt;2e-16 ***\nlog(PTFSM6CLA1A) -0.207443   0.004054  -51.18   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1438 on 3246 degrees of freedom\nMultiple R-squared:  0.4465,    Adjusted R-squared:  0.4464 \nF-statistic:  2619 on 1 and 3246 DF,  p-value: &lt; 2.2e-16\n\n\n\nEverything now looks highly statistically significant\n\\(R^2\\) is much better than before - 45%"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-3",
    "href": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-3",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - the log-log transformation",
    "text": "Linear Regression - the log-log transformation\n\n\n\n\n\n\n\n\n\n\nThe residuals are now randomly scattered around zero - suggesting a linear relationship"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-4",
    "href": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-4",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - the log-log transformation",
    "text": "Linear Regression - the log-log transformation\n\n\n\n\n\n\n\n\n\n\nThe residuals are now normally distributed - the Q-Q plot shows most points on the line"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-5",
    "href": "sessions/week6_lecture.html#linear-regression---the-log-log-transformation-5",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - the log-log transformation",
    "text": "Linear Regression - the log-log transformation\n\nInterpreting the log-log (elasticity) relationship:\n\nWhere the relationship is negative, a small absolute change in the independent variable at the lower end has a bigger impact than the same small absolute change at the higher end\nOn our original plot: the effect of levels of disadvantage on attainment is much stronger at very low levels of disadvantage and weakens as the level of disadvantage increases"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---level-log-model",
    "href": "sessions/week6_lecture.html#linear-regression---level-log-model",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Level-log model",
    "text": "Linear Regression - Level-log model\n\\[Y = \\beta_0 + \\beta_1log(X_1) + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\nOther options are to use a level-log model, where the dependent variable is not transformed but the independent variable is log-transformed.\nUse when the effect of the independent variable has diminishing returns - e.g. a change from \\(log(2.7%)\\) or \\(exp(1)\\) to \\(log(7.3%)\\) \\(exp(2)\\) disadvantaged students has same effect as a change from \\(log(20%)\\) \\(exp(3)\\) to \\(log(54.5%)\\) \\(exp(4)\\)"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---level-log-model-1",
    "href": "sessions/week6_lecture.html#linear-regression---level-log-model-1",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Level-log model",
    "text": "Linear Regression - Level-log model\n\n\n\nCall:\nlm(formula = ATT8SCR ~ log(PTFSM6CLA1A), data = england_filtered)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.631  -4.377  -0.919   3.504  34.071 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       79.0199     0.6080  129.97   &lt;2e-16 ***\nlog(PTFSM6CLA1A) -10.3070     0.1911  -53.92   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.78 on 3246 degrees of freedom\nMultiple R-squared:  0.4725,    Adjusted R-squared:  0.4723 \nF-statistic:  2908 on 1 and 3246 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---level-log-model-2",
    "href": "sessions/week6_lecture.html#linear-regression---level-log-model-2",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Level-log model",
    "text": "Linear Regression - Level-log model"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---level-log-model-3",
    "href": "sessions/week6_lecture.html#linear-regression---level-log-model-3",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - Level-log model",
    "text": "Linear Regression - Level-log model"
  },
  {
    "objectID": "sessions/week6_lecture.html#linear-regression---to-log-or-not-to-log",
    "href": "sessions/week6_lecture.html#linear-regression---to-log-or-not-to-log",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Linear Regression - to log or not to log?",
    "text": "Linear Regression - to log or not to log?\n\nThe log-log model appears a better fit than the level-log model, however both are better than the basic linear model\nHow to proceed will depend a little on whether we want to extend our model with other explanatory variables and the relationship they have with attainment\nLog-log - use when the relationship is believed to be multiplicative and the effects are best understood in terms of percentage changes.\n\nThis model is also excellent at addressing issues with heteroscedasticity and skewed variables\n\nLevel-log - use when the effect of the independent variable has diminishing returns"
  },
  {
    "objectID": "sessions/week6_lecture.html#conclusions",
    "href": "sessions/week6_lecture.html#conclusions",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Conclusions",
    "text": "Conclusions\n\nRegression Basics - DONE!\nWon’t summarise here, but please go over the slides again in your own time\nNow is your chance to put everything I have just said into practice in your own very similar analysis in the practical\nPractical is long - you won’t complete in this session, but spend time going through it at home\nMake sure you complete the practical and understand all of this before next week’s Regression Sessions Vol. 2 as it will build on and extend this session"
  },
  {
    "objectID": "sessions/week6_lecture.html#extension-activities",
    "href": "sessions/week6_lecture.html#extension-activities",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Extension Activities",
    "text": "Extension Activities\nExtension 1 - Data Visualisation"
  },
  {
    "objectID": "sessions/week5.html",
    "href": "sessions/week5.html",
    "title": "Week 5",
    "section": "",
    "text": "This week, we’ll explore how to tell if several groups are truly different and how to measure the strength and direction of the relationship between two variables.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#introduction",
    "href": "sessions/week5.html#introduction",
    "title": "Week 5",
    "section": "",
    "text": "This week, we’ll explore how to tell if several groups are truly different and how to measure the strength and direction of the relationship between two variables.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#learning-objectives",
    "href": "sessions/week5.html#learning-objectives",
    "title": "Week 5",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand ANOVA.\nUnderstand correlation between two variables.\nUnderstand the difference of Pearson and Spearman correlation.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#lecture",
    "href": "sessions/week5.html#lecture",
    "title": "Week 5",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#quiz",
    "href": "sessions/week5.html#quiz",
    "title": "Week 5",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#practical",
    "href": "sessions/week5.html#practical",
    "title": "Week 5",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#further-resources",
    "href": "sessions/week5.html#further-resources",
    "title": "Week 5",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week4_lecture.html#overview-of-lecture-3",
    "href": "sessions/week4_lecture.html#overview-of-lecture-3",
    "title": "Linear Algebra",
    "section": "Overview of lecture 3",
    "text": "Overview of lecture 3\nLooked at hypothesis testing:\n\nWhat makes a good hypothesis\nHow to formally state a hypothesis\nTypes of statistical tests"
  },
  {
    "objectID": "sessions/week4_lecture.html#back-to-the-start",
    "href": "sessions/week4_lecture.html#back-to-the-start",
    "title": "Linear Algebra",
    "section": "Back to the start",
    "text": "Back to the start\nMaths underpins quantitative methods\n\nquantitative methods includes data analysis and machine learning\nfocused on algorithms and methodologies\nAND practical examples of how these can be applied"
  },
  {
    "objectID": "sessions/week4_lecture.html#maths-underpins-it",
    "href": "sessions/week4_lecture.html#maths-underpins-it",
    "title": "Linear Algebra",
    "section": "Maths underpins it",
    "text": "Maths underpins it\n\n\n\n\n\nImage credit: [xkcd](https://xkcd.com/1838/)\n\n\n\n\nThis lecture covers some of the key concepts.\nThe goal is to facilitate deeper understanding of the methods."
  },
  {
    "objectID": "sessions/week4_lecture.html#maths-doesnt-bite",
    "href": "sessions/week4_lecture.html#maths-doesnt-bite",
    "title": "Linear Algebra",
    "section": "Maths doesn’t bite!",
    "text": "Maths doesn’t bite!\n\n\n\n\nMaths can seem scary – but goal is to have a better understanding of key ideas. Going to cover quite abit - so focus on the goal of being able to understand equations.\nNot expecting students to be experts!"
  },
  {
    "objectID": "sessions/week4_lecture.html#learning-objectives",
    "href": "sessions/week4_lecture.html#learning-objectives",
    "title": "Linear Algebra",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nDefine concept of linear maps.\nCompute linear algebra equations using vectors and matrices.\n\nDescribe how linear algebra relates to solving linear regression."
  },
  {
    "objectID": "sessions/week4_lecture.html#what-does-it-mean",
    "href": "sessions/week4_lecture.html#what-does-it-mean",
    "title": "Linear Algebra",
    "section": "What does it mean?",
    "text": "What does it mean?\nThe goal is to understand equations like this:\n\\[\\begin{align}\ny = \\sum_{i=1}^n \\beta_i x_i\n\\end{align}\\]\n\nThis is a linear equation."
  },
  {
    "objectID": "sessions/week4_lecture.html#but-what-does-it-mean",
    "href": "sessions/week4_lecture.html#but-what-does-it-mean",
    "title": "Linear Algebra",
    "section": "But what does it mean???",
    "text": "But what does it mean???\nEquations are often used in the methods sections of papers to describe the model.\n\n\n\nTaken from: Chiou, Jou, & Yang, (2015). Factors affecting public transportation usage rate: Geographically weighted regression. Transportation Research Part A: Policy and Practice.\n\n\n\nThis is an equation for a geographically weighted regression model."
  },
  {
    "objectID": "sessions/week4_lecture.html#mathematical-models",
    "href": "sessions/week4_lecture.html#mathematical-models",
    "title": "Linear Algebra",
    "section": "Mathematical models",
    "text": "Mathematical models\n\nMathematical models help us to understand the data\nIn a regression setting the model describes a function that maps input to real-valued outputs\nWe can use mathematical models to validate our hypotheses/research questions\n\n\nThinking back to hypotheses from last week - mathematical models help us to evaluate our research question. Lats week we looked at simple statistical tests - but might have a more sophisticated model of what’s happening."
  },
  {
    "objectID": "sessions/week4_lecture.html#machine-learning",
    "href": "sessions/week4_lecture.html#machine-learning",
    "title": "Linear Algebra",
    "section": "Machine learning",
    "text": "Machine learning\nA model which improves after data is taken into account.\n\nMayne of these concepts are also integral to machine learning\nReally just a specific type of mathematical model\nThe learning part is about automatically finding patterns\n\n\nThese ideas are also fundamental for machine learning.\nLots of different definitions of machine learning - but this us a simple one.\nIn later weeks of the course we will look at machine learning concepts."
  },
  {
    "objectID": "sessions/week4_lecture.html#mathematical-notation",
    "href": "sessions/week4_lecture.html#mathematical-notation",
    "title": "Linear Algebra",
    "section": "Mathematical notation",
    "text": "Mathematical notation\nGoing to be using some mathematical notation\n\nas this is what’s used in papers!\n\n\nIt’s just a formal way of writing maths.\n\nProvides a universal way of writing and understanding maths."
  },
  {
    "objectID": "sessions/week4_lecture.html#cheat-sheet",
    "href": "sessions/week4_lecture.html#cheat-sheet",
    "title": "Linear Algebra",
    "section": "Cheat sheet",
    "text": "Cheat sheet\nMathematical notation cheat sheet: https://www.upyesp.org/posts/makrdown-vscode-math-notation/"
  },
  {
    "objectID": "sessions/week4_lecture.html#letters-for-numbers",
    "href": "sessions/week4_lecture.html#letters-for-numbers",
    "title": "Linear Algebra",
    "section": "Letters for numbers",
    "text": "Letters for numbers\nThere are mathematical conventions for how we describe different things.\n\n\\(a, b, c\\) represent constants\n\n\\(x, y, z, \\dots\\) represent variables\n\n\\(f, g, h, \\dots\\) represent functions\n\n\\(i, j, \\dots\\) often used for indices (i.e. counting)\n\n\\(a_i\\) means the \\(i\\)-th element of a sequence\n\n\\(A, B, C\\) represent matrices"
  },
  {
    "objectID": "sessions/week4_lecture.html#numbers-replaced-by-letters",
    "href": "sessions/week4_lecture.html#numbers-replaced-by-letters",
    "title": "Linear Algebra",
    "section": "Numbers replaced by letters",
    "text": "Numbers replaced by letters\n\n\nThe power to represent any number!\n\n\n\n\n\n\nThis is powerful as it allows is to represent abstract concepts - a universal x, rather than a specific number."
  },
  {
    "objectID": "sessions/week4_lecture.html#sums",
    "href": "sessions/week4_lecture.html#sums",
    "title": "Linear Algebra",
    "section": "Sums",
    "text": "Sums\nSummation notation is a compact way to write repeated addition.\n\\[\\begin{align}\n\\sum_{i=1}^n a_i = a_1 + a_2 + a_3 + \\dots + a_n\n\\end{align}\\]\n\nExample:\n\\[\\begin{align}\n\\sum_{i=1}^5 i = 1+2+3+4+5 = 15\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#product",
    "href": "sessions/week4_lecture.html#product",
    "title": "Linear Algebra",
    "section": "Product",
    "text": "Product\nProduct notation is a compact way to write repeated multiplication.\n\\[\\begin{align}\n\\prod_{i=1}^n a_i = a_1 \\cdot a_2 \\cdot a_3 \\cdot \\dots \\cdot a_n\n\\end{align}\\]\n\nExample:\n\\[\\begin{align}\n\\prod_{i=1}^4 i = 1 \\cdot 2 \\cdot 3 \\cdot 4 = 24\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#a-little-bit-of-epsilon",
    "href": "sessions/week4_lecture.html#a-little-bit-of-epsilon",
    "title": "Linear Algebra",
    "section": "a little bit of epsilon",
    "text": "a little bit of epsilon\n\\(\\epsilon\\) is used to mean a small, but arbitrary, number.\n\n\nExample:\n\\[\\begin{align}\ny = 2x + \\epsilon\n\\end{align}\\]\n\n\nThis means \\(y\\) is equal to \\(2\\) times \\(x\\) plus a small value. So if \\(x=3\\), then we would expect \\(y\\) to be close to \\(6\\), but not exactly \\(6\\)."
  },
  {
    "objectID": "sessions/week4_lecture.html#what-is-a-function",
    "href": "sessions/week4_lecture.html#what-is-a-function",
    "title": "Linear Algebra",
    "section": "What is a function?",
    "text": "What is a function?\nA function is a mathematical operation which maps an input value to an output value.\n\nMathematical description of a function\n\\[\\begin{align}\nf(x) = y\n\\end{align}\\]\n\n\nMaps values from a domain \\(X\\) to a range \\(Y\\).\n\\[\\begin{align}\nf(x) = y \\text{ for } x \\in X, y \\in Y\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#domain-and-range",
    "href": "sessions/week4_lecture.html#domain-and-range",
    "title": "Linear Algebra",
    "section": "Domain and range",
    "text": "Domain and range\nDomain - the set of all possible input numbers for the function\nExample:\nIn \\(f(x)=y\\), \\(x\\) is the domain.\n\nRange: the set of all possible output numbers from the function\nExample:\nIn \\(f(x)=y\\), \\(y\\) is the range."
  },
  {
    "objectID": "sessions/week4_lecture.html#number-systems",
    "href": "sessions/week4_lecture.html#number-systems",
    "title": "Linear Algebra",
    "section": "Number systems",
    "text": "Number systems\nIn the applied sciences the domain and range are typically \\(\\mathbb N\\) or \\(\\mathbb Z\\) or \\(\\mathbb R\\)\n\n\n\\(\\mathbb N\\)\n\nNatural numbers\n0,1,2,3,4,5,6…\n\n\\(\\mathbb Z\\)\n\nIntegers\n… -4, -3, -2, -1, 0, 1, 2, 3, 4, …\n\n\\(\\mathbb R\\)\n\nReal numbers"
  },
  {
    "objectID": "sessions/week4_lecture.html#data-represented-algebraically",
    "href": "sessions/week4_lecture.html#data-represented-algebraically",
    "title": "Linear Algebra",
    "section": "Data represented algebraically",
    "text": "Data represented algebraically\nAlgebra is a way of expressing numbers in a generalised or abstract form.\nExample:\n\\[\\begin{align}\nx \\in \\mathbb N\n\\end{align}\\]\n\n\nThis is the data represented algebraically.\nTypically a vector of numbers \\(X^n\\)\n\n\\[\\begin{align}\nX^n = (x_1, x_2, x_3, ... , x_n)\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#example-1",
    "href": "sessions/week4_lecture.html#example-1",
    "title": "Linear Algebra",
    "section": "Example 1",
    "text": "Example 1\nProbability density function of normal distribution\n\\[\\begin{align}\nf(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\end{align}\\]\nwhere \\(x \\in \\mathbb N\\) and \\(f(x) \\in [ 0 , 1 ]\\).\n \n\nNote\n\\([0, 1]\\) is the set of real numbers between \\(0\\) and \\(1\\), inclusive of \\(0\\) and \\(1\\).\n\n\nLast lecture we were thinking about PDFs - these are a function. They map the input value, the ‘x’ to the output range which is a probability between 0 and 1."
  },
  {
    "objectID": "sessions/week4_lecture.html#example-2",
    "href": "sessions/week4_lecture.html#example-2",
    "title": "Linear Algebra",
    "section": "Example 2",
    "text": "Example 2\nThe other function we’ve seen is a linear equation.\n\\[\\begin{align}\nf(x) = ax + b\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#linear-equation",
    "href": "sessions/week4_lecture.html#linear-equation",
    "title": "Linear Algebra",
    "section": "Linear equation",
    "text": "Linear equation\nA linear equation is a linear combination of variables.\nExamples include:\n\\[\\begin{align}\nf(x) = ax + b\n\\end{align}\\]\n\n\n“linea” is the latin word for line or string."
  },
  {
    "objectID": "sessions/week4_lecture.html#straight-lines",
    "href": "sessions/week4_lecture.html#straight-lines",
    "title": "Linear Algebra",
    "section": "Straight lines",
    "text": "Straight lines\nGraphically linear equations are straight lines."
  },
  {
    "objectID": "sessions/week4_lecture.html#linear-equations-1",
    "href": "sessions/week4_lecture.html#linear-equations-1",
    "title": "Linear Algebra",
    "section": "Linear equation(s)",
    "text": "Linear equation(s)\nWe can generalise to multiple equations.\n\nThey are:\n\nA system of multiple linear functions\nWhich can be represented by matrices\nThey can have 0, 1, or many solutions"
  },
  {
    "objectID": "sessions/week4_lecture.html#example-1-1",
    "href": "sessions/week4_lecture.html#example-1-1",
    "title": "Linear Algebra",
    "section": "Example 1",
    "text": "Example 1\n\\[\\begin{align}\nx+y=10\n\\end{align}\\]\n\n\nWhat could \\(x\\) and \\(y\\) be?\nCould have \\(x=y=5\\)\nOr \\(x=2.5\\) and \\(y=7.5\\)\n\n\n\nThere are many solutions!!!\nMany solutions = under-specified\n\nUnder-specified but simple to solve!"
  },
  {
    "objectID": "sessions/week4_lecture.html#example-2-1",
    "href": "sessions/week4_lecture.html#example-2-1",
    "title": "Linear Algebra",
    "section": "Example 2",
    "text": "Example 2\n\\[\\begin{align}\nx+y=10\n\\\\\n2x+y=15\n\\end{align}\\]\n\nIn school might have solved this using substitution.\n\nRearrange the first equation to get \\(y=10-x\\)\nSubstituting in we get \\(2x+(10-x)=15\\)\n\\(x+10=15\\) \\(\\implies\\) \\(x=5\\) \\(\\implies\\) \\(y=5\\)\n\n\n\nThere is exactly one solution!\n\nPerfectly specified and we can solve it without too much difficulty"
  },
  {
    "objectID": "sessions/week4_lecture.html#example-3",
    "href": "sessions/week4_lecture.html#example-3",
    "title": "Linear Algebra",
    "section": "Example 3",
    "text": "Example 3\n\\[\\begin{align}\nx_1+x_2+x_3+x_4=10\n\\\\ x_1+4x_2+x_3+x_4=25\n\\\\ x_1+4x_2+43x_3+x_4=37\n\\\\ x_1+4x_2+7x_3+59x_4=1073\n\\end{align}\\]\n\nVery hard to solve!\n\nAn example (when things get more complicated)"
  },
  {
    "objectID": "sessions/week4_lecture.html#matrix-notation",
    "href": "sessions/week4_lecture.html#matrix-notation",
    "title": "Linear Algebra",
    "section": "Matrix notation",
    "text": "Matrix notation\n\\[\\begin{align}\nx_1+x_2+x_3+x_4=10\n\\\\ x_1+4x_2+x_3+x_4=25\n\\\\ x_1+4x_2+43x_3+x_4=37\n\\\\ x_1+4x_2+7x_3+59x_4=1073\n\\end{align}\\]\n\nCan be written as:\n\\[\\begin{align}\n\\begin{bmatrix}1&1&1&1\\cr1&4&1&1\\cr1&4&43&1\\cr1&4&7&59\\end{bmatrix}\\begin{pmatrix}x_1\\cr x_2\\cr x_3\\cr x_4\\end{pmatrix} = \\begin{pmatrix}10\\cr 25\\cr 37\\cr 1073\\end{pmatrix}\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#generalised-matrix-form",
    "href": "sessions/week4_lecture.html#generalised-matrix-form",
    "title": "Linear Algebra",
    "section": "Generalised matrix form",
    "text": "Generalised matrix form\nThe generalised matrix form (for a 4x4 matrix is):\n\\[\\begin{align}\n\\begin{bmatrix}a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4}\\cr a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4}\\cr a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4}\\cr a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}\\end{bmatrix}\\begin{pmatrix}x_1\\cr x_2\\cr x_3\\cr x_4\\end{pmatrix} = \\begin{pmatrix}y_1 \\cr y_2 \\cr y_3 \\cr y_4\\end{pmatrix}\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#along-the-corridor-down-the-stairs",
    "href": "sessions/week4_lecture.html#along-the-corridor-down-the-stairs",
    "title": "Linear Algebra",
    "section": "Along the corridor, down the stairs",
    "text": "Along the corridor, down the stairs\n\n\nMatrices are indexed by row (\\(m\\)) and by column (\\(n\\))."
  },
  {
    "objectID": "sessions/week4_lecture.html#example",
    "href": "sessions/week4_lecture.html#example",
    "title": "Linear Algebra",
    "section": "Example",
    "text": "Example\n\\(m=2\\), \\(n=2\\) matrix:\n\\[\\begin{align}\n\\begin{bmatrix}1&1\\cr1&4\\end{bmatrix}\n\\end{align}\\]\n\n\\(m=3\\), \\(n=2\\) matrix:\n\\[\\begin{align}\n\\begin{bmatrix}1&1&2\\cr1&4&7\\end{bmatrix}\n\\end{align}\\]\n\n\n\nNote\nWhen \\(m=n\\) we have a square matrix."
  },
  {
    "objectID": "sessions/week4_lecture.html#matrix-addition",
    "href": "sessions/week4_lecture.html#matrix-addition",
    "title": "Linear Algebra",
    "section": "Matrix addition",
    "text": "Matrix addition\nWe denote matrices by capital letters: \\(A\\), \\(B\\), …\n\nMatrix addition is element-wise:\n\\[\\begin{align}\n(A+B)_{ij} = A_{ij} + B_{ij}\n\\end{align}\\]\n\n\nExample:\n\\[\\begin{align}\n\\begin{bmatrix}1&1\\cr1&4\\end{bmatrix} +\n\\begin{bmatrix}1&0\\cr2&6\\end{bmatrix} =\n\\begin{bmatrix}2&1\\cr3&10\\end{bmatrix}\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#matrix-multiplication",
    "href": "sessions/week4_lecture.html#matrix-multiplication",
    "title": "Linear Algebra",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nMatrix multiplication is row by column.\n\\[\\begin{align}\n(AB){ij} = \\sum{k} A_{ik} B_{kj}\n\\end{align}\\]\n\nExample:\n\\[\\begin{align}\n\\begin{bmatrix}1 & 2\\cr 3 & 4\\end{bmatrix}\n\\begin{bmatrix}5 & 6\\cr 7 & 8\\end{bmatrix}\n=\n\\begin{bmatrix}\n1\\cdot 5 + 2\\cdot 7 & 1\\cdot 6 + 2\\cdot 8 \\cr\n3\\cdot 5 + 4\\cdot 7 & 3\\cdot 6 + 4\\cdot 8\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n19 & 22 \\cr 43 & 50\n\\end{bmatrix}\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#identity-matrix",
    "href": "sessions/week4_lecture.html#identity-matrix",
    "title": "Linear Algebra",
    "section": "Identity matrix",
    "text": "Identity matrix\nThe identity matrix \\(I\\) acts like the number \\(1\\) in multiplication.\nFor any compatible matrix \\(A\\):\n\\[\\begin{align}\nAI = IA = A\n\\end{align}\\]\n\nExample:\n\\[\\begin{align}\nI = \\begin{bmatrix}\n1 & 0 & 0 \\cr\n0 & 1 & 0 \\cr\n0 & 0 & 1\n\\end{bmatrix}\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#determinant-of-a-matrix",
    "href": "sessions/week4_lecture.html#determinant-of-a-matrix",
    "title": "Linear Algebra",
    "section": "Determinant of a matrix",
    "text": "Determinant of a matrix\nThe determinant of a square matrix \\(A\\) is a scalar value that gives information about:\n\nWhether \\(A\\) is invertible\n\nHow \\(A\\) scales space (volume/area)\n\nOrientation (positive or negative)\n\nWe write this as \\(\\det(A)\\) or \\(|A|\\)."
  },
  {
    "objectID": "sessions/week4_lecture.html#determinant-of-a-22-matrix",
    "href": "sessions/week4_lecture.html#determinant-of-a-22-matrix",
    "title": "Linear Algebra",
    "section": "Determinant of a 2×2 matrix",
    "text": "Determinant of a 2×2 matrix\nFor\n\\[\\begin{align}\nA = \\begin{bmatrix}\na & b \\cr\nc & d\n\\end{bmatrix}\n\\end{align}\\]\nthe determinant is:\n\\[\\begin{align}\n\\det(A) = ad - bc\n\\end{align}\\]\n\nExample:"
  },
  {
    "objectID": "sessions/week4_lecture.html#inverse-matrix",
    "href": "sessions/week4_lecture.html#inverse-matrix",
    "title": "Linear Algebra",
    "section": "Inverse matrix",
    "text": "Inverse matrix\nThe inverse of a square matrix \\(A\\) is denoted \\(A^{-1}\\) and satisfies:\n\\[\\begin{align}\nAA^{-1} = A^{-1}A = I\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#inverse-matrix-2x2",
    "href": "sessions/week4_lecture.html#inverse-matrix-2x2",
    "title": "Linear Algebra",
    "section": "Inverse matrix (2x2)",
    "text": "Inverse matrix (2x2)\nFor a \\(2 \\times 2\\) matrix A:\n\\[\\begin{align}\nA = \\begin{bmatrix}\na & b \\cr\nc & d\n\\end{bmatrix}\n\\end{align}\\]\nif \\(\\det(A) \\neq 0\\), then the inverse is:\n\\[\\begin{align}\nA^{-1} = \\frac{1}{\\det(A)}\n\\begin{bmatrix}\nd & -b \\cr\n-c & a\n\\end{bmatrix},\n\\quad \\text{where } \\det(A) = ad - bc\n\\end{align}\\]\n\nIf \\(\\det(A) = 0\\), the matrix has no inverse."
  },
  {
    "objectID": "sessions/week4_lecture.html#system-of-equations",
    "href": "sessions/week4_lecture.html#system-of-equations",
    "title": "Linear Algebra",
    "section": "System of equations",
    "text": "System of equations\nRecall that a system of linear equations can be written compactly as:\n\\[\\begin{align}\nAx = y\n\\end{align}\\]\nwhere: - \\(A\\) is the coefficient matrix - \\(x\\) is the vector of unknowns - \\(y\\) is the vector of constants"
  },
  {
    "objectID": "sessions/week4_lecture.html#solving-the-system",
    "href": "sessions/week4_lecture.html#solving-the-system",
    "title": "Linear Algebra",
    "section": "Solving the system",
    "text": "Solving the system\nIf \\(A\\) is invertible (i.e. \\(\\det(A) \\neq 0\\)), we can solve for \\(x\\):\n\\[\\begin{align}\nAx &= y \\\\\nA^{-1}Ax &= A^{-1}y \\\\\nIx &= A^{-1}y \\\\\nx &= A^{-1}y\n\\end{align}\\]\n\nThus, the solution exists and is unique whenever \\(A\\) has an inverse."
  },
  {
    "objectID": "sessions/week4_lecture.html#so-whats-does-it-mean",
    "href": "sessions/week4_lecture.html#so-whats-does-it-mean",
    "title": "Linear Algebra",
    "section": "So whats does it mean?",
    "text": "So whats does it mean?\n\n\n\nTaken from: Chiou, Jou, & Yang, (2015). Factors affecting public transportation usage rate: Geographically weighted regression. Transportation Research Part A: Policy and Practice."
  },
  {
    "objectID": "sessions/week4_lecture.html#take-another-look",
    "href": "sessions/week4_lecture.html#take-another-look",
    "title": "Linear Algebra",
    "section": "Take another look",
    "text": "Take another look\nLink to the paper…"
  },
  {
    "objectID": "sessions/week4_lecture.html#writing-the-equation",
    "href": "sessions/week4_lecture.html#writing-the-equation",
    "title": "Linear Algebra",
    "section": "Writing the equation",
    "text": "Writing the equation\nEquation 1:\n\\[\\begin{align}\ny_i = \\beta_0(u_i, v_i) + \\sum_{k=1}^p \\beta_{ik}(u_i, v_i)x_{ik} + \\epsilon_i\n\\end{align}\\]\nEquation 2:\n\\[\\begin{align}\n\\hat{\\beta}(i) = [X^TW(i)X]^{-1}X^TW(i)Y\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week4_lecture.html#equation-1",
    "href": "sessions/week4_lecture.html#equation-1",
    "title": "Linear Algebra",
    "section": "Equation 1",
    "text": "Equation 1\n\\[\\begin{align}\ny_i = \\beta_0(u_i, v_i) + \\sum_{k=1}^p \\beta_{ik}(u_i, v_i)x_{ik} + \\epsilon_i\n\\end{align}\\]\nwhere:\n\n\\(y_i\\): the outcome (response) for observation \\(i\\)\n\\(\\beta_0(u_i,v_i)\\): the intercept, which can vary with location \\((u_i,v_i)\\)\n\\(\\sum_{k=1}^p \\beta_{ik}(u_i,v_i) x_{ik}\\): the weighted sum of predictors \\(x_{ik}\\), where each predictor has its own coefficient that may depend on \\((u_i,v_i)\\)\n\\(\\epsilon_i\\): the error term for observation \\(i\\)\n\n\nRead the equation out"
  },
  {
    "objectID": "sessions/week4_lecture.html#translating-equation-1",
    "href": "sessions/week4_lecture.html#translating-equation-1",
    "title": "Linear Algebra",
    "section": "Translating equation 1",
    "text": "Translating equation 1\n\\[\\begin{align}\ny_i = \\beta_0(u_i, v_i) + \\sum_{k=1}^p \\beta_{ik}(u_i, v_i)x_{ik} + \\epsilon_i\n\\end{align}\\]\nThe outcome \\(y_i\\) is explained by an intercept and a weighted combination of predictors, with coefficients that may change depending on the location \\((u_i,v_i)\\), plus some error."
  },
  {
    "objectID": "sessions/week4_lecture.html#equation-2",
    "href": "sessions/week4_lecture.html#equation-2",
    "title": "Linear Algebra",
    "section": "Equation 2",
    "text": "Equation 2\n\\[\\begin{align}\n\\hat{\\beta}(i) = [X^TW(i)X]^{-1}X^TW(i)Y\n$\\hat{\\beta}(i)$: the estimated coefficients at location $i$\n\\end{align}\\]\nwhere:\n\n\\(X\\): the matrix of predictor variables\n\\(Y\\): the vector of observed outcomes\n\\(W(i)\\): a weight matrix that depends on location \\(i\\)\n\\(X^T\\): the transpose of \\(X\\)\n\\([X^TW(i)X]^{-1}\\): the inverse of the weighted cross-product matrix\n\n\nRead the equation out"
  },
  {
    "objectID": "sessions/week4_lecture.html#translating-equation-2",
    "href": "sessions/week4_lecture.html#translating-equation-2",
    "title": "Linear Algebra",
    "section": "Translating equation 2",
    "text": "Translating equation 2\n\\[\\begin{align}\n\\hat{\\beta}(i) = [X^TW(i)X]^{-1}X^TW(i)Y\n$\\hat{\\beta}(i)$: the estimated coefficients at location $i$\n\\end{align}\\]\nThe estimated coefficients \\(\\hat{\\beta}(i)\\) are obtained by solving a weighted least squares problem: take the predictors \\(X\\), weight them with \\(W(i)\\), and solve for the coefficients that best fit \\(Y\\)."
  },
  {
    "objectID": "sessions/week4_lecture.html#covered",
    "href": "sessions/week4_lecture.html#covered",
    "title": "Linear Algebra",
    "section": "Covered",
    "text": "Covered\nWe’ve covered:\n\nMathematical notation\nSums and Products\nFunctions\nMatrices\nAlgebraic representations"
  },
  {
    "objectID": "sessions/week4_lecture.html#key-takeaways",
    "href": "sessions/week4_lecture.html#key-takeaways",
    "title": "Linear Algebra",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nCan use mathematical notation to write equations in a univeral language.\nLinear algebra helps us to solve systems of linear equations.\n\n\nIf in doubt:\n\n\nUse the maths cheat sheet!"
  },
  {
    "objectID": "sessions/week3_practical.html",
    "href": "sessions/week3_practical.html",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "",
    "text": "This week is focussed on defining research hypotheses, and using statistical tests to evaluate them. In particular we will use the Student’s T-test, and the KS distribution test."
  },
  {
    "objectID": "sessions/week3_practical.html#loading-the-data",
    "href": "sessions/week3_practical.html#loading-the-data",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "Loading the data",
    "text": "Loading the data\nWe are going to look at schools perfomance data in England once again.\nThe data is sourced from gov.uk here - HOWEVER I have also saved a copy of the relevant dataset to the Github repo (in case the dataset is removed from the website) which you can load directly in the code below. In this notebook we’re using the performance table for academic year 2022/23: ‘Performancetables_130242/2022-2023’.\nIf you do want to download the data directly from the gov.uk website then you need to choose the academic year ‘2022 to 2023’, then ‘All of England’, then ‘Key stage 4 results (final)’. Then you’ll want to download the ‘Data in CSV format’ and also ‘Explanation of terminology used in the data files’.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as sps\nimport numpy as np \n\n# Read CSV file, handling common missing value entries\nna_vals = [\"\", \"NA\", \"SUPP\", \"NP\", \"NE\", \"SP\", \"SN\", \"SUPPMAT\"]\ndf_ks4 = pd.read_csv('https://raw.githubusercontent.com/huanfachen/QM/refs/heads/main/sessions/L2_data/england_ks4final.csv',\n    na_values = na_vals\n)\n\n/tmp/ipykernel_23793/2179567837.py:8: DtypeWarning:\n\nColumns (75,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,144,145,146,147,148,149,150,151,152,177,178,179,180,181,182,183,186,187,188,189,190,191,192,194,195,196,198,199,200,202,203,204,206,207,208,210,211,212,214,215,216,218,219,220,222,223,224,230,233,234,235,236,237,238,239,242,243,244,245,246,247,248,251,252,253,254,255,256,257,266,267,268,269,270,271,272,281,282,283,284,285,286,287,296,297,298,299,300,301,302,311,312,313,314,315,316,317,335,336,337,340,341,342,345,346,347,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\nFollowing the same steps as last week we’re going to do some initial data pre-processing:\n\ninfo_cols = ['RECTYPE', 'LEA', 'SCHNAME', 'TOTPUPS', 'TOWN']\nebaccs_cols = ['EBACCAPS', 'EBACCAPS_GIRLS', 'EBACCAPS_BOYS']\n\ndf = df_ks4[info_cols + ebaccs_cols]\n\n# make a copy of the dataframe to avoid SettingWithCopyWarning\ndf = df.copy()\n\ndf.loc[:, ebaccs_cols] = df.loc[:, ebaccs_cols].apply(pd.to_numeric, errors='coerce')\ndf['TOTPUPS'] = pd.to_numeric(df['TOTPUPS'], errors='coerce').fillna(0).astype('int64')\n\ndf = df[df['RECTYPE'].isin([1, 2])]\n\ndf.head()\n\n\n\n\n\n\n\n\nRECTYPE\nLEA\nSCHNAME\nTOTPUPS\nTOWN\nEBACCAPS\nEBACCAPS_GIRLS\nEBACCAPS_BOYS\n\n\n\n\n0\n1\n201.0\nCity of London School\n1045\nLondon\n2.10\nNaN\n2.10\n\n\n1\n1\n201.0\nCity of London School for Girls\n739\nLondon\n1.51\n1.51\nNaN\n\n\n2\n1\n201.0\nDavid Game College\n365\nNaN\n0.56\n0.46\n0.60\n\n\n4\n1\n202.0\nAcland Burghley School\n1163\nLondon\n4.62\n4.05\n4.82\n\n\n5\n1\n202.0\nThe Camden School for Girls\n1047\nLondon\n6.25\n6.25\nNaN\n\n\n\n\n\n\n\nLooking at the metadata (which you can see in ‘L2_data/ks4_meta.xlsx’) we can see the full meaning of each column header:\n\n‘RECTYPE’ = Record type (1=mainstream school; 2=special school; 4=local authority; 5=National (all schools); 7=National (maintained schools))\n‘LEA’ = Local authority\n‘SCHNAME’ = School name\n‘TOTPUPS’ = Number of pupils on roll (all ages)\n‘TOWN’ = School town\n‘EBACCAPS’ = Average EBacc APS score per pupil\n‘EBACCAPS_GIRLS’ = Average EBacc APS score per girl\n‘EBACCAPS_BOYS’ = Average EBacc APS score per boy"
  },
  {
    "objectID": "sessions/week3_practical.html#research-question",
    "href": "sessions/week3_practical.html#research-question",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "Research question",
    "text": "Research question\nThe department for education is worried about regional inequality in school grades. With this in mind they’ve come up with a research question they’d like to address.\nResearch question: Is average pupil attainment on the EBacc significantly different in London compared to the rest of England?\nTo do this we’re going to use the mean comparison test to compare the schools in London to those outside of London."
  },
  {
    "objectID": "sessions/week3_practical.html#preparing-the-data",
    "href": "sessions/week3_practical.html#preparing-the-data",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "Preparing the data",
    "text": "Preparing the data\n\nSplitting the groups\nFor simplicity let’s create two dataframes for the two different areas.\n\ndf_London = df[df['TOWN'] == 'London']\ndf_London = df_London[df_London['EBACCAPS'].notna()]\n\ndf_notLondon = df[df['TOWN'] != 'London']\ndf_notLondon = df_notLondon[df_notLondon['EBACCAPS'].notna()]\n\nAnd look at the summary statistics for each group.\n\ndf_London['EBACCAPS'].describe()\n\ncount    385.000000\nmean       3.788260\nstd        1.851894\nmin        0.000000\n25%        3.020000\n50%        4.290000\n75%        4.970000\nmax        8.700000\nName: EBACCAPS, dtype: float64\n\n\n\ndf_notLondon['EBACCAPS'].describe()\n\ncount    4246.000000\nmean        3.395921\nstd         1.659090\nmin         0.000000\n25%         2.820000\n50%         3.670000\n75%         4.380000\nmax         8.560000\nName: EBACCAPS, dtype: float64\n\n\nFrom looking at the summary statistics the two groups are different sizes. The two groups also have different means - but we want to test if these means are statistically-significantly different."
  },
  {
    "objectID": "sessions/week3_practical.html#the-hypothesis-test",
    "href": "sessions/week3_practical.html#the-hypothesis-test",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "The hypothesis test",
    "text": "The hypothesis test\nWe’re now going to work through the steps of the hypothesis test according to the five steps discussed in the lecture:\n\nDefine the null and alternative hypothesis\nSet the significance level\nIdentify the evidence\nCalculate the p-value\nCompare p-value with hypothesis level\n\n\nStep 1\nWhat is the null and alternative hypothesis?\n\nQuestionAnswer\n\n\nH_0 = '??'\nH_1 = '??'\n\nprint(f'The null hypothesis is {H_0}')\nprint(f'The alternative hypothesis is {H_1}')\n\n\nH_0 = 'Mean EBacc score in London = Mean EBacc score outside London'\nH_1 = 'Mean EBacc score in London &gt; Mean EBacc score outside London OR Mean EBacc score in London &lt; Mean EBacc score outside London'\n\nprint(f'The null hypothesis is {H_0}')\nprint(f'The alternative hypothesis is {H_1}')\nThe null hypothesis is Mean EBacc score in London = Mean EBacc score outside London\nThe alternative hypothesis is Mean EBacc score in London &gt; Mean EBacc score outside London OR Mean EBacc score in London &lt; Mean EBacc score outside London\n\n\n\n\n\nStep 2\nSet the significance level.\n\n# Set the level of statistical significance \n\nalpha = 0.05\n\n\n\nStep 3\nIdentify the evidence.\nWe already have the evidence - it’s our datasets df_London['EBACCAPS'] and df_notLondon['EBACCAPS'].\n\n\nStep 4\nCalculate the p-value.\nThe statistical test we’re using here is Student’s T-test for two samples. We can use a built in function from scipy.stats called ttest_ind to do the statistical test for us. You can read more about this function here.\nStrictly, Student’s T-test for two-samples assumes both samples have equal standard deviations. However, practically the test is robust as long as the standard deviations are similar - a heuristic being that as long as neither standard deviation is double the other, we’re ok to use the test.\n\nLondon_std = df_London['EBACCAPS'].std()\nnotLondon_std = df_notLondon['EBACCAPS'].std()\n\n# Calculate the ratio of standard deviations \nstd_ratio = London_std/notLondon_std\n\nprint(\"std ratio =\", std_ratio)\n\nif std_ratio &gt; 0.5 and std_ratio &lt; 2:\n    print(\"Can assume equal population standard deviations.\")\n    equal_stds = True\nelse:\n    print(\"Cannot assume equal population standard deviations.\")\n    equal_stds = False\n\nstd ratio = 1.1162102116458004\nCan assume equal population standard deviations.\n\n\nThere are two outputs from the function scipy.stats.ttest_ind: the test statistic and the p value.\n\ntest_stat, p_value = sps.ttest_ind(df_London['EBACCAPS'], df_notLondon['EBACCAPS'], equal_var = equal_stds)\n\nprint(\"test statistic = \", test_stat)\nprint(\"p-value =\", p_value)\n\ntest statistic =  4.398340904538903\np-value = 1.1153058452679495e-05\n\n\n\n\nStep 5\nCompare p-value with hypothesis level.\nFor the final step we compare the p-value to the significance value in order to reach a decision.\n\nQuestionAnswer\n\n\nif p_value ?? ?? :\n    print(f\"Reject the null hypothesis ({H_0}). Accept the alternative hypothesis ({H_1}).\")\n    print(\"Conclude that samples are drawn from populations with different means.\")\nelif p_value ?? ?? :\n    print(f\"No significant evidence to reject the null hypothesis ({H_0}).\")\n    print(\"Assume samples are drawn from populations with the same mean.\")\n\n\nif p_value &lt; alpha:\n    print(f\"Reject the null hypothesis ({H_0}). Accept the alternative hypothesis ({H_1}).\")\n    print(\"Conclude that samples are drawn from populations with different means.\")\nelif p_value &gt;= alpha:\n    print(f\"No significant evidence to reject the null hypothesis ({H_0}).\")\n    print(\"Assume samples are drawn from populations with the same mean.\")\nReject the null hypothesis (Mean EBacc score in London = Mean EBacc score outside London). Accept the alternative hypothesis (Mean EBacc score in London &gt; Mean EBacc score outside London OR Mean EBacc score in London &lt; Mean EBacc score outside London).\nConclude that samples are drawn from populations with different means.\n\n\n\nHence we can conclude that the evidence supports there is a statistically significant difference between the mean student attainment on the EBacc in London, versus outside of London."
  },
  {
    "objectID": "sessions/week3_practical.html#a-more-complicated-research-question",
    "href": "sessions/week3_practical.html#a-more-complicated-research-question",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "A more complicated research question",
    "text": "A more complicated research question\nNow I’d like to know, are the EBacc scores for boys and girls in England distributed similarly?\nIn the dataset we have the average EBacc score for each school split by gender. To address whether these two samples come from the same distribution we’re going to use the Kolmogorov-Smirnov two sample test."
  },
  {
    "objectID": "sessions/week3_practical.html#preparing-the-data-1",
    "href": "sessions/week3_practical.html#preparing-the-data-1",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "Preparing the data",
    "text": "Preparing the data\n\nSummary statistics\nAs before, let’s start by looking at the summary statistics.\n\nQuestionAnswer\n\n\ndf[[??]].??\n\n\ndf[['EBACCAPS_GIRLS', 'EBACCAPS_BOYS']].describe()\n\n\n\n\n\n\nEBACCAPS_GIRLS\nEBACCAPS_BOYS\n\n\n\n\ncount\n3851.000000\n3801.000000\n\n\nmean\n3.820223\n3.492636\n\n\nstd\n1.409632\n1.395525\n\n\nmin\n0.000000\n0.000000\n\n\n25%\n3.290000\n2.980000\n\n\n50%\n3.950000\n3.630000\n\n\n75%\n4.610000\n4.250000\n\n\nmax\n8.700000\n8.560000\n\n\n\n\n\n\n\n\n\n\n\n\nEBACCAPS_GIRLS\nEBACCAPS_BOYS\n\n\n\n\ncount\n3851.000000\n3801.000000\n\n\nmean\n3.820223\n3.492636\n\n\nstd\n1.409632\n1.395525\n\n\nmin\n0.000000\n0.000000\n\n\n25%\n3.290000\n2.980000\n\n\n50%\n3.950000\n3.630000\n\n\n75%\n4.610000\n4.250000\n\n\nmax\n8.700000\n8.560000\n\n\n\n\n\n\n\n\n\n\nPlotting the data\nLet’s also visually inspect the data by plotting a histogram.\n\nplt.hist(df['EBACCAPS_GIRLS'], bins=20, color='#abc766', alpha=0.5, label='Girls')\nplt.hist(df['EBACCAPS_BOYS'], bins=20, color='#4e3c56', alpha=0.5, label='Boys')\nplt.xlim(0,9) # the EBacc has a maximum score of 9\nplt.xlabel('EBacc Score')\nplt.ylabel('Number of Schools')\nplt.title('Distribution of EBacc Scores')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nFrom visually looking at the plot we can see that the two distributions look quite similar - but we want to formally test that. As before, we’re going to follow the five steps of hypothesis testing.\n\n\nStep 1\nWhat is the null and alternative hypothesis?\n\nQuestionAnswer\n\n\nH_0 = '??'\nH_1 = '??'\n\nprint(f'The null hypothesis is {H_0}')\nprint(f'The alternative hypothesis is {H_1}')\n\n\nH_0 = 'Mean EBacc score for Girls is drawn from the same distribution as the mean EBacc score for Boys.'\nH_1 = 'Mean EBacc score for Girls is drawn from a different distirbution to the mean EBacc score for Boys.'\n\nprint(f'The null hypothesis is {H_0}')\nprint(f'The alternative hypothesis is {H_1}')\nThe null hypothesis is Mean EBacc score for Girls is drawn from the same distribution as the mean EBacc score for Boys.\nThe alternative hypothesis is Mean EBacc score for Girls is drawn from a different distirbution to the mean EBacc score for Boys.\n\n\n\n\n\nStep 2\nSet the significance level.\n\nQuestionAnswer\n\n\n# Set the level of statistical significance \n\nalpha = ??\n\n\n# Set the level of statistical significance \n\nalpha = 0.05\n\n\n\n\n\nStep 3\nIdentify the evidence.\nWe already have the evidence - it’s our datasets df['EBACCAPS_GIRLS'] and df['EBACCAPS_BOYS'].\n\n\nStep 4\nCalculate the p-value.\nThe statistical test we’re going to use here is the Kolmogorov Smirnov test. We can use a built in function from scipy.stats called ks_2samp to do step 4 for us. You can read more about this function here.\nThere are two outputs from the function scipy.stats.ks_2samp: the test statistic and the p value.\n\nQuestionAnswer\n\n\n\n# first define dataset so as to ignore 'na' values\ndf_girls = df[df['EBACCAPS_GIRLS'].notna()]['EBACCAPS_GIRLS']\ndf_boys = df[df['EBACCAPS_BOYS'].notna()]['EBACCAPS_BOYS']\n\nks_stat, ks_p_value = sps.ks_2samp(??, ??) \n\nprint(\"KS test statistic = \", ks_stat)\nprint(\"KS p-value =\", ks_p_value)\n\n\n# first define dataset so as to ignore 'na' values\ndf_girls = df[df['EBACCAPS_GIRLS'].notna()]['EBACCAPS_GIRLS']\ndf_boys = df[df['EBACCAPS_BOYS'].notna()]['EBACCAPS_BOYS']\n\nks_stat, ks_p_value = sps.ks_2samp(df_girls, df_boys) \n\nprint(\"KS test statistic = \", ks_stat)\nprint(\"KS p-value =\", ks_p_value)\nKS test statistic =  0.14379151408924834\nKS p-value = 5.931389311696697e-35\n\n\n\n\n\nStep 5\nCompare p-value with hypothesis level.\nFor the final step we compare the p-value to the significance value in order to reach a decision.\n\nQuestionAnswer\n\n\nif ??\n    print(??)\nelse ?? \n    print(??)\n\n\nif ks_p_value &lt; alpha:\n    print(f\"Reject the null hypothesis that the samples are drawn from the same distribution.\")\n    print(\"Conclude that samples are drawn from populations with different distributions.\")\nelse: \n    print(f\"Fail to reject the null hypothesis that the samples are drawn from the same distribution.\")\n    print(\"Assume that samples are drawn from populations with the same distribution.\")\nReject the null hypothesis that the samples are drawn from the same distribution.\nConclude that samples are drawn from populations with different distributions."
  },
  {
    "objectID": "sessions/week3_practical.html#extension",
    "href": "sessions/week3_practical.html#extension",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "Extension",
    "text": "Extension\nIf you’ve finished working through the examples in the tutorial then have a go at coming up with your own research question and hypothesis.\nA good place to start would be by looking at the data in df_ks4 - theres a lot of numerical information here - are there two variables you could compare?\n\ndf_ks4.hist(bins=50, figsize=(20, 15))\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "sessions/week3_practical.html#youre-done",
    "href": "sessions/week3_practical.html#youre-done",
    "title": "Practical 3: Establishing and testing the hypothesis",
    "section": "You’re Done!",
    "text": "You’re Done!\nWell done you’ve completed this weeks practical on establishing and evaluating hypothesis questions. If you are still working on it, take your time. If you have any questions just ask!"
  },
  {
    "objectID": "sessions/week3.html",
    "href": "sessions/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "This week will introduce hypothesis testing - firstly how do we define research questions and hypotheses, and secondly, what are the key statistical tests for evaluating hypotheses.\nThis week builds on the statistical concepts introduced in week 2. If you’re new to hypothesis testing then I’d recommend having a look at some of the resources below.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#introduction",
    "href": "sessions/week3.html#introduction",
    "title": "Week 3",
    "section": "",
    "text": "This week will introduce hypothesis testing - firstly how do we define research questions and hypotheses, and secondly, what are the key statistical tests for evaluating hypotheses.\nThis week builds on the statistical concepts introduced in week 2. If you’re new to hypothesis testing then I’d recommend having a look at some of the resources below.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#learning-objectives",
    "href": "sessions/week3.html#learning-objectives",
    "title": "Week 3",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nEstablish a hypothesis for a given research project.\nDefine the Type I and Type II errors.\nEvaluate a hypothesis using appropriate statistical tests.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#lecture",
    "href": "sessions/week3.html#lecture",
    "title": "Week 3",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#quiz",
    "href": "sessions/week3.html#quiz",
    "title": "Week 3",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#practical",
    "href": "sessions/week3.html#practical",
    "title": "Week 3",
    "section": "Practical",
    "text": "Practical\nTo access the practical:\n\nPreview\nDownload\n\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#further-resources",
    "href": "sessions/week3.html#further-resources",
    "title": "Week 3",
    "section": "Further resources",
    "text": "Further resources\n\nIf you’re new to hypothesis testing then this Khan Academy course might be helpful: https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample\nIf you want to refresh on hypothesis testing this chapter is helpful: https://us.sagepub.com/sites/default/files/upm-binaries/40007_Chapter8.pdf - note that is it largely framed in terms of the behavioural sciences, and focuses on statistical tests commonly used in the behavioural sciences.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week2_lecture.html#last-week",
    "href": "sessions/week2_lecture.html#last-week",
    "title": "Exploratory Data Analysis 2",
    "section": "Last week",
    "text": "Last week\nOverview of lecture 1\n\nDifferent data types\nKey data metrics\n\nthe middle and the spread\n\nVisualising the data\nData outliers\n\n\n4 data types: nominal, ordinal, interval, ratio Numerical vs categorical\nKey data metrics - the middle: mean, median, mode and the spread: variance and standard deviation\ndifferent types of data outliers: caused by measurement error, or irregular pattern or influential outliers"
  },
  {
    "objectID": "sessions/week2_lecture.html#the-scientific-method",
    "href": "sessions/week2_lecture.html#the-scientific-method",
    "title": "Exploratory Data Analysis 2",
    "section": "The scientific method",
    "text": "The scientific method\n\n\n\n\nA specific research model - quite formal - been around from the 19th century. Typifies how scientific research is done in the classical sciences like biology. Link to scientific process and how we test and then iterate over ideas and concepts. Something we will talk about more next lecture."
  },
  {
    "objectID": "sessions/week2_lecture.html#exploratory-data-analysis",
    "href": "sessions/week2_lecture.html#exploratory-data-analysis",
    "title": "Exploratory Data Analysis 2",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nWhen we’re working with large datasets, exploratory data analysis is the first step in the scientific method.\n\nHow to understand the dataset\nWhat do the variables represent\nWhat statistical techniques should be used\n\n\nMore modern approach approach devloped by computer scientist John Turkey in the 1970s - whilst at Princeton. ‘Father of data science’. rather than being driven by more classical statistical analyses (something we’ll look at next week) - the idea was to have a more flexible approach."
  },
  {
    "objectID": "sessions/week2_lecture.html#introducing-statistical-concepts",
    "href": "sessions/week2_lecture.html#introducing-statistical-concepts",
    "title": "Exploratory Data Analysis 2",
    "section": "Introducing statistical concepts",
    "text": "Introducing statistical concepts\n\nData science is about using ideas from statistics to describe large datasets\nFocus on numerical data\nUsing probability distributions to characterise them\n\n\nIn large datasets its not practical to describe every data point - instead interested in the overall pattern. Moreover maybe that general pattern is more interetsing"
  },
  {
    "objectID": "sessions/week2_lecture.html#learning-objectives",
    "href": "sessions/week2_lecture.html#learning-objectives",
    "title": "Exploratory Data Analysis 2",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this lecture you should be able to:\n\nDescribe the characteristic features of common probability distributions.\nCalculate exponentials and logarithms.\nEvaluate whether a dataset is representative."
  },
  {
    "objectID": "sessions/week2_lecture.html#the-dream-vs-reality",
    "href": "sessions/week2_lecture.html#the-dream-vs-reality",
    "title": "Exploratory Data Analysis 2",
    "section": "The dream vs reality",
    "text": "The dream vs reality\nIdeally, we would like all the relevant data.\n\n… in reality we normally only have some.\n\n\n\n\nIt would be great if I knew the height of everyone at UCL, but unrealistic to collect – however maybe I could collect all students in this lecture theatre."
  },
  {
    "objectID": "sessions/week2_lecture.html#approximating",
    "href": "sessions/week2_lecture.html#approximating",
    "title": "Exploratory Data Analysis 2",
    "section": "Approximating",
    "text": "Approximating\nHence, we sample a subset of the data. We need to choose our sample carefully - we want what happens in the sample to approximate what happens in the whole population.\n\nIn practise we might try different sampling approaches such as:\n\nrandom sampling\nsystematic sampling\n\n\nI might assign everyone a number and then choose 100 random numbers and measure them. Or I might choose a specific subset, for exampel just everyone in this lecture theatre."
  },
  {
    "objectID": "sessions/week2_lecture.html#bias",
    "href": "sessions/week2_lecture.html#bias",
    "title": "Exploratory Data Analysis 2",
    "section": "Bias",
    "text": "Bias\nIt’s important to understand if your dataset is unrepresentative or biased.\n\n\n\n\nBias is the idea that there is an unrepresentative pattern in your data."
  },
  {
    "objectID": "sessions/week2_lecture.html#cognitive-bias",
    "href": "sessions/week2_lecture.html#cognitive-bias",
    "title": "Exploratory Data Analysis 2",
    "section": "Cognitive bias",
    "text": "Cognitive bias\nSystematic patterns in how we think about, and perceive, the world.\n\n\nOur cognitive biases can impact:\n\ndata collection\ndata selection\ndata processing\nmodelling choices\n\n\n\n\n\n\n\n\nWe all have our own cognitive biases - shaped around our values and our backgrounds. Example of cognitive bias - confirmation bias, whereby I favour information that agrees with my existing beliefs. I’m writing a report on the spatial impact of a new railway line. Suppose I hate the idea of a new railway line, and then I see some data, fact a: 60% of residents living near the railline don’t want it due to noise. fact b: 75% of residents in the UK do want the railway line. i might only include fact a in my report as it confirms my existing beliefs."
  },
  {
    "objectID": "sessions/week2_lecture.html#why-is-this-important",
    "href": "sessions/week2_lecture.html#why-is-this-important",
    "title": "Exploratory Data Analysis 2",
    "section": "Why is this important?",
    "text": "Why is this important?\nIf we’re not careful we can propagate bias to the research, and hence results.\n\nThis can lead to incorrect conclusions.\n\nAs scientists we should try and present an accurate and fair balance of information. If we’re more aware of our biases it can be easier to try and actively avoid them."
  },
  {
    "objectID": "sessions/week2_lecture.html#dataset-bias",
    "href": "sessions/week2_lecture.html#dataset-bias",
    "title": "Exploratory Data Analysis 2",
    "section": "Dataset bias",
    "text": "Dataset bias\n\nParticularly important when thinking about analysing large datasets as there could be non-obvious patterns reflecting biases.\nMore important in the era of large AI models\n\n\nTypes of dataset bias include: - Historical bias - Selection bias\n\nAlready talked about the idea of cognitive bias. There are lots and lots of different types of biases - going to talk a little about different ways they can bias the dataset.\nDataset bias has become a more prevalent topic of concern on the era of large AI models. These models are really good at learnign patterns which aren’t obvious to the human eye - but the pattern might reflect a bias. For example generative AI models learn that scientists are white men, even though if I asked you you’d tell me anyone can be a scientist.\n\nHungry judges"
  },
  {
    "objectID": "sessions/week2_lecture.html#historical-bias",
    "href": "sessions/week2_lecture.html#historical-bias",
    "title": "Exploratory Data Analysis 2",
    "section": "Historical bias",
    "text": "Historical bias\nReflects existing, real world, inequalities\nExamples:\n\nPolice profiling\n\nAutomated tools to detect ‘criminals’.\nTrained on datasets which reflect current racist practises.\n\n\n\nPolice profiling models - meant to make easy to spot criminals - but use exisitng crime data - which reflect racist prejudices in society - and the model learns to pick out people of certain ethnic backgrounds.\nTools also used to help the sentnecing of criminals in courts of law - observe the hungry judges effect, where they sentnece differently based on how hungry they are - tend to be harsher just before lunch and more lenient after lunch."
  },
  {
    "objectID": "sessions/week2_lecture.html#selection-bias",
    "href": "sessions/week2_lecture.html#selection-bias",
    "title": "Exploratory Data Analysis 2",
    "section": "Selection bias",
    "text": "Selection bias\nWhen the sample chosen doesn’t represent the whole population of interest\nExamples:\n\nSelf selection Roy Model\n\nUnderlying characteristics of people who self select into certain groups.\n\nWEIRD people\n\nCommonly sampled in behavioural sciences.\nReflects a very small proportion of global population.\n\n\n\nRoy model - example from economics - basically there’s not a random selection of people in a specific job - thye have chosen that job based on underlying unobserved characterisitcs. For example suppose I’m interested in wages of workers in different occupations - but they have self-selected into that occupation based on their unique skill set - so it’s a biased representation.\nWEIRD = western, educated, industrialised, rich, democratic"
  },
  {
    "objectID": "sessions/week2_lecture.html#can-data-ever-be-truly-representative",
    "href": "sessions/week2_lecture.html#can-data-ever-be-truly-representative",
    "title": "Exploratory Data Analysis 2",
    "section": "Can data ever be truly representative?",
    "text": "Can data ever be truly representative?\nProbably not.\n\nEven when we think we have really good data, someone has made the choice to collect this data. Why that data over other data? Does that data reflect their underlying cognitive biases? For example the choice to collect information of peoples opnion about a new trainline might be driven by a political goal of understanding public support for a new government infrastructure project.\nNot necessarilly a bad thing, but important to keep in mind. And if the bias is noticeable then it should be acknowledged and reported.\n\n\nFailing that… .. we can acknowledge our biases!\n\n\n\nImage credit: [xkcd](https://xkcd.com/2494/)"
  },
  {
    "objectID": "sessions/week2_lecture.html#what-to-declare",
    "href": "sessions/week2_lecture.html#what-to-declare",
    "title": "Exploratory Data Analysis 2",
    "section": "What to declare",
    "text": "What to declare\nDescriptive statistics refers to the most basic statistical information about the dataset.\n\n\nSample size (n)\nMean, median, mode\nStandard deviation\nRange"
  },
  {
    "objectID": "sessions/week2_lecture.html#example-1",
    "href": "sessions/week2_lecture.html#example-1",
    "title": "Exploratory Data Analysis 2",
    "section": "Example 1",
    "text": "Example 1\n\n\nLet’s look at a dataset of students height.\n\nEasy to print the summary statistics in Python, using pandas:\n\nimport pandas as pd \n\nheight_df = pd.read_csv(\"L2_data/heights.csv\")\nheight_df.describe().round(2)\n\n\n\n\n\n\n\n\n\n\n\nHeight_cm\n\n\n\n\ncount\n1000.00\n\n\nmean\n161.19\n\n\nstd\n9.79\n\n\nmin\n128.59\n\n\n25%\n154.52\n\n\n50%\n161.25\n\n\n75%\n167.48\n\n\nmax\n199.53"
  },
  {
    "objectID": "sessions/week2_lecture.html#example-2",
    "href": "sessions/week2_lecture.html#example-2",
    "title": "Exploratory Data Analysis 2",
    "section": "Example 2",
    "text": "Example 2\nSometimes we need more information.\n\nimport pandas as pd \n\n# read data\ndf_4datasets = pd.read_csv(\"L2_data/anscombe_quartet.csv\")\n# print descriptive statistics\ndf_4datasets.groupby(\"dataset\").describe()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n11.0\n9.0\n3.316625\n4.0\n6.5\n9.0\n11.5\n14.0\n11.0\n7.500909\n2.031568\n4.26\n6.315\n7.58\n8.57\n10.84\n\n\n2\n11.0\n9.0\n3.316625\n4.0\n6.5\n9.0\n11.5\n14.0\n11.0\n7.500909\n2.031657\n3.10\n6.695\n8.14\n8.95\n9.26\n\n\n3\n11.0\n9.0\n3.316625\n4.0\n6.5\n9.0\n11.5\n14.0\n11.0\n7.500000\n2.030424\n5.39\n6.250\n7.11\n7.98\n12.74\n\n\n4\n11.0\n9.0\n3.316625\n8.0\n8.0\n8.0\n8.0\n19.0\n11.0\n7.500909\n2.030579\n5.25\n6.170\n7.04\n8.19\n12.50\n\n\n\n\n\n\n\n\nall 4 datasets have very similar descriptive statistics of both the x, and y variables."
  },
  {
    "objectID": "sessions/week2_lecture.html#same-same-but-different",
    "href": "sessions/week2_lecture.html#same-same-but-different",
    "title": "Exploratory Data Analysis 2",
    "section": "Same same but different",
    "text": "Same same but different\n\n\n\n\nThese all have mean x = 9, variance x = 11. mean y = 7.5, variance y =4.1.\nBut we can see from the diagrams that the distribution of the data is very different, some is linear, some is elliptic etc… So we need more advanced ways of describing the data."
  },
  {
    "objectID": "sessions/week2_lecture.html#key-features",
    "href": "sessions/week2_lecture.html#key-features",
    "title": "Exploratory Data Analysis 2",
    "section": "Key features",
    "text": "Key features\nYou’ve seen it all before\n\nData is continuous\n\nit is something you measure not something you count\n\nData is equally likely to be larger or smaller than average\n\nsymmetric\n\nCharacteristic size, all data points are close to the mean\n\nsingle peak\n\nThere is less data further away from the mean\n\nsmooth tails on both sides"
  },
  {
    "objectID": "sessions/week2_lecture.html#uniquely-described-by-two-variables",
    "href": "sessions/week2_lecture.html#uniquely-described-by-two-variables",
    "title": "Exploratory Data Analysis 2",
    "section": "Uniquely described by two variables…",
    "text": "Uniquely described by two variables…"
  },
  {
    "objectID": "sessions/week2_lecture.html#and-a-probability-distribution-function",
    "href": "sessions/week2_lecture.html#and-a-probability-distribution-function",
    "title": "Exploratory Data Analysis 2",
    "section": "…and a probability distribution function",
    "text": "…and a probability distribution function\n\n\n\nThe probability density function (PDF) describes the likelihood of different outcomes for a continuous random variable"
  },
  {
    "objectID": "sessions/week2_lecture.html#the-distribution-of-the-random-variable",
    "href": "sessions/week2_lecture.html#the-distribution-of-the-random-variable",
    "title": "Exploratory Data Analysis 2",
    "section": "The distribution of the random variable",
    "text": "The distribution of the random variable\nA random variable is a way to map the outcome of a random process to a probability.\nIn mathematical notation, a random variable \\(X\\) is approximately normally distributed about a mean of \\(\\mu\\) with a standard deviation of \\(\\sigma\\):\n\\[\\begin{align}\nX \\sim N(\\mu,\\sigma)\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week2_lecture.html#sampling-distributions",
    "href": "sessions/week2_lecture.html#sampling-distributions",
    "title": "Exploratory Data Analysis 2",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nThe distribution of the random variable when derived from a random sample of size \\(n\\)\n\nIn the case of the normal distribution the standard deviation becomes:\n\\[\\begin{align}\n\\frac{\\sigma}{\\sqrt{n}}\n\\end{align}\\]\nIn practise as the sample size increases the sampling distribution becomes more and more centralised.\n\ni.e. with more data we have more certainty about the distribution.\n\n\nTypically we think of the distribution as describing the entirety of the relevant popualtion - i.e. every relevant example. Link to sampling - normally only have a subset of the data.\nNeed to account for the fact that we don’t have all the data. So our statistic will vary a bit according to what sample of the data we’re looking at.\nIn practise - as the sample size increases the sampling distribution of the normal distribution becomes more and more centralised. I.e. we have more certainty about the distribution."
  },
  {
    "objectID": "sessions/week2_lecture.html#calculating-probabilities",
    "href": "sessions/week2_lecture.html#calculating-probabilities",
    "title": "Exploratory Data Analysis 2",
    "section": "Calculating probabilities",
    "text": "Calculating probabilities\nCan use the PDF to evaluate the probability at a specific point.\n\\[\\begin{align}\nX \\sim N(0,1)\n\\end{align}\\]\n\n\\[\\begin{align}\np(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\end{align}\\]\n\n\n\\[\\begin{align}\np(x=0.5) = \\frac{1}{\\sqrt{2 \\pi \\times 1^2}} e^{-\\frac{(0.5-0)^2}{(2\\times 1)^2}}\n\\\\\n= \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{0.25}{4}} = 0.3747\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week2_lecture.html#not-everything-is-normal",
    "href": "sessions/week2_lecture.html#not-everything-is-normal",
    "title": "Exploratory Data Analysis 2",
    "section": "Not everything is normal",
    "text": "Not everything is normal\nMany real world datasets are approximately normally distributed.\n\nBut not all\n\nnot continuous\nno characteristic size\nnot symmetric"
  },
  {
    "objectID": "sessions/week2_lecture.html#continuous-vs.-discrete",
    "href": "sessions/week2_lecture.html#continuous-vs.-discrete",
    "title": "Exploratory Data Analysis 2",
    "section": "Continuous vs. Discrete",
    "text": "Continuous vs. Discrete\nContinuous data Measurable data which can take any value within a given range.\nexample: height\nDiscrete data Measurable data which can take seperate, countable values.\nexample: shoe size\n\nSuppose I have someone who is x cm tall, and someone who is y cm tall, I can find someone in between whose x.5cm tall.\nBut shoe sizes are discrete (not the length of your foot)."
  },
  {
    "objectID": "sessions/week2_lecture.html#back-to-the-probability-function",
    "href": "sessions/week2_lecture.html#back-to-the-probability-function",
    "title": "Exploratory Data Analysis 2",
    "section": "Back to the probability function",
    "text": "Back to the probability function\n\\[\\begin{align}\np(x)\n\\end{align}\\]\nHaving a function for the distribution allows us to evaluate the probability of events, and hence evaluate hypotheses.\n\nFor discrete distributions we have the probability mass function.\n\n\nSampling distributions\nAs for the normal distribution, in the general case we should be aware of the sampling distribution."
  },
  {
    "objectID": "sessions/week2_lecture.html#coin-toss",
    "href": "sessions/week2_lecture.html#coin-toss",
    "title": "Exploratory Data Analysis 2",
    "section": "Coin toss",
    "text": "Coin toss\n\n\n\nI flip a coin 10 times\nHow often can I expect to get at least 7 heads?\n\n\n \n\n\n\n\n\nDiscrete outcomes Describes the frequency of successes in a test with 2 outcomes. Coin flip is the classic example - have exactly two outcomes, either heads or tails."
  },
  {
    "objectID": "sessions/week2_lecture.html#probability-mass-function",
    "href": "sessions/week2_lecture.html#probability-mass-function",
    "title": "Exploratory Data Analysis 2",
    "section": "Probability mass function",
    "text": "Probability mass function\n\\[\\begin{align}\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\end{align}\\]\nWhere \\(n\\) is the number of trials, and \\(p\\) is the probability of success for each trial.\n\nThe probability mass function (PMF) describes the likelihood of different outcomes for a discrete random variable\n\nNote the combination here."
  },
  {
    "objectID": "sessions/week2_lecture.html#plotting-the-distribution",
    "href": "sessions/week2_lecture.html#plotting-the-distribution",
    "title": "Exploratory Data Analysis 2",
    "section": "Plotting the distribution",
    "text": "Plotting the distribution"
  },
  {
    "objectID": "sessions/week2_lecture.html#example",
    "href": "sessions/week2_lecture.html#example",
    "title": "Exploratory Data Analysis 2",
    "section": "Example",
    "text": "Example\n\nI flip a coin 10 times\n\n\\(n=10\\), \\(p=0.5\\)\n\nHow often can I expect to get at least 7 heads?\n\n\\(k \\geq 7\\)\n\n\n\nEvaluating the PMF, we get:\n\\[\\begin{align}\nP(X \\geq 7) &= P(X=7) + P(X=8) + P(X=9) + P(X=10)\n\\\\\n&= 0.1719\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week2_lecture.html#death-by-horse-kicks",
    "href": "sessions/week2_lecture.html#death-by-horse-kicks",
    "title": "Exploratory Data Analysis 2",
    "section": "Death by horse kicks",
    "text": "Death by horse kicks\n\n\n\nIt’s 1894.\nYou’re the statistician Ladislaus Bortkiewicz.\nAnd you’re wondering,\nHow many soldiers in the Prussian army have been killed by horse kicks?"
  },
  {
    "objectID": "sessions/week2_lecture.html#measuring-rare-events",
    "href": "sessions/week2_lecture.html#measuring-rare-events",
    "title": "Exploratory Data Analysis 2",
    "section": "Measuring rare events",
    "text": "Measuring rare events\n\nImagine a situation where certain rare events (like arrival of mail) can occur in an independent fashion.\nThe Poisson distribution estimates how many such events are expected within a time interval\nFixed interval (e.g. one minute)\nFixed rate of events (\\(\\lambda\\)) (e.g. 4 cars per minute, \\(\\lambda=4\\))\nPoisson distribution gives the probability of \\(k\\) events.\n\n\nTypically used to measure rare events like mail arriving or death by horse kicks."
  },
  {
    "objectID": "sessions/week2_lecture.html#probability-mass-function-1",
    "href": "sessions/week2_lecture.html#probability-mass-function-1",
    "title": "Exploratory Data Analysis 2",
    "section": "Probability mass function",
    "text": "Probability mass function\n\\[\\begin{align}\nP(X = k) = \\frac{\\lambda^k e^{- \\lambda}}{k!}\n\\end{align}\\]\nWhere \\(\\lambda\\) is the expected number of events in a given interval."
  },
  {
    "objectID": "sessions/week2_lecture.html#plotting-the-distribution-1",
    "href": "sessions/week2_lecture.html#plotting-the-distribution-1",
    "title": "Exploratory Data Analysis 2",
    "section": "Plotting the distribution",
    "text": "Plotting the distribution"
  },
  {
    "objectID": "sessions/week2_lecture.html#example-3",
    "href": "sessions/week2_lecture.html#example-3",
    "title": "Exploratory Data Analysis 2",
    "section": "Example",
    "text": "Example\n\nBetween 1883 and 1893 there were an average of 2 deaths from horse kicks a year.\n\n\\(\\lambda=2\\)\n\nWhat’s the probability of seeing 10 deaths from horse kicks in 1894?\n\n\n\\[\\begin{align}\nP(X = 10) = \\frac{2^{10} e^{-2}}{10!}\n\\\\ = 0.000038\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week2_lecture.html#exponentials",
    "href": "sessions/week2_lecture.html#exponentials",
    "title": "Exploratory Data Analysis 2",
    "section": "Exponentials",
    "text": "Exponentials\nIf the Poisson measures the probability of \\(x\\) events within a time period, then the exponential measures how long we are likely to wait between events.\n\nThe greatest shortcoming of the human race is our inability to understand the exponential function – Albert Bartlett (physicist)\n\nHe’s a bit dramatic"
  },
  {
    "objectID": "sessions/week2_lecture.html#a-game-of-chess",
    "href": "sessions/week2_lecture.html#a-game-of-chess",
    "title": "Exploratory Data Analysis 2",
    "section": "A game of chess…",
    "text": "A game of chess…\n\n\n\n\nA scholar has invented chess\nThe emperor is really grateful - and asks what gift the scholar would like in thanks\nThe scholar asks for grains of rice\n\nSpecifically, rice to fill the chessboard, such that the number of grains double on each square\n\n\n\n\n\nImage credit: https://simple.wikipedia.org/wiki/Chaturanga\n\n\n\n\nFamous story, of the man who invented chess. Emperor was so grateful, he said what do you want in return The man asked for rice, such that on each square of the chess board, the number of grains of rice doubled. The Emperor thought this was a really small ask for such a great invention. He asked if he didn’t want a better gift. But the man insisted that what he wanted was rice.\nImage is of Chatarunga - which is the earliest known form of chess dating to 6th century AD northern India."
  },
  {
    "objectID": "sessions/week2_lecture.html#and-rice",
    "href": "sessions/week2_lecture.html#and-rice",
    "title": "Exploratory Data Analysis 2",
    "section": "…and rice",
    "text": "…and rice\n\n\n\n\n\n\n10^18 - which is more than the global production of rice.\n\n\nA bowl of rice is around 4,000 grains."
  },
  {
    "objectID": "sessions/week2_lecture.html#what-does-this-look-like-on-a-graph",
    "href": "sessions/week2_lecture.html#what-does-this-look-like-on-a-graph",
    "title": "Exploratory Data Analysis 2",
    "section": "What does this look like on a graph?",
    "text": "What does this look like on a graph?"
  },
  {
    "objectID": "sessions/week2_lecture.html#generally-we-have",
    "href": "sessions/week2_lecture.html#generally-we-have",
    "title": "Exploratory Data Analysis 2",
    "section": "Generally we have",
    "text": "Generally we have"
  },
  {
    "objectID": "sessions/week2_lecture.html#the-exponential-function",
    "href": "sessions/week2_lecture.html#the-exponential-function",
    "title": "Exploratory Data Analysis 2",
    "section": "The exponential function",
    "text": "The exponential function\nThe (natural) exponential function is:\n\\[\\begin{align}\ny=e^x\n\\end{align}\\]\n\nNote\n\\(e\\) here is eulers number - a mathematical constant.\n\\[\\begin{align}\ne \\approx 2.718...\n\\end{align}\\]\n\n\nIts a mathematical constant similar to pi - it’s just a fixed number which we have a name for. Like pi comes from geometry (circles etc), e comes from the limit of the equation of compound interest."
  },
  {
    "objectID": "sessions/week2_lecture.html#the-exponential-distribution",
    "href": "sessions/week2_lecture.html#the-exponential-distribution",
    "title": "Exploratory Data Analysis 2",
    "section": "The exponential distribution",
    "text": "The exponential distribution\n\n\n\n\nWe’ve talked about the exp function - but now want to talk about the exp distribution.\nThis is a continous probability distribution."
  },
  {
    "objectID": "sessions/week2_lecture.html#probability-density-function",
    "href": "sessions/week2_lecture.html#probability-density-function",
    "title": "Exploratory Data Analysis 2",
    "section": "Probability density function",
    "text": "Probability density function\nThe PDF of the exponential distribution is:\n\\[\\begin{align}\nP(x) = \\lambda e^{-\\lambda x}\n\\end{align}\\]\nwhere \\(\\lambda\\) is the rate parameter. As in the poisson distribution \\(\\lambda\\) is the fixed rates of events for a predetermined time interval.\n\nNote\nThis is sometimes referred to as the negative exponential distribution - as it’s a negative exponent."
  },
  {
    "objectID": "sessions/week2_lecture.html#time-between-events",
    "href": "sessions/week2_lecture.html#time-between-events",
    "title": "Exploratory Data Analysis 2",
    "section": "Time between events",
    "text": "Time between events\nTraditionally used to model time between rare events.\n\ntime between volcanic eruptions\ntime between customers entering a shop\ntime of radioactive decay"
  },
  {
    "objectID": "sessions/week2_lecture.html#important-questions",
    "href": "sessions/week2_lecture.html#important-questions",
    "title": "Exploratory Data Analysis 2",
    "section": "Important questions",
    "text": "Important questions\nThe exponential allows us to answer questions like:\n\nWhat’s the probability that it will be ten years until the next volcanic eruption?\n\n\\(\\lambda=1\\)\n\\(p(10) = 1 \\times e^{-1 \\times 10}\\)\n\nWhen will the probability of volcanic eruptions be equal to \\(p=0.5\\)?\n\n\\(\\lambda=1\\)\n\\(0.5 = p(x) = 1 \\times e^{-1 \\times x}\\)\nwhat is \\(x\\)?\n\n\n\nThis is easier said than done – the best way is to invert the equation."
  },
  {
    "objectID": "sessions/week2_lecture.html#inverse-operations",
    "href": "sessions/week2_lecture.html#inverse-operations",
    "title": "Exploratory Data Analysis 2",
    "section": "Inverse operations",
    "text": "Inverse operations\nThe mathematical operation that reverses.\nSubtract is the inverse of adding.\n\\[\\begin{align}\n2 + x=5 \\implies 5-2=x\n\\end{align}\\]\nDivide is the inverse of multiplying.\n\\[\\begin{align}\n2 \\times x =6 \\implies 6 \\div 2=x\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week2_lecture.html#logarithms",
    "href": "sessions/week2_lecture.html#logarithms",
    "title": "Exploratory Data Analysis 2",
    "section": "Logarithms",
    "text": "Logarithms\nTaking the logarithm is the inverse of taking the exponential.\n\n\\[\\begin{align}\n2^3 = 8 \\implies \\log_2(8) =3\n\\end{align}\\]\n\n\nMore generally:\n\\[\\begin{align}\na^x = b \\implies \\log_a(b) =x\n\\end{align}\\]\n\n\nFor the natural logarithm:\n\\[\\begin{align}\ne^x = b \\implies \\log_e(b) =ln(b) = x\n\\end{align}\\]\n\n\nMultiply and divide are inverse operations of one another (they reverse the process).\nRead the equation as log of 8 base 2 equals 3."
  },
  {
    "objectID": "sessions/week2_lecture.html#natural-logarithm",
    "href": "sessions/week2_lecture.html#natural-logarithm",
    "title": "Exploratory Data Analysis 2",
    "section": "Natural logarithm",
    "text": "Natural logarithm"
  },
  {
    "objectID": "sessions/week2_lecture.html#log-rules",
    "href": "sessions/week2_lecture.html#log-rules",
    "title": "Exploratory Data Analysis 2",
    "section": "Log rules",
    "text": "Log rules\nThere are some general rules for how we apply logarithms:\n\\[\\begin{align}\nlog_a(b \\times c) &= log_a(b) + log_a(c)\n\\\\ log_a(\\frac{b}{c}) &= log_a(b)-loc_b(c)\n\\\\ log_a(b^c) &= c \\times log_a(b)\n\\\\ log_a(1)&=0\n\\\\ log_a(a)&=1\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week2_lecture.html#transforming-data",
    "href": "sessions/week2_lecture.html#transforming-data",
    "title": "Exploratory Data Analysis 2",
    "section": "Transforming data",
    "text": "Transforming data\nSome of the most important rules:\n\\[\\begin{align}\nlog_a(a^x) = x\n\\\\ ln(e^x) = x\n\\end{align}\\]\n\nWhen we have exponential data we can take the logarithm of it - and hence simplify it.\n\nWe will return to the idea of taking the log of data in future weeks - especially in the context of regression analyses."
  },
  {
    "objectID": "sessions/week1_practical.html",
    "href": "sessions/week1_practical.html",
    "title": "Practical 1: describing and representing data",
    "section": "",
    "text": "This week is focussed on ensuring that you’re able to access the teaching materials and to run Jupyter notebooks locally, as well as describing a dataset in Python."
  },
  {
    "objectID": "sessions/week1_practical.html#learning-outcomes",
    "href": "sessions/week1_practical.html#learning-outcomes",
    "title": "Practical 1: describing and representing data",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou have familiarised yourself with how to access the lecture notes and Python notebook of this module.\nYou have familiarised yourself with running the Python notebooks locally.\nYou have familiarised yourself with describing a dataset in Python."
  },
  {
    "objectID": "sessions/week1_practical.html#set-up-the-tools",
    "href": "sessions/week1_practical.html#set-up-the-tools",
    "title": "Practical 1: describing and representing data",
    "section": "Set up the tools",
    "text": "Set up the tools\nPlease follow the Setup page of CASA0013 to install and configure the computing platform, and this page to get started on using the container & JupyterLab."
  },
  {
    "objectID": "sessions/week1_practical.html#download-the-notebook",
    "href": "sessions/week1_practical.html#download-the-notebook",
    "title": "Practical 1: describing and representing data",
    "section": "Download the Notebook",
    "text": "Download the Notebook\nSo for this week, visit the Week 1 of QM page, you’ll see that there is a ‘preview’ link and a a ‘download’ link. If you click the preview link you will be taken to the GitHub page for the notebook where it has been ‘rendered’ as a web page, which is not editable. To make the notebook useable on your computer, you need to download the IPYNB file.\nSo now:\n\nClick on the Download link.\nThe file should download automatically, but if you see a page of raw code, select File then Save Page As....\nMake sure you know where to find the file (e.g. Downloads or Desktop).\nMove the file to your Git repository folder (e.g. ~/Documents/CASA/QM/)\nCheck to see if your browser has added .txt to the file name:\n\nIf no, then you can move to adding the file.\nIf yes, then you can either fix the name in the Finder/Windows Explore, or you can do this in the Terminal using mv &lt;name_of_practical&gt;.ipynb.txt &lt;name_of_practical&gt;.ipynb (you can even do this in JupyterLab’s terminal if it’s already running)."
  },
  {
    "objectID": "sessions/week1_practical.html#running-notebooks-on-jupyterlab",
    "href": "sessions/week1_practical.html#running-notebooks-on-jupyterlab",
    "title": "Practical 1: describing and representing data",
    "section": "Running notebooks on JupyterLab",
    "text": "Running notebooks on JupyterLab\nI am assuming that most of you are already running JupyterLab via Podman using the command.\nIf you are a bit confused with container, JupyterLab, terminal, or Git, please feel free to ask any questions."
  },
  {
    "objectID": "sessions/week1_practical.html#loading-data",
    "href": "sessions/week1_practical.html#loading-data",
    "title": "Practical 1: describing and representing data",
    "section": "Loading data",
    "text": "Loading data\nWe are going to describe the population of local authorities in the UK.\nThe data is sourced from Office for National Statistics and is donwloadable here.\nWe have saved a copy of this dataset to the Github repo, in case that the dataset is removed from the website.\n\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'https://raw.githubusercontent.com/huanfachen/QM/refs/heads/main/sessions/L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())\n\n   Area code          Area name Area type  Population 2011  Population 2021  \\\n0  K04000001  England and Wales  National       56075912.0       59597542.0   \n1  E92000001            England   Country       53012456.0       56490048.0   \n2  W92000004              Wales   Country        3063456.0        3107494.0   \n3  E12000001         North East    Region        2596886.0        2647013.0   \n4  E12000002         North West    Region        7052177.0        7417397.0   \n\n   Percentage change  \n0                6.3  \n1                6.6  \n2                1.4  \n3                1.9  \n4                5.2  \n\n\nYou might wonder why skipping the first 5 rows and setting thousands=‘,’. I knew about this after opening this csv file in a text editor and lots of trial-and-errors.\n\n\nThen, we check the first few rows of this dataset using df_pop.head()."
  },
  {
    "objectID": "sessions/week1_practical.html#describing-the-dataframe",
    "href": "sessions/week1_practical.html#describing-the-dataframe",
    "title": "Practical 1: describing and representing data",
    "section": "Describing the dataframe",
    "text": "Describing the dataframe\n\nWhich columns are included?\n\nlist(df_pop.columns)\n\n['Area code',\n 'Area name',\n 'Area type',\n 'Population 2011',\n 'Population 2021',\n 'Percentage change']\n\n\nIt is a pain to deal with whitespaces in a column, so good practice is to replace the whitespaces (eg tabs, multiple spaces) within column names with underscore.\n\ndf_pop.columns = df_pop.columns.str.replace(r'\\s+', '_', regex=True)\nprint(list(df_pop.columns)) # check again\n\n['Area_code', 'Area_name', 'Area_type', 'Population_2011', 'Population_2021', 'Percentage_change']\n\n\n\n\nHow many rows & cols are included?\n\nrows, cols = df_pop.shape\nprint(f\"Rows: {rows}, Columns: {cols}\")\n\nRows: 369, Columns: 6\n\n\n\n\nGeography matters\nThis dataset contains multiple geographies of UK and different geographies are incomparable. We can check the Area_type column:\n\nprint(df_pop.Area_type.value_counts())\n\nArea_type\nLocal Authority    355\nRegion               9\nCountry              2\nNational             1\nName: count, dtype: int64\n\n\nSo there are 355 records of Local Authority， 9 records of Region, 2 of Country, and 1 of ‘National’. For an introduction to these terms, see this article on ONS.\nWe will focus on the local authorities, so we apply a filter:\n\ndf_pop_la = df_pop[df_pop['Area_type'] == 'Local Authority']\n\n\n\nOverview of the columns\nThere are two pandas functions that give overview of a dataframe. - info(): shows column data types, non‑null counts, and memory usage. - describe(): shows summary statistics for numeric data (count, mean, std, min, quartiles, max) - describe(include='all'): for both numeric data and non‑numeric data (count, unique, top value, frequency).\n\nprint(df_pop_la.info())\nprint(df_pop_la.describe())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 355 entries, 12 to 366\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Area_code          355 non-null    object \n 1   Area_name          355 non-null    object \n 2   Area_type          355 non-null    object \n 3   Population_2011    355 non-null    float64\n 4   Population_2021    355 non-null    float64\n 5   Percentage_change  355 non-null    float64\ndtypes: float64(3), object(3)\nmemory usage: 19.4+ KB\nNone\n       Population_2011  Population_2021  Percentage_change\ncount     3.550000e+02     3.550000e+02         355.000000\nmean      2.132867e+05     2.268876e+05           6.070423\nstd       2.099628e+05     2.245442e+05           4.608338\nmin       2.203000e+03     2.054000e+03          -9.600000\n25%       1.000530e+05     1.055705e+05           2.950000\n50%       1.382650e+05     1.477760e+05           5.800000\n75%       2.487865e+05     2.628895e+05           9.000000\nmax       1.463740e+06     1.576069e+06          22.100000"
  },
  {
    "objectID": "sessions/week1_practical.html#describing-census-2021-population",
    "href": "sessions/week1_practical.html#describing-census-2021-population",
    "title": "Practical 1: describing and representing data",
    "section": "Describing census 2021 population",
    "text": "Describing census 2021 population\nNow, we focus on describing the local authority population from census 2021. The first question is, what data type is this variable - nominal, ordinal, interval, or ratio？\n\n\n\n\n\n\nNote\n\n\n\nThe data type of a variable is different from how it’s stored in a computer. For example, the Area_type variable can be encoded for convenience as 0 (“national”), 1 (“country”), and 2 (“local authority”). Although these are stored as numbers, Area_type is not truly numeric data — it’s an nominal variable.\n\n\nDoes it make sense to say ‘the population of LA AAA is twice of LA BBB’? Yes. So, this variable is of ratio type.\n\nmax and min\nWhat is the maximum population size in census 2021?\nprint(\"Max population: \", df_pop_la['Population_2021'].max(skipna=True))\nWhich LAs have the maximum population size? The code below is a bit complicated.\n\nprint(\"{} have the maximum population of {}\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Population_2021'] == df_pop_la['Population_2021'].max(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].max(skipna=True))\n    )\n\nKent have the maximum population of 1576069.0\n\n\nWhat it does:\n\nFinds the max population while ignoring NaNs. It is always safe to use skipna=True, even though there is no NA values.\nSelects all rows with that population.\nJoins their Area_name values into a comma-separated string.\n\nTwo new Python functions here:\n\nformat(): Inserts variables into a string by replacing {} placeholders in order with provided arguments.\njoin(): Combines the elements of an iterable into one string using the given separator before .join().\n\nWhich LAs have the minimum population?\n\nprint(\"{} have the minimum population of {}\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Population_2021'] == df_pop_la['Population_2021'].min(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].min(skipna=True))\n    )\n\nIsles of Scilly have the minimum population of 2054.0\n\n\n\n\nStandard deviation\nThe result from df_pop_la.describe() indicates that the standard deviation of Population_2021 is 2.245442e+05.\nAnother way to calculate this standard deviation and to reformat it is:\n\nstd_dev = df_pop_la['Population_2021'].std()\n# plain notation\nprint(\"The standard deviation of Population_2021 is: {}\".format(std_dev)) \n# scientific notation\nprint(\"Using scientific notation: {:.3e}\".format(std_dev)) \n# thousands separator notation + 2 decimal places\nprint(\"Using thousands separator notation: {:,.2f}\".format(std_dev))\n\nThe standard deviation of Population_2021 is: 224544.20636612535\nUsing scientific notation: 2.245e+05\nUsing thousands separator notation: 224,544.21\n\n\nThere are several ways to represent numbers, and which one you choose depends on the situation.\nEqually important is to ensure the numbers are meaningful, or to use proper significant figures. For example, reporting a population’s standard deviation with 10 decimal places does not make sense.\n\n\nNull value and outliers?\nAre there Null values or outliers in this variable? From results of info(), there are no NA values.\nTo detect outliers, we will implement the Tukey Fences method using pandas function, as pandas does not provide a built-in function for this method.\n\n# Calculate Q1, Q3, and IQR\nQ1 = df_pop_la['Population_2021'].quantile(0.25)\nQ3 = df_pop_la['Population_2021'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Tukey's fences\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Detect outliers\noutliers = df_pop_la[\n    (df_pop_la['Population_2021'] &lt; lower_bound) |\n    (df_pop_la['Population_2021'] &gt; upper_bound)\n]\n\nprint(\"Lower bound:\", lower_bound)\nprint(\"Upper bound:\", upper_bound)\nprint(\"How many outliers?\", outliers.shape[0])\nprint(\"Outliers:\\n\", outliers)\n\nLower bound: -130408.0\nUpper bound: 498868.0\nHow many outliers? 33\nOutliers:\n      Area_code        Area_name        Area_type  Population_2011  \\\n56   E06000047    County Durham  Local Authority         513242.0   \n60   E06000052         Cornwall  Local Authority         532273.0   \n62   E06000054        Wiltshire  Local Authority         470981.0   \n68   E06000060  Buckinghamshire  Local Authority         505283.0   \n254  E08000003       Manchester  Local Authority         503127.0   \n270  E08000019        Sheffield  Local Authority         552698.0   \n275  E08000025       Birmingham  Local Authority        1073045.0   \n282  E08000032         Bradford  Local Authority         522452.0   \n285  E08000035            Leeds  Local Authority         751485.0   \n321  E10000003   Cambridgeshire  Local Authority         621210.0   \n322  E10000006          Cumbria  Local Authority         499858.0   \n323  E10000007       Derbyshire  Local Authority         769686.0   \n324  E10000008            Devon  Local Authority         746399.0   \n325  E10000011      East Sussex  Local Authority         526671.0   \n326  E10000012            Essex  Local Authority        1393587.0   \n327  E10000013  Gloucestershire  Local Authority         596984.0   \n328  E10000014        Hampshire  Local Authority        1317788.0   \n329  E10000015    Hertfordshire  Local Authority        1116062.0   \n330  E10000016             Kent  Local Authority        1463740.0   \n331  E10000017       Lancashire  Local Authority        1171339.0   \n332  E10000018   Leicestershire  Local Authority         650489.0   \n333  E10000019     Lincolnshire  Local Authority         713653.0   \n334  E10000020          Norfolk  Local Authority         857888.0   \n335  E10000023  North Yorkshire  Local Authority         598376.0   \n336  E10000024  Nottinghamshire  Local Authority         785802.0   \n337  E10000025      Oxfordshire  Local Authority         653798.0   \n338  E10000027         Somerset  Local Authority         529972.0   \n339  E10000028    Staffordshire  Local Authority         848489.0   \n340  E10000029          Suffolk  Local Authority         728163.0   \n341  E10000030           Surrey  Local Authority        1132390.0   \n342  E10000031     Warwickshire  Local Authority         545474.0   \n343  E10000032      West Sussex  Local Authority         806892.0   \n344  E10000034   Worcestershire  Local Authority         566169.0   \n\n     Population_2021  Percentage_change  \n56          522068.0                1.7  \n60          570305.0                7.1  \n62          510330.0                8.4  \n68          553078.0                9.5  \n254         551938.0                9.7  \n270         556521.0                0.7  \n275        1144919.0                6.7  \n282         546412.0                4.6  \n285         811953.0                8.0  \n321         678849.0                9.3  \n322         499846.0                0.0  \n323         794636.0                3.2  \n324         811640.0                8.7  \n325         545847.0                3.6  \n326        1503521.0                7.9  \n327         645076.0                8.1  \n328        1400899.0                6.3  \n329        1198798.0                7.4  \n330        1576069.0                7.7  \n331        1235354.0                5.5  \n332         712366.0                9.5  \n333         768364.0                7.7  \n334         916120.0                6.8  \n335         615491.0                2.9  \n336         824822.0                5.0  \n337         725291.0               10.9  \n338         571547.0                7.8  \n339         876104.0                3.3  \n340         760688.0                4.5  \n341        1203108.0                6.2  \n342         596773.0                9.4  \n343         882676.0                9.4  \n344         603676.0                6.6  \n\n\nThere are 33 outliers in this dataset. Think about the three types of outliers that we discussed. Which type do these 33 outliers beloong to?\n\nError Outlier\nIrregular Pattern Outlier\nInfluential Outlier\n\n\n\nBoxplot\nTo create a boxplot of this variable:\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la['Population_2021'].plot(kind='box', title='LA Population 2021 Boxplot')\n\nplt.ylabel('Population')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\nWhat do you observe from this boxplot? There are lots of values above the"
  },
  {
    "objectID": "sessions/week1_practical.html#exploring-percentage_change",
    "href": "sessions/week1_practical.html#exploring-percentage_change",
    "title": "Practical 1: describing and representing data",
    "section": "Exploring Percentage_change",
    "text": "Exploring Percentage_change\nNow, we turn to explore the variable Percentage_change, which represents the relative change from the 2011 census to 2021 census.\nTry completing the code below on your own. Practice makes perfect!\n\nWhich LAs experienced the largest population percentage change? To what extent?\n\nQuestionAnswer\n\n\nprint(\"{} have the largest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[??['??'] == ??['??'].max(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].??(skipna=True))\n    )\n\n\nprint(\"{} have the largest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Percentage_change'] == df_pop_la['Percentage_change'].max(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].max(skipna=True))\n    )\nTower Hamlets have the largest population percentage change of 22.1%\n\n\n\n\n\nWhich LAs experienced the smallest population percentage change? To what extent?\n\nQuestionAnswer\n\n\nprint(\"{} have the smallest population percentage change of {}%\".format(\n    \", \".??(df_pop_la.loc[df_pop_la[??] == ??['Percentage_change'].??(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].??(skipna=True))\n    )\n\n\nprint(\"{} have the smallest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Percentage_change'] == df_pop_la['Percentage_change'].min(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].min(skipna=True))\n    )\nKensington and Chelsea have the smallest population percentage change of -9.6%\n\n\n\n\n\nMaking a boxplot of Percentage_change\n\nQuestionAnswer\n\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la[??].plot(kind=??, title='LA Population Percentage Change Boxplot')\n\nplt.??('Percentage change')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la['Percentage_change'].plot(kind='box', title='LA Population Percentage Change Boxplot')\n\nplt.ylabel('Percentage change')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()"
  },
  {
    "objectID": "sessions/week1_practical.html#youre-done",
    "href": "sessions/week1_practical.html#youre-done",
    "title": "Practical 1: describing and representing data",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the first QM practical session! If you are still working on it, take you time.\nDon’t worry about understanding every detail of the Python code — what matters most is knowing which functions to use for a specific task, like checking minimum and maximum values or generating boxplots, and knowing how to debug when it goes wrong. Remember, practice makes perfect."
  },
  {
    "objectID": "sessions/week10.html",
    "href": "sessions/week10.html",
    "title": "Week 10",
    "section": "",
    "text": "This week will introduce how to group similar data points together to discover patterns and structures within a dataset.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#introduction",
    "href": "sessions/week10.html#introduction",
    "title": "Week 10",
    "section": "",
    "text": "This week will introduce how to group similar data points together to discover patterns and structures within a dataset.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#learning-objectives",
    "href": "sessions/week10.html#learning-objectives",
    "title": "Week 10",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the principle and purpose of clustering analysis.\nUnderstand K-Means and apply K-Means to various datasets.\nInterpret the results of clustering analysis.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#lecture",
    "href": "sessions/week10.html#lecture",
    "title": "Week 10",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#quiz",
    "href": "sessions/week10.html#quiz",
    "title": "Week 10",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#practical",
    "href": "sessions/week10.html#practical",
    "title": "Week 10",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#further-resources",
    "href": "sessions/week10.html#further-resources",
    "title": "Week 10",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/index.html",
    "href": "sessions/index.html",
    "title": "Overview",
    "section": "",
    "text": "Coding alone is not enough.\nUnderstanding models aids in correct tool selection and bug handling.\nGoogle/ChatGPT can make mistakes; detecting them is crucial.\nMathematics understanding is not always required.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#why-study-a-quantitative-methods-course",
    "href": "sessions/index.html#why-study-a-quantitative-methods-course",
    "title": "Overview",
    "section": "",
    "text": "Coding alone is not enough.\nUnderstanding models aids in correct tool selection and bug handling.\nGoogle/ChatGPT can make mistakes; detecting them is crucial.\nMathematics understanding is not always required.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#objectives",
    "href": "sessions/index.html#objectives",
    "title": "Overview",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the structure and focus of the module.\nDevelop a method for tackling quantitative problems.\nFormulate a research question and structure quantitative writing.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#course-objectives",
    "href": "sessions/index.html#course-objectives",
    "title": "Overview",
    "section": "Course Objectives",
    "text": "Course Objectives\n\nUnderstand a broad range of quantitative techniques.\nApply these skills in research.\nFormulate a coherent quantitative argument.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#prerequisites",
    "href": "sessions/index.html#prerequisites",
    "title": "Overview",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nNo prerequisite of university-level maths/statistics.\nNo prerequisite of programming.\nPython (or R) is required for practicals and assessments.\nThis module doesn’t teach programming, so CASA0013 is strongly recommneded if you don’t know Python before.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#course-structure",
    "href": "sessions/index.html#course-structure",
    "title": "Overview",
    "section": "Course Structure",
    "text": "Course Structure\n\nLectures: Wednesdays 9:00–10:30; G13 Torrington Place (1-19).\nTutorials: Wednesdays 10:30–12:30; G13 Torrington Place (1-19).",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#platforms",
    "href": "sessions/index.html#platforms",
    "title": "Overview",
    "section": "Platforms",
    "text": "Platforms\n\nEmail for important notices and private questions.\nGithub & website for lecture notes and notebooks.\nMoodle for lectures recording and assessments.\nSlack for public questions.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#weekly-schedule",
    "href": "sessions/index.html#weekly-schedule",
    "title": "Overview",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\n\n\nSession\nTopic\nLecturer\n\n\n\n\n1\nIntroduction to data\nHuanfa\n\n\n2\nProbability and distribution\nBea\n\n\n3\nHypothesis testing\nBea\n\n\n4\nIntroduction to linear algebra\nBea\n\n\n5\nCorrelation and regression\nHuanfa\n\n\n6\nIntro to Regression\nAdam\n\n\n7\nMultiple Regression\nAdam\n\n\n8\nMultilevel regression\nAdam\n\n\n9\nDimensionality reduction\nHuanfa\n\n\n10\nClustering Analysis\nHuanfa",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#assessment-1",
    "href": "sessions/index.html#assessment-1",
    "title": "Overview",
    "section": "Assessment",
    "text": "Assessment\n\nWritten Investigation (summative): 100%\nWeekly quiz (formative)",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#ucl-assessment-policy",
    "href": "sessions/index.html#ucl-assessment-policy",
    "title": "Overview",
    "section": "UCL Assessment Policy",
    "text": "UCL Assessment Policy\n\nAll submissions via Moodle, not emails.\nLate penalties: Up to 2 working days (-10 points); up to 7 working days (capped at 50); over 7 working days (scores 0).\nDAP or Extenuating circumstances: to submit on Portico.\nRespect word count limits\nAvoid plagiarism and unverified references",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#moodle-feedback",
    "href": "sessions/index.html#moodle-feedback",
    "title": "Overview",
    "section": "Moodle Feedback",
    "text": "Moodle Feedback\nPlease provide anonymous feedback on Moodle",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#github-feedback",
    "href": "sessions/index.html#github-feedback",
    "title": "Overview",
    "section": "Github Feedback",
    "text": "Github Feedback\nYou can also give feedback on Github issues.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "We all need help from time to time, and while we will always do our best to support you because we know that this module is hard for students who are new to quantitative modules or statistics, the best way to ‘get help’ will also always be taking steps to ‘help yourself’ first."
  },
  {
    "objectID": "help.html#how-to-help-yourself",
    "href": "help.html#how-to-help-yourself",
    "title": "Getting Help",
    "section": "How to Help Yourself",
    "text": "How to Help Yourself\nHere are at least six things that you can do to ‘help yourself’:\n\nMake use of practical sessions–we can’t help you if we don’t know that you’re struggling. Please talk to the lecturer or TAs during the pratical sessions.\nUse the dedicated #casa0007_qm channel on Slack –this provides a much richer experience than the Moodle Forum and should be your primary means of requesting help outside of scheduled teaching hours.\nDo the readings–regardless of whether we ask you questions in class about them (or not), the readings are designed to support the module’s learning outcomes, so if you are struggling with a concept or an idea then please look to the week’s readings! You should also review the full bibliography while developing your thinking for the final project.\nUse Google or Stack Overflow–as you become a better programmer you’ll start to understand how to frame your question in ways that produce the right answer right away, but whether you’re a beginner or an expert Stack Overflow is your friend.\nSign up for online classes–there are lots of plausible online classes on LinkedIn course or Coursera. Please check the reviews before you take an online module."
  },
  {
    "objectID": "assessments/index.html",
    "href": "assessments/index.html",
    "title": "CASA0007 Assessment",
    "section": "",
    "text": "This Assessment is worth 100% of the grade for this course.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#assessment-aims",
    "href": "assessments/index.html#assessment-aims",
    "title": "CASA0007 Assessment",
    "section": "Assessment Aims",
    "text": "Assessment Aims\nThis assessment is designed to test your understanding of the quantitative methods introduced in this course, but also, crucially, your ability to really understand how, if used correctly and applied to appropriate data, these methods can help you tell a powerful story and be the underpinning evidence-base for a relevant local, national or international issue.\nUsing quantitative methods to contribute to public debates is vital if students, academics and universities are to demonstrate our value to the wider world. We don’t want you to just apply a method and regurgitate unintelligible coefficients. Quantitative methods can only have real value if they are used to support wider debates in ways that everyone should be able to understand.\nWe are not testing your coding abilities – you may use either of the main coding languages taught this term (R or Python). We are not testing your ability to write a standard piece of academic writing. But we are testing your ability to use quantitative methods appropriately and with a clear connection between the data, methods and outputs, to help others understand a particular issue through a data lens.\nPart 1 - Article\nYour task is to write a short 800-1000-word piece of ‘public-facing scholarly writing’ in the style of an article that that could appear in The Conversation or the Financial Times in their data section. This piece could relate to a local, national or international topic which could be either serious or frivolous but must employ appropriate quantitative methods learned in this course to derive novel insights from a particular dataset (or range of datasets) associated with a particular topic of your choosing. Data analysis and the use of quantitative methods should be central to your piece, but outputs must be appropriate for a general (non-academic) audience with your piece illuminated by appropriate visual outputs – maps, graphs or other data visualisations. Your piece should contain a range of graphical or tabular elements.\nPart 2 - Technical Appendix\nYour 800-1000-word article should be accompanied by a max 1000-word technical appendix detailing the analysis you have carried out behind the scenes to allow you to make the observations you have in your article. Here you might want to include additional exploratory visualisations, tabular outputs, interpretations of those outputs, equations etc. and you could also include any observations about the dataset or the validity / statistical significance of any models you employ. The purpose of the appendix is to reassure anyone who wants to delve deeper, that the observations you made in your main article are valid and reliable and your interpretations valid.\nTopic\nYour topic can be anything you like broadly related to human, urban or social issues, as long as you can find some suitable data to analyse. For inspiration on relevant topics, you might want to review some of the articles that John Burn-Murdoch has written for the Financial Times in recent years - https://www.ft.com/john-burn-murdoch (you can log-in via your UCL credentials) or some of the pieces in the Conversation - https://theconversation.com/.\nIf you are struggling for inspiration, you are welcome to explore an educational topic using DfE schools data used in class, but you are encouraged to be creative in your data choices (as you are being partially marked on your originality), and you should not repeat analyses carried out on variables in any of the practical sessions. The only topic we will not permit in this assessment is anything related to AirBnB as this is the focus of CASA0013.\nArticle Content\nYou will note that most of John Burn-Murdoch’s articles generally contain the sorts of analyses we would describe as exploratory. While your article should contain basic exploratory analysis in the form of visualisations, you should also use an appropriate method from the second half of the course (lectures 5-10) related to either more sophisticated exploratory analysis like multivariate statistical analysis (e.g. dimensionality reduction or cluster analysis) or some explanatory / predictive methods such as ANOVA, linear regression or some of the generalised linear models also introduced.\nStyle of Briefing\nYou should write in plain English and avoid the use of jargon or technical language. For tips on how to write in this style, The Conversation has produced a guide: https://socsci.web.ox.ac.uk/files/conversation-writing-public-why-and-how\nTypes of Data Permissible and Sources\nAnything you like, but you should choose carefully so that you are able to demonstrate the appropriate skills. There are many potential sources of data – these could be linked from FT or Conversation articles, or you could try sites like:\nhttps://data.gov/\nhttps://www.data.gov.uk/\nhttps://opendata.nhsbsa.net/\nhttps://tfl.gov.uk/info-for/open-data-users/  \nhttps://data.europa.eu/data/datasets?locale=en\nhttps://data.worldbank.org/\nhttps://data.london.gov.uk/\nYou can probably find many more!\nReferencing\nWe will not expect standard academic referencing in this piece, however, this doesn’t mean that you shouldn’t include references – you should. In this style of public facing scholarly writing, it is common to use hyperlinks and footnotes and you should make use of these to support your narrative.\nFormat of the Piece\nExamples – Here is just one example of the kind of article you might produce (minus the technical appendix), but read widely around the data journalism sites linked from publications like the Financial Times, New York Times, Guardian.\nhttps://theconversation.com/constituency-level-data-reveals-which-parties-are-most-threatened-by-reform-264422\nDeadline and Handing In\nThe Deadline for the assessment is Tuesday, 13 January 2026 @ 10:00.\nYour report and technical appendix should be uploaded to Moodle as a single PDF document.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#mark-scheme",
    "href": "assessments/index.html#mark-scheme",
    "title": "CASA0007 Assessment",
    "section": "Mark Scheme",
    "text": "Mark Scheme\nThis is how we will mark your work - take note of them\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriterion\n80-100%\n70-79% (A)\n60-69% (B)\n50-59% (C)\n40-49% (D)\n1-39% (E)\n\n\n\nExploratory Analysis (25%)\nOutstanding selection of exploratory statistics and/or sophisticated data visualisations which illuminate the underlying data, revealing distributions / trends / relationships and associations in the data with absolute clarity. Graphics are labelled such that readers are in no doubt about what is being shown. Data may have been transformed, normalised or standardised in some way to reveal otherwise hidden patterns and justified impeccably.\nExcellent selection of exploratory statistics and/or sophisticated data visualisations which illuminate the underlying data, revealing distributions / trends / relationships and associations in the data. Graphics are labelled such that readers are in no doubt about what is being shown. Data may have been transformed, normalised or standardised in some way to reveal otherwise hidden patterns and justified.\nGood selection of exploratory statistics and/or data visualisations which illuminate the underlying data, revealing distributions / trends / relationships and associations. Graphics are labelled such that readers are able to interpret the plots with ease.\nAdequate selection of exploratory statistics and/or data visualisations which illuminate some of the underlying data, revealing some distributions / trends / relationships and associations. Graphics are labelled, but may lack clarity. Data may not be transformed, normalised or standardised in a way to reveal patterns.\nInadequate selection of statistics or visualisations. Graphics are poorly labelled or unclear, and do not illuminate the underlying data. Data transformations are incorrect or missing.\nNegligible use of statistics or visualisations. Any visualisations are irrelevant, inaccurate, or inaccessible. The analysis shows no grasp of the underlying data.\n\n\nUse of multivariate statistical analysis or explanatory / predictive methods (25%)\nExceptional selection of data / variables entirely appropriate for the chosen article topic. Masterful understanding of the nuances related to the careful pruning and selection of appropriate variables. Outstanding understanding of the methods employed and their interaction with the data to hand with masterful understanding of outputs produced\nExcellent selection of data / variables entirely appropriate for the chosen article topic. Highly competent understanding of the nuances related to the careful pruning and selection of appropriate variables. Highly competent understanding of the methods employed and their interaction with the data to hand.\nGood selection of data / variables appropriate for the chosen article topic. Competent understanding of the methods employed and their interaction with the data.\nAdequate selection of data / variables for the chosen article topic. Basic understanding of the methods employed and their interaction with the data. May get some of the nuances in the outputs, but may also ignore some key features in the data\nInadequate selection of data / variables for the chosen article topic. Little to no understanding of the methods employed or how they interact with the data.\nNegligible or irrelevant selection of data / variables. No grasp of the methods or their application to the data.\n\n\nOriginality, article narrative and communication (25%)\nHighly original topic selection, or of exceptional relevance to a contemporary debate in the society, the media or politics at a local, national or international level with broad interest. Article narrative shows flare or originality which draws the reader in and reveals something entirely new. Writing style is highly accessible - clear, concise and creative and the reader is left without query or misunderstanding.\nExcellent topic selection, of high relevance to a contemporary debate. The narrative is engaging and written with excellent clarity, flowing well from one section to the next. The work is of a very high standard. Writing style clear and concise with few wasted words.\nGood topic selection, of relevance to a contemporary debate. The narrative is clear and well-structured. The work shows some evidence of originality. The narrative is good and the message emerging from the analysis is conveyed well.\nAdequate topic selection, but may lack relevance or wider interest. The narrative is satisfactory but may lack clarity or logical flow – ideas appearing slightly disorganised. The reader can understand the piece but may have to work hard to derive meaning from it.\nInadequate topic selection – perhaps dated or totally irrelevant to the degree programme (i.e. not even a human topic). The narrative is lacking in clarity and is difficult to follow. Deriving meaning from the work is a challenge.\nThe work has no clear topic or narrative. The communication is confused, unclear, or inaccessible.\n\n\nConceptual understanding and Critical Reflection (25%)\nBoth the Article and Technical Appendix show exemplary understanding of the topic / wider issues associated with it and of the methods employed to interrogate the data. A clear understanding of any data / methodological shortcomings / issues / challenges is presented with a highly sophisticated degree of critical reflection in relation to the substantive topic and / or methods employed is demonstrated.\nArticle and Technical Appendix show excellent and highly competent conceptual understanding of key concepts and theories related to both the topic and methods employed. The work demonstrates a thorough understanding of the chosen example and recognises and reflects lucidly on any shortcomings and / or the wider significance of the findings in a way that is not contrived or formulaic but shows a sophisticated level of insight.\nArticle and Technical Appendix show good understanding of key concepts and theories related to both the topic and methods employed. The work demonstrates a sound understanding of the chosen example and recognises and reflects lucidly on any shortcomings and / or the wider significance of the findings in a way that is not contrived or formulaic but shows a sophisticated level of insight.\nArticle and Technical Appendix show basic understanding of key concepts and theories related to both the topic and methods employed. The work demonstrates a rudimentary understanding of the chosen example and recognises and may offer only some reflection on the shortcomings of the work or not at all / contrived at the bottom end.\nInadequate and insufficient conceptual understanding of key concepts and theories. The work demonstrates an invalid or lack of understanding of the concepts introduced. Any analysis attempted fails to support the observations made.\nNegligible or no conceptual understanding of key concepts and theories. The work demonstrates an irrelevant, inaccurate, confused, unclear, or inaccessible understanding of the concepts.\n\n\n\n\n\n\nThe purpose of this assessment is to test your understanding of the various methods introduced in this course and your ability to apply them appropriately to a topic of your choice. One of the differentiators at Masters level is the ability to think both creatively and critically while showing an awareness or knowledge of contemporary issues either in relation to your specific discipline of study or more widely. Being able to demonstrate how established techniques of research and enquiry can be used to create and interpret knowledge is at the core of Masters level thinking and this assessment piece. The ability to demonstrate self-direction (in choosing an appropriate topic for this assessment) and to think autonomously in designing your own article.\nMark Scheme – Explained\nLevel 7 Descriptors - https://www.qaa.ac.uk/docs/qaa/quality-code/the-frameworks-for-higher-education-qualifications-of-uk-degree-awarding-bodies-2024.pdf - p26\nMaster’s degree\nThe descriptor provided for this level of the Frameworks is for any master’s degree which should meet the descriptor in full. This qualification descriptor should also be used as a reference point for other qualifications at Level 7 on the FHEQ/SCQF Level 11 on the FQHEIS, including postgraduate certificates and postgraduate diplomas.\nMaster’s degrees are awarded to students who have demonstrated:\n•         a systematic understanding of knowledge, and a critical awareness of current problems and/or new insights, much of which is at, or informed by, the forefront of their academic discipline, field of study or area of professional practice\n•         a comprehensive understanding of techniques applicable to their own research or advanced scholarship\n•         originality in the application of knowledge, together with a practical understanding of how established techniques of research and enquiry are used to create and interpret knowledge in the discipline\n•         conceptual understanding that enables the student:\n•         to evaluate critically current research and advanced scholarship in the discipline\n•         to evaluate methodologies and develop critiques of them and, where appropriate, to propose new hypotheses.\nTypically, holders of the qualification will be able to:\n•         deal with complex issues - both systematically and creatively, make sound judgements in the absence of complete data, and communicate their conclusions clearly to specialist and non-specialist audiences\n•         demonstrate self-direction and originality in tackling and solving problems, and act autonomously in planning and implementing tasks at a professional or equivalent level\n•         continue to advance their knowledge and understanding, and to develop new skills to a high level. \nAnd holders will have:\n•         the qualities and transferable skills necessary for employment requiring:  \n•         the exercise of initiative and personal responsibility\n•         decision-making in complex and unpredictable situations\n•         the independent learning ability required for continuing professional development.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "The Quantitative Methods (QM) module is a compulsory element of CASA’s MSc/MRes USS programmes and is intended provide an introduction to quantitative methods for all students.\nAs data becomes central to decision-making across all sectors, the ability to move beyond simple description to robust and evidence-based analysis is essential for researchers and practitioners in geography and urban studies. Making sense of the complex patterns that define our world requires a firm understanding of the language of data. This module provides the essential grammar for that language and aims to equip students with the foundational quantitative skills needed to describe data, build models, and derive meaningful insights from numerical evidence.\nTo achieve this, the module is structured progressively across three core sections. The first section establishes the fundamental toolkit for quantitative analysts. Beginning with Exploratory Data Analysis, students will learn how to visualise, summarise, and critically interrogate datasets. This is followed by a rigorous grounding in the principles of statistical inference through hypothesis testing, and an introduction to the linear algebra that provides the mathematical architecture for many of the models used in modern data science.\nThe second section builds upon this foundation to explore the core of statistical modelling: understanding and quantifying relationships between variables. Students will progress from measuring correlations to building sophisticated regression models. The curriculum covers the workhorse of social science, the Generalised Linear Model (GLM), before advancing to Multilevel Models, a critical technique for handling the nested and hierarchical data that is common in geographical and social research.\nThe final section of the course introduces advanced techniques for uncovering hidden structures within complex, high-dimensional datasets. Students will learn methods for dimensionality reduction to simplify complexity without losing vital information, and clustering analysis to identify natural groupings and patterns in data.\nTherefore, this module guides students on a complete analytical journey, from foundational principles to the application of advanced modelling techniques. It serves as a vital prerequisite for more specialised analytical modules and is essential for students wishing to undertake quantitative geospatial research. Ultimately, this module will provide quantitative skills that are in high demand across public, private, and academic sectors."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe really appreciate the support from various people:\n\nJon for setting up this wonderful CASA-themed quarto template.\nOllie and Andy for inspiring us to pursue a Quarto-based module website."
  },
  {
    "objectID": "sessions/week1.html",
    "href": "sessions/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "This week will introduce data types and key metrics for describing and representing data.",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#introduction",
    "href": "sessions/week1.html#introduction",
    "title": "Week 1",
    "section": "",
    "text": "This week will introduce data types and key metrics for describing and representing data.",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#learning-objectives",
    "href": "sessions/week1.html#learning-objectives",
    "title": "Week 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand data types for quantitative research.\nDescribe and represent data using key metrics.\nVisualise data with boxplot.",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#lecture",
    "href": "sessions/week1.html#lecture",
    "title": "Week 1",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#quiz",
    "href": "sessions/week1.html#quiz",
    "title": "Week 1",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#practical",
    "href": "sessions/week1.html#practical",
    "title": "Week 1",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#further-resources",
    "href": "sessions/week1.html#further-resources",
    "title": "Week 1",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1_lecture.html#understanding-and-describing-data",
    "href": "sessions/week1_lecture.html#understanding-and-describing-data",
    "title": "Exploratory Data Analysis #1",
    "section": "Understanding and describing data",
    "text": "Understanding and describing data\n\nQuantitative research is the process of collecting and analysing numerical data to describe, model, and predict variables of interest.\nGarbage in, garbage out.\n\n\nThis lecture focuses on understanding and describing data."
  },
  {
    "objectID": "sessions/week1_lecture.html#learning-objectives",
    "href": "sessions/week1_lecture.html#learning-objectives",
    "title": "Exploratory Data Analysis #1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand basic data types;\nConsider how to summarise and represent data."
  },
  {
    "objectID": "sessions/week1_lecture.html#four-levels-of-measurements",
    "href": "sessions/week1_lecture.html#four-levels-of-measurements",
    "title": "Exploratory Data Analysis #1",
    "section": "Four levels of measurements",
    "text": "Four levels of measurements\n\n\n\n\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCategorizes and labels variables\n✔\n✔\n✔\n✔\n\n\nRanks categories in order\n\n✔\n✔\n✔\n\n\nHas known, equal intervals\n\n\n✔\n✔\n\n\nHas a true or meaningful zero\n\n\n\n✔\n\n\n\n\nDeveloped by psychologist Stanley Smith Stevens (1906 - 1973)"
  },
  {
    "objectID": "sessions/week1_lecture.html#nominal",
    "href": "sessions/week1_lecture.html#nominal",
    "title": "Exploratory Data Analysis #1",
    "section": "Nominal",
    "text": "Nominal\n\nDifferentiates items based only on names; no order between them.\nAlso called categorical data\nExample: colour, gender, country names\nWhat can be said about them?\n\nEquality: ‘apple’ is not ‘pear’, ‘apple’ is ‘apple’\nMode: the most common item"
  },
  {
    "objectID": "sessions/week1_lecture.html#ordinal",
    "href": "sessions/week1_lecture.html#ordinal",
    "title": "Exploratory Data Analysis #1",
    "section": "Ordinal",
    "text": "Ordinal\n\nAllow for rank order, but not the relative degree of difference\nExample: measurement of opinions\n\ncompletely agree\nmostly agree\nneither degree nor disagree\nmostly disagree\ncompletely disagree\n\nWhat can be said about them?\n\n✅ Equality; mode\n✅ Median: middle-ranked item\n❌ Differences between two levels; arithmetic mean"
  },
  {
    "objectID": "sessions/week1_lecture.html#interval",
    "href": "sessions/week1_lecture.html#interval",
    "title": "Exploratory Data Analysis #1",
    "section": "Interval",
    "text": "Interval\n\nAllow for degree of difference between items, but not the ratio\nThe zero value is arbitrary\nExample: Celsius temperature\n\nDefinition: define 0°C & 100°C, and then separate them into 100 intervals.\nDepends on altitude/elevation\n\nWhat can be said about them?\n\n✅ equality, mode, median\n✅ addition, arithmetic mean\n❌ ratio (100°C is NOT twice 50°C)"
  },
  {
    "objectID": "sessions/week1_lecture.html#interval-1",
    "href": "sessions/week1_lecture.html#interval-1",
    "title": "Exploratory Data Analysis #1",
    "section": "Interval",
    "text": "Interval\n\nAnother example: longtitude & latitude coordinates\nThe coordinate of 8 is twice as far as that of 4? ❌"
  },
  {
    "objectID": "sessions/week1_lecture.html#ratio",
    "href": "sessions/week1_lecture.html#ratio",
    "title": "Exploratory Data Analysis #1",
    "section": "Ratio",
    "text": "Ratio\n\nAllow for ratio between items\nThe zero value is unique and non-arbitrary.\nExample: mass, length, energy\nWhat can be said about them?\n\n✅ Equality, mode, median, arithmetic mean\n✅ Ratio (2kg is twice as heavy as 1kg)"
  },
  {
    "objectID": "sessions/week1_lecture.html#notes",
    "href": "sessions/week1_lecture.html#notes",
    "title": "Exploratory Data Analysis #1",
    "section": "Notes",
    "text": "Notes\n\n‘Encoding’ does not change the nature of a measurement.\nGender variable (male, female, others) is NOMINAL.\nIf this variable is encodeded {male:0, female:1, others:2}, is it NOMINAL, or INTERVAL?"
  },
  {
    "objectID": "sessions/week1_lecture.html#another-categorisation-numerical-vs.-categorical",
    "href": "sessions/week1_lecture.html#another-categorisation-numerical-vs.-categorical",
    "title": "Exploratory Data Analysis #1",
    "section": "Another categorisation: numerical vs. categorical",
    "text": "Another categorisation: numerical vs. categorical\n\n\n\n\n\n\n\n\nType\nCategory\nNotes\n\n\n\n\nQuantitative (numerical) data\nDiscrete data\nOnly in whole numbers, e.g. number of staffs\n\n\n\nContinuous data\ne.g. temperature, 23°C, 23.4°C\n\n\nQualitative (categorical) data\nNominal\nSame as nominal in ‘Levels of measurement’\n\n\n\nOrdinal\nSee above"
  },
  {
    "objectID": "sessions/week1_lecture.html#quiz-time",
    "href": "sessions/week1_lecture.html#quiz-time",
    "title": "Exploratory Data Analysis #1",
    "section": "Quiz time",
    "text": "Quiz time\n\nMentimeter quiz"
  },
  {
    "objectID": "sessions/week1_lecture.html#key-metrics",
    "href": "sessions/week1_lecture.html#key-metrics",
    "title": "Exploratory Data Analysis #1",
    "section": "Key metrics",
    "text": "Key metrics\n\n\n\nQuantity\nThe middle\nThe spread"
  },
  {
    "objectID": "sessions/week1_lecture.html#us-city-population",
    "href": "sessions/week1_lecture.html#us-city-population",
    "title": "Exploratory Data Analysis #1",
    "section": "US city population",
    "text": "US city population\n\n\n\nQuantity: 282 values\nThe middle: Mean 302869.3\nThe middle: Median 167744.5 (Very different from mean)\nThe middle: Mode 106433 (Useful? ❌)\nShould use mean or median? median is better than mean, as data is unlikely normal distributed"
  },
  {
    "objectID": "sessions/week1_lecture.html#variance-quantifying-spread",
    "href": "sessions/week1_lecture.html#variance-quantifying-spread",
    "title": "Exploratory Data Analysis #1",
    "section": "Variance: quantifying spread",
    "text": "Variance: quantifying spread\nDenote city population by \\([y_1, y_2, ..., y_n]\\) and variance by \\(\\sigma^2\\)\n\\[\n\\begin{aligned}\n\\sigma^2 &= \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{n} \\\\\n&= \\frac{(y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\dots + (y_n - \\bar{y})^2}{n}\n\\end{aligned}\n\\]\nA large variance means considerable spreadedness in data."
  },
  {
    "objectID": "sessions/week1_lecture.html#standard-deviation-sigma",
    "href": "sessions/week1_lecture.html#standard-deviation-sigma",
    "title": "Exploratory Data Analysis #1",
    "section": "Standard deviation \\(\\sigma\\)",
    "text": "Standard deviation \\(\\sigma\\)\n\\[\n\\begin{aligned}\n\\text{Standard Deviation} = \\sqrt{\\text{Variance}}\n\\end{aligned}\n\\]\n\nUnit: if \\(y\\) is in unit of meter (m), then St. Dev is in unit of m, variance in unit of \\(\\text{m}^2\\)"
  },
  {
    "objectID": "sessions/week1_lecture.html#summary-of-key-metrics",
    "href": "sessions/week1_lecture.html#summary-of-key-metrics",
    "title": "Exploratory Data Analysis #1",
    "section": "Summary of key metrics",
    "text": "Summary of key metrics\n\n\nQuantity\nThe middle: mean, median, mode\nThe spread: variance, standard deviation\nVisualising data is also important: BOXPLOT!\nPlease check two types of SPECIAL VALUES before computing these metrics."
  },
  {
    "objectID": "sessions/week1_lecture.html#ft-visual-vocabulary",
    "href": "sessions/week1_lecture.html#ft-visual-vocabulary",
    "title": "Exploratory Data Analysis #1",
    "section": "FT visual vocabulary",
    "text": "FT visual vocabulary\n\nhttps://ft-interactive.github.io/visual-vocabulary/"
  },
  {
    "objectID": "sessions/week1_lecture.html#boxplot-for-a-dataset",
    "href": "sessions/week1_lecture.html#boxplot-for-a-dataset",
    "title": "Exploratory Data Analysis #1",
    "section": "Boxplot for a dataset",
    "text": "Boxplot for a dataset\n\nShowing distribution of a dataset"
  },
  {
    "objectID": "sessions/week1_lecture.html#boxplot-for-comparing-multiple-datasets",
    "href": "sessions/week1_lecture.html#boxplot-for-comparing-multiple-datasets",
    "title": "Exploratory Data Analysis #1",
    "section": "Boxplot for comparing multiple datasets",
    "text": "Boxplot for comparing multiple datasets"
  },
  {
    "objectID": "sessions/week1_lecture.html#when-boxplot-fails",
    "href": "sessions/week1_lecture.html#when-boxplot-fails",
    "title": "Exploratory Data Analysis #1",
    "section": "When boxplot fails?",
    "text": "When boxplot fails?\n\nWhen there are lots of ‘outliers’ in the data\nWhat are ‘Outliers’?"
  },
  {
    "objectID": "sessions/week1_lecture.html#null-values",
    "href": "sessions/week1_lecture.html#null-values",
    "title": "Exploratory Data Analysis #1",
    "section": "Null values",
    "text": "Null values\n\nRepresenting the absence of data or an unknow value\nDifferent from zero or a blank space\nNull values should be excluded before computing mean or std"
  },
  {
    "objectID": "sessions/week1_lecture.html#null-island-in-geography",
    "href": "sessions/week1_lecture.html#null-island-in-geography",
    "title": "Exploratory Data Analysis #1",
    "section": "Null island in geography",
    "text": "Null island in geography\n\nlong=0, lat=0. Not even an island!\nIt became famous because some software assigned “0,0” for long-lat when location is missing\nMany events were mapped to this one, including lots on Strava\nSource: wikipedia.org"
  },
  {
    "objectID": "sessions/week1_lecture.html#outliers",
    "href": "sessions/week1_lecture.html#outliers",
    "title": "Exploratory Data Analysis #1",
    "section": "Outliers",
    "text": "Outliers\n\nA data point that differs significantly from other observations\nThere are three types of outliers\nPlease explain the rationale for removing any identified outliers, including the criteria and methods used"
  },
  {
    "objectID": "sessions/week1_lecture.html#outliers-1",
    "href": "sessions/week1_lecture.html#outliers-1",
    "title": "Exploratory Data Analysis #1",
    "section": "Outliers",
    "text": "Outliers\n\n\n\n\n\n\n\n\nType\nSource\nHandling\n\n\n\n\nError Outliers\nFrom mistakes in data collection/entry/measurement, e.g. a temperature sensor reading 500 °C\nShould be corrected or removed\n\n\nIrregular Pattern Outliers\n\n\n\n\nInfluential Outliers"
  },
  {
    "objectID": "sessions/week1_lecture.html#outliers-2",
    "href": "sessions/week1_lecture.html#outliers-2",
    "title": "Exploratory Data Analysis #1",
    "section": "Outliers",
    "text": "Outliers\n\n\n\n\n\n\n\n\nType\nSource\nHandling\n\n\n\n\nError Outliers\n\n\n\n\nIrregular Pattern Outliers\nGenuinely occur, but do not follow general pattern or relationship in the dataset, e.g. sudden spikes in sales in Black Friday\nThey might indicate unusual events or anomalies worth investigating. If the purpose is to study overall pattern, they should be removed\n\n\nInfluential Outliers"
  },
  {
    "objectID": "sessions/week1_lecture.html#outliers-3",
    "href": "sessions/week1_lecture.html#outliers-3",
    "title": "Exploratory Data Analysis #1",
    "section": "Outliers",
    "text": "Outliers\n\n\n\n\n\n\n\n\nType\nSource\nHandling\n\n\n\n\nError Outliers\n\n\n\n\nIrregular Pattern Outliers\n\n\n\n\nInfluential Outliers\nAppear extreme but are integral to the underlying pattern or model, e.g. NYC in US city population data\nShould keep them, as removing them could distort the analysis or overlook important features of the data"
  },
  {
    "objectID": "sessions/week1_lecture.html#detecting-outlier-interquartile-range-iqr",
    "href": "sessions/week1_lecture.html#detecting-outlier-interquartile-range-iqr",
    "title": "Exploratory Data Analysis #1",
    "section": "Detecting outlier: interquartile range (IQR)",
    "text": "Detecting outlier: interquartile range (IQR)"
  },
  {
    "objectID": "sessions/week1_lecture.html#quiz-which-type-of-outliers",
    "href": "sessions/week1_lecture.html#quiz-which-type-of-outliers",
    "title": "Exploratory Data Analysis #1",
    "section": "Quiz: which type of outliers?",
    "text": "Quiz: which type of outliers?\n\nerror vs. irregular pattern vs. influential\n\n\nIn building height data, 10-storey building with 1m height\nIn gender ratios of countries data, Vatican City with gender ratio of 7:1\nIn UK city GDP data, GDP of Greater London"
  },
  {
    "objectID": "sessions/week1_lecture.html#summary-describing-a-dataset",
    "href": "sessions/week1_lecture.html#summary-describing-a-dataset",
    "title": "Exploratory Data Analysis #1",
    "section": "Summary: describing a dataset",
    "text": "Summary: describing a dataset\n\nKey metrics\nNull values and outliers (and handle them!)\nVisualising data"
  },
  {
    "objectID": "sessions/week2.html",
    "href": "sessions/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "Continuing on the theme of exploratory data analysis, this week we introduce common probability distributions, discuss the risks of biased data, and get to grips with exponential functions. We’ll be covering ideas from statistics and probability, so if this is new to you, or something you haven’t studied in a while, I’d recommend checking out some of the resources below.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#introduction",
    "href": "sessions/week2.html#introduction",
    "title": "Week 2",
    "section": "",
    "text": "Continuing on the theme of exploratory data analysis, this week we introduce common probability distributions, discuss the risks of biased data, and get to grips with exponential functions. We’ll be covering ideas from statistics and probability, so if this is new to you, or something you haven’t studied in a while, I’d recommend checking out some of the resources below.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#learning-objectives",
    "href": "sessions/week2.html#learning-objectives",
    "title": "Week 2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nDescribe the characteristic features of common probability distributions.\nCalculate exponentials and logarithms.\nEvaluate whether a dataset is representative.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#lecture",
    "href": "sessions/week2.html#lecture",
    "title": "Week 2",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#quiz",
    "href": "sessions/week2.html#quiz",
    "title": "Week 2",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#practical",
    "href": "sessions/week2.html#practical",
    "title": "Week 2",
    "section": "Practical",
    "text": "Practical\nTo access the practical:\n\nPreview\nDownload\n\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#further-resources",
    "href": "sessions/week2.html#further-resources",
    "title": "Week 2",
    "section": "Further resources",
    "text": "Further resources\n\nIf you want to brush up on probability and statistics check out this Khan Academy course: https://www.khanacademy.org/math/statistics-probability. In particular I’d recommend unit 9 on ‘Random variables’ and unit 10 on ‘Sampling distributions’.\nIf you’re interested in reading a bit more about the risks of biased data: https://plus.maths.org/content/ai-be-judge-use-algorithms-criminal-justice-system\nIf you’re interested in reading more generally/casually about statistics I’d recommend ‘The Art of Statistics: Learning from Data’ by David Spielgelhalter.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2_practical.html",
    "href": "sessions/week2_practical.html",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "",
    "text": "This week is focussed on using some common functions in Python to plot data and distributions."
  },
  {
    "objectID": "sessions/week2_practical.html#checking-the-data-type",
    "href": "sessions/week2_practical.html#checking-the-data-type",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "Checking the data type",
    "text": "Checking the data type\nLet’s check the data type of each of the columns - based on the metadata I am expecting:\n\n\n\n\n\n\n\n\nVariable name\nMeaning\ndtype\n\n\n\n\nRECTYPE\nRecord type (1=mainstream school; 2=special school; 4=local authority; 5=National (all schools); 7=National (maintained schools))\nint\n\n\nLEA\nLocal authority\nint\n\n\nSCHNAME\nSchool name\nstring\n\n\nTOTPUPS\nNumber of pupils on roll (all ages)\nint\n\n\nEBACCAPS\nAverage EBacc APS score per pupil\nfloat\n\n\nEBACCAPS_LO\nAverage EBacc APS score per pupil with low prior attainment\nfloat\n\n\nEBACCAPS_MID\nAverage EBacc APS score per pupil with middle prior attainment\nfloat\n\n\nEBACCAPS_HI\nAverage EBacc APS score per pupil with high prior attainment\nfloat\n\n\n\n\ndf.dtypes\n\nRECTYPE           int64\nLEA             float64\nSCHNAME          object\nTOTPUPS          object\nEBACCAPS        float64\nEBACCAPS_LO     float64\nEBACCAPS_MID    float64\nEBACCAPS_HI     float64\ndtype: object\n\n\nOK, so we can see that not all the data types are quite as expected. Let’s reformat them to make it easier later.\n\ndf.loc[:, ebaccs_cols] = df.loc[:, ebaccs_cols].apply(pd.to_numeric, errors='coerce')\ndf['TOTPUPS'] = pd.to_numeric(df['TOTPUPS'], errors='coerce').fillna(0).astype('int64')\n\nAnd we can check again:\n\ndf.dtypes\n\nRECTYPE           int64\nLEA             float64\nSCHNAME          object\nTOTPUPS           int64\nEBACCAPS        float64\nEBACCAPS_LO     float64\nEBACCAPS_MID    float64\nEBACCAPS_HI     float64\ndtype: object"
  },
  {
    "objectID": "sessions/week2_practical.html#check-how-much-data-is-missing",
    "href": "sessions/week2_practical.html#check-how-much-data-is-missing",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "Check how much data is missing",
    "text": "Check how much data is missing\nLet’s check how much data is missing for each column in the dataframe - this will help us understand the completeness of the data.\n\n# print how much data is missing for each column\ndf.isna().mean() * 100 \n\nRECTYPE          0.000000\nLEA              0.034406\nSCHNAME          2.683640\nTOTPUPS          0.000000\nEBACCAPS        17.684500\nEBACCAPS_LO     38.775159\nEBACCAPS_MID    41.149148\nEBACCAPS_HI     42.490969\ndtype: float64\n\n\nIt seems suspicious that the school names are missing for some of the entries - let’s check these.\n\n# return rows where SCHNAME is missing\ndf[df['SCHNAME'].isna()]\n\n\n\n\n\n\n\n\nRECTYPE\nLEA\nSCHNAME\nTOTPUPS\nEBACCAPS\nEBACCAPS_LO\nEBACCAPS_MID\nEBACCAPS_HI\n\n\n\n\n3\n4\n201.0\nNaN\n0\nNaN\nNaN\nNaN\nNaN\n\n\n24\n4\n202.0\nNaN\n10865\n4.36\n2.03\n4.19\n6.33\n\n\n55\n4\n203.0\nNaN\n18998\n4.13\n2.14\n4.21\n5.77\n\n\n88\n4\n204.0\nNaN\n15214\n4.72\n2.32\n4.64\n6.72\n\n\n112\n4\n205.0\nNaN\n9905\n5.12\n2.26\n4.93\n6.90\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5748\n4\n941.0\nNaN\n31801\n3.96\n2.16\n4.04\n6.08\n\n\n5776\n4\n942.0\nNaN\n17442\n3.81\n2.08\n3.85\n5.54\n\n\n5810\n4\n943.0\nNaN\n14503\n3.97\n2.29\n3.81\n5.80\n\n\n5811\n5\nNaN\nNaN\n4129401\n3.88\nNaN\nNaN\nNaN\n\n\n5812\n7\nNaN\nNaN\n3688033\n4.05\n2.07\n4.05\n6.09\n\n\n\n\n156 rows × 8 columns\n\n\n\nSo, these are all record type 4, 5, or 7. Looking back at the metadata I see that record types 4, 5 and 7 refer to aggregated data - i.e. they’re not individual schools! Let’s limit the selection to only individual schools, that is 1-mainstream schools and 2-special schools.\n\n# only keep rows where RECTYPE is 1 or 2 \ndf = df[df['RECTYPE'].isin([1, 2])].copy()\n\nNow we can check the data dimensions again.\n\nrows, cols = df.shape\nprint(f\"Rows: {rows}, Columns: {cols}\")\n\nRows: 5657, Columns: 8\n\n\n\nSummary statistics\nWe can use the describe function in pandas to easily get the summary statistics for a dataframe.\n\nnumerical_cols = ['TOTPUPS', 'EBACCAPS', 'EBACCAPS_LO', 'EBACCAPS_MID', 'EBACCAPS_HI']\n\ndf[numerical_cols].describe()\n\n\n\n\n\n\n\n\nTOTPUPS\nEBACCAPS\nEBACCAPS_LO\nEBACCAPS_MID\nEBACCAPS_HI\n\n\n\n\ncount\n5657.000000\n4631.000000\n3406.000000\n3268.000000\n3190.000000\n\n\nmean\n726.067527\n3.428538\n2.090628\n4.033562\n5.782893\n\n\nstd\n554.805752\n1.679245\n0.769925\n0.818922\n0.856998\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.100000\n0.590000\n\n\n25%\n156.000000\n2.820000\n1.830000\n3.560000\n5.250000\n\n\n50%\n750.000000\n3.700000\n2.200000\n3.980000\n5.820000\n\n\n75%\n1143.000000\n4.440000\n2.520000\n4.470000\n6.350000\n\n\nmax\n3440.000000\n8.700000\n5.320000\n7.170000\n8.730000"
  },
  {
    "objectID": "sessions/week2_practical.html#histograms",
    "href": "sessions/week2_practical.html#histograms",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "Histograms",
    "text": "Histograms\nLet’s try plotting the Average EBacc APS score per pupil.\n\nn_bins = 10\nfig, ax = plt.subplots(figsize=(12, 8))\n\nax.hist(df['EBACCAPS'].dropna(), bins=n_bins, color='#abc766', edgecolor='black')\nax.set_title('EBacc distribution')\nax.set_xlabel('EBacc score')\nax.set_ylabel('Number of schools')  \nax.set_xlim(0,9) # the EBacc has a maximum score of 9\n\nplt.show()\n\n\n\n\n\n\n\n\nWhat do you observe from this boxplot? Because the bins are quite large it’s difficult to get a sense of the distribution - try changing this to get a better idea of the data spread.\n\nQuestionAnswer\n\n\nn_bins = ??\nfig, ax = plt.subplots(figsize=(12, 8))\n\nax.hist(df['EBACCAPS'].dropna(), bins=n_bins, color='#abc766', edgecolor='black')\nax.set_title('EBacc distribution')\nax.set_xlabel('EBacc score')\nax.set_ylabel('Number of schools')  \nax.set_xlim(0,9) # the EBacc has a maximum score of 9\n\nplt.show()\n\n\nn_bins = 40\nfig, ax = plt.subplots(figsize=(12, 8))\n\nax.hist(df['EBACCAPS'].dropna(), bins=n_bins, color='#abc766', edgecolor='black')\nax.set_title('EBacc distribution')\nax.set_xlabel('EBacc score')\nax.set_ylabel('Number of schools')  \nax.set_xlim(0,9) # the EBacc has a maximum score of 9\n\nplt.show()\n\n\n\n\n\nIs it normally distributed?\nLooking at the histogram does the data look normally distributed?\nRemember the key features of the normal distribution:\n\nData is continuous\n\nit is something you measure not something you count\n\nData is equally likely to be larger or smaller than average\n\nsymmetric\n\nCharacteristic size, all data points are close to the mean\n\nsingle peak\n\nThere is less data further away from the mean\n\nsmooth tails on both sides\n\n\nOne way to consider whether it is normally distributed is to overlay the normal distribution on top.\nWe can use the package scipy.stats which has functions for generating probability density functions for common distributions. You can see which common distributions here.\n\nimport scipy.stats as sps\n\n# first let's get the mean and stand deviation \nmu = df['EBACCAPS'].dropna().mean()\nstd = df['EBACCAPS'].dropna().std()\n\n## Create the plot \n\n# plot the histogram\nplt.figure(figsize=(12, 8))\nplt.hist(df['EBACCAPS'], bins=40, density=True, color='#abc766', edgecolor='black')\n\n# plot the Probability Density Function (PDF)\nxmin = 0 \nxmax = 9\nx = np.linspace(xmin, xmax, 100)\np = sps.norm.pdf(x, mu, std)\nplt.plot(x, p, linewidth=2, color=\"#e16fca\")\n\nplt.title(f\"Fit results: $\\\\mu$ = {mu:.2f},  $\\\\sigma$ = {std:.2f}\")\nplt.xlim(0, 9)\nplt.xlabel(\"EBacc score\")\nplt.ylabel(\"Density\")\n\nplt.show()\n\n\n\n\n\n\n\n\nNote that this time we plot the histogram with density=True - this scales the histogram data.\nAnother way to think about whether something is normally distirbuted is to look at the Q-Q plot.\n\n\nQ-Q plot\nA Q-Q plot is used to see if a dataset follows a certain distribution, by comparing the quartiles of the two. You can read a bit more about Q-Q plots here.\nQ-Q plots are a good way to check if a dataset follows a normal distribution.\n\nQuestionAnswer\n\n\nThinking about the characteristics of a normal distribution, what would you expect the Q-Q plot of a normally distributed dataset to look like?\n\n\nIf the data is normally distributed, the points in the Q-Q plot will lie close to a straight diagonal line. This happens because the quantiles of the sample match the quantiles of the theoretical normal distribution.\n\n\n\nThe results of the Q-Q plot can help us to understand the dataset:\n\nFor perfectly normal data: points fall almost exactly on the diagonal line\nHeavy tails (more extreme values than normal): points curve away at the end\nLight tails (fewer extreme values than normal): points bend inward at the ends\nSkewed data: points systematically deviate above or below the line\n\nWe can plot the Q-Q plot in Python. Note that before we Q-Q plot the sample data we will need to normalise it - this accounts for the fact that our data is not centred around 0.\n\n# Drop missing values\ndata = df['EBACCAPS'].dropna().values\nn = len(data)\n\n# normalise the data\ndata = (data - np.mean(data)) / np.std(data)\n\n# Sort the sample data\nsample_quantiles = np.sort(data)\n\n# Compute plotting positions (probabilities)\np = (np.arange(1, n+1) - 0.5) / n\n\n# Compute theoretical quantiles from standard normal\ntheoretical_quantiles = sps.norm.ppf(p)\n\n# Plot\nplt.scatter(theoretical_quantiles, sample_quantiles, alpha=0.5, color='#abc766')\nplt.plot(theoretical_quantiles, theoretical_quantiles, color='black', linestyle='--')  # reference line\nplt.title(\"Q-Q plot for EBacc scores\")\nplt.xlabel(\"Theoretical quantiles (Normal)\")\nplt.ylabel(\"Sample quantiles\")\n\nplt.show()\n\n\n\n\n\n\n\n\nOk, so in case it wasn’t clear from the previous plot we can see that the data definitely isn’t perfectly normal."
  },
  {
    "objectID": "sessions/week2_practical.html#whats-causing-the-skew",
    "href": "sessions/week2_practical.html#whats-causing-the-skew",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "What’s causing the skew?",
    "text": "What’s causing the skew?\nIf you remember above this dataset includes schools of two different types, we have mainstream schools and special schools. ‘special schools’ refers to schools which cater to students with special educational needs. Let’s see what happens when we split our histogram according to the type of school.\n\n# histogram of # plot histogram of EBACCAPS by RECTYPE\nn_bins = {1:40, 2:20}\nfig, ax = plt.subplots(figsize=(12, 8))\nfor rectype, color, label in zip([1, 2], ['#abc766', '#66c2ab'], ['Mainstream', 'Special']):\n    ax.hist(df_ks4.loc[df_ks4['RECTYPE'] == rectype, 'EBACCAPS'].dropna(), bins=n_bins[rectype], alpha=0.7, color=color, edgecolor='black', label=label)\nax.set_title('EBacc distribution by school type')\nax.set_xlabel('EBacc score')\nax.set_ylabel('Number of schools')  \nax.set_xlim(0,9)\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nNote that I’ve played with the number of total bins for each group so that the histograms look nice.\nLooking now at the histogram it becomes clear that the skewness of the data is caused by the different school types."
  },
  {
    "objectID": "sessions/week2_practical.html#another-check-for-normality",
    "href": "sessions/week2_practical.html#another-check-for-normality",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "Another check for normality",
    "text": "Another check for normality\nLet’s try again to see if the data is normal - this time splitting the dataset according to the school type.\nWe can start by splitting the dataset according to the school type.\n\nQuestionAnswer\n\n\n# mainstream school\ndf1 = df[??]\n\n# special schools \ndf2 = df[??]\n\n\n# mainstream school\ndf1 = df[df['RECTYPE'] == 1]\n\n# special schools \ndf2 = df[df['RECTYPE'] == 2]\n\n\n\n\nQ-Q plot for mainstream schools\n\nQuestionAnswer\n\n\n# Drop missing values\ndata = df1['EBACCAPS'].dropna().values\nn = len(data)\n\n# normalise the data\ndata = (data - np.mean(data)) / np.std(data)\n\n# Sort the sample data\nsample_quantiles = ??\n\n# Compute plotting positions (probabilities)\np = ??\n\n# Compute theoretical quantiles from standard normal\ntheoretical_quantiles = ??\n\n# Plot\nplt.scatter(theoretical_quantiles, sample_quantiles, alpha=0.5, color='#abc766')\nplt.plot(theoretical_quantiles, theoretical_quantiles, color='black', linestyle='--')  # reference line\nplt.title(\"Mainstream schools: Q-Q plot for EBacc scores\")\nplt.xlabel(\"Theoretical quantiles (Normal)\")\nplt.ylabel(\"Sample quantiles\")\n\nplt.show()\n\n\n# Drop missing values\ndata = df1['EBACCAPS'].dropna().values\nn = len(data)\n\n# normalise the data\ndata = (data - np.mean(data)) / np.std(data)\n\n# Sort the sample data\nsample_quantiles = np.sort(data)\n\n# Compute plotting positions (probabilities)\np = (np.arange(1, n+1) - 0.5) / n\n\n# Compute theoretical quantiles from standard normal\ntheoretical_quantiles = sps.norm.ppf(p)\n\n# Plot\nplt.scatter(theoretical_quantiles, sample_quantiles, alpha=0.5, color='#abc766')\nplt.plot(theoretical_quantiles, theoretical_quantiles, color='black', linestyle='--')  # reference line\nplt.title(\"Mainstream schools: Q-Q plot for EBacc scores\")\nplt.xlabel(\"Theoretical quantiles (Normal)\")\nplt.ylabel(\"Sample quantiles\")\n\nplt.show()"
  },
  {
    "objectID": "sessions/week2_practical.html#q-q-plot-for-special-schools",
    "href": "sessions/week2_practical.html#q-q-plot-for-special-schools",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "Q-Q plot for special schools",
    "text": "Q-Q plot for special schools\n\nQuestionAnswer\n\n\n# Drop missing values\ndata = df2['EBACCAPS'].dropna().values\nn = len(data)\n\n# normalise the data\ndata = (data - np.mean(data)) / np.std(data)\n\n# Sort the sample data\nsample_quantiles = ??\n\n# Compute plotting positions (probabilities)\np = ??\n\n# Compute theoretical quantiles from standard normal\ntheoretical_quantiles = ??\n\n# Plot\nplt.scatter(theoretical_quantiles, sample_quantiles, alpha=0.5, color='#abc766')\nplt.plot(theoretical_quantiles, theoretical_quantiles, color='black', linestyle='--')  # reference line\nplt.title(\"Special schools: Q-Q plot for EBacc scores\")\nplt.xlabel(\"Theoretical quantiles (Normal)\")\nplt.ylabel(\"Sample quantiles\")\n\nplt.show()\n\n\n# Drop missing values\ndata = df2['EBACCAPS'].dropna().values\nn = len(data)\n\n# normalise the data\ndata = (data - np.mean(data)) / np.std(data)\n\n# Sort the sample data\nsample_quantiles = np.sort(data)\n\n# Compute plotting positions (probabilities)\np = (np.arange(1, n+1) - 0.5) / n\n\n# Compute theoretical quantiles from standard normal\ntheoretical_quantiles = sps.norm.ppf(p)\n\n# Plot\nplt.scatter(theoretical_quantiles, sample_quantiles, alpha=0.5, color='#abc766')\nplt.plot(theoretical_quantiles, theoretical_quantiles, color='black', linestyle='--')  # reference line\nplt.title(\"Special schools: Q-Q plot for EBacc scores\")\nplt.xlabel(\"Theoretical quantiles (Normal)\")\nplt.ylabel(\"Sample quantiles\")\n\nplt.show()\n\n\n\n\nLooking at the resultant Q-Q plots what can we say about the data? We see that the mainstream schools is a skewed dataset, whilst the special schools dataset has a heavy tail."
  },
  {
    "objectID": "sessions/week2_practical.html#extension",
    "href": "sessions/week2_practical.html#extension",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "Extension",
    "text": "Extension\n\nEBacc by prior attainment\nIf you’ve finished looking at the histogram plots of the EBacc data, try plotting the histograms of the columns ‘EBACCAPS_LO’, ‘EBACCAPS_MID’, ‘EBACCAPS_HI’. What can you say about these data distributions? Does plotting the Q-Q plots help?\n\nQuestionAnswer\n\n\nn_bins = ??\nfig, ax = plt.subplots(3, 1, figsize=(12, 18))\nplot_names = ['EBacc low prior', 'EBacc mid prior', 'EBacc high prior']\n\nfor i, col in ??:\n    ax[i].hist(df[col].dropna(), bins=n_bins, color='#abc766', edgecolor='black')\n    ax[i].set_title(plot_names[i])\n    ax[i].set_xlabel('EBacc score')\n    ax[i].set_ylabel('Number of schools')  \n    ax[i].set_xlim(0,9) # the EBacc has a maximum score of 9\n    \nplt.show()\n\n\nn_bins = 40\nfig, ax = plt.subplots(3, 1, figsize=(12, 18))\nplot_names = ['EBacc low prior', 'EBacc mid prior', 'EBacc high prior']\n\nfor i, col in enumerate(['EBACCAPS_LO', 'EBACCAPS_MID', 'EBACCAPS_HI']):\n    ax[i].hist(df[col].dropna(), bins=n_bins, color='#abc766', edgecolor='black')\n    ax[i].set_title(plot_names[i])\n    ax[i].set_xlabel('EBacc score')\n    ax[i].set_ylabel('Number of schools')  \n    ax[i].set_xlim(0,9) # the EBacc has a maximum score of 9\n\nplt.show()\n\n\n\n\n\n\nEBacc by school size\nIf you’ve finished everything else, then you might like to look at some of the other variables in the dataset. Try plotting the EBacc scores against the total number of pupils (‘TOTPUPS’) - for example using a scatter plot - what can you say about this distribution?"
  },
  {
    "objectID": "sessions/week2_practical.html#youre-done",
    "href": "sessions/week2_practical.html#youre-done",
    "title": "Practical 2: Recognising and plotting probability distributions",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the second QM practical session! If you are still working on it, take your time.\nDon’t worry about understanding every detail of the Python code — what matters most is knowing which functions to use for a specific task, like plotting histograms of data, and knowing how to debug when it goes wrong. Remember, practice makes perfect."
  },
  {
    "objectID": "sessions/week3_lecture.html#motivation",
    "href": "sessions/week3_lecture.html#motivation",
    "title": "Hypothesis Testing",
    "section": "Motivation",
    "text": "Motivation\nHow do we understand how likely events are to occur?\nQuestion\n\nWhat is the probability of someone at UCL being over 190cm?\n\nAnswer\nTry to understand the distribution of heights.\n\nLast week we asked this question and we thought about the distribution of the data. How to describe the distribution of the data mathematically.\nThis week we look at how to properly ask the research question, and how to answer it."
  },
  {
    "objectID": "sessions/week3_lecture.html#the-scientific-method",
    "href": "sessions/week3_lecture.html#the-scientific-method",
    "title": "Hypothesis Testing",
    "section": "The scientific method",
    "text": "The scientific method\n\n\n\n\n\n\nStatistical tests: - formal way to define a threshold of what is an interesting result - and hence evaluate the hypothesis\n\n\nLast week we looked at exploratory data analysis - now the formal process of coming up with and evaluating questions - very much part of the scientific method.\nWhy is this important?\nWe see stories in the news, how do we know what to believe and what not to believe? This is where having an understanding of statistical tests can help us - we can evaluate claims to try and figure out which are/ aren’t statistically significant.\nA clearly defined, and reproducible way to evaluate findings."
  },
  {
    "objectID": "sessions/week3_lecture.html#learning-objectives",
    "href": "sessions/week3_lecture.html#learning-objectives",
    "title": "Hypothesis Testing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should be able to:\n\nEstablish a hypothesis for a given research project.\nDefine the Type I and Type II errors.\nEvaluate a hypothesis using appropriate statistical tests."
  },
  {
    "objectID": "sessions/week3_lecture.html#research-question-vs.-hypothesis",
    "href": "sessions/week3_lecture.html#research-question-vs.-hypothesis",
    "title": "Hypothesis Testing",
    "section": "Research question vs. hypothesis",
    "text": "Research question vs. hypothesis\nResearch question\nA research question focuses on a specific problem.\n\n\nHypothesis\nA formal statement that you will seek to prove or disprove.\n\nThe hypothesis is a statement you can explicitly test. It can either be true or false - and hence using evidence you can come to a judgement about it’s truth value."
  },
  {
    "objectID": "sessions/week3_lecture.html#flip-a-coin",
    "href": "sessions/week3_lecture.html#flip-a-coin",
    "title": "Hypothesis Testing",
    "section": "Flip a coin",
    "text": "Flip a coin\n\n\n\nYou have a coin.\nYou think it’s a fair coin.\nYou toss it 10 times.\nIt comes up heads 7 times.\n\n\n\nWhat do you think is the hypothesis here?\n\nAsk the class.\nHint: which of these is something you can actually test?"
  },
  {
    "objectID": "sessions/week3_lecture.html#the-hypothesis",
    "href": "sessions/week3_lecture.html#the-hypothesis",
    "title": "Hypothesis Testing",
    "section": "The hypothesis",
    "text": "The hypothesis\n\n\n\nYou have a coin.\nYou think it’s a fair coin.\nYou toss it 10 times.\nIt comes up heads 7 times.\n\n\n\nThe hypothesis is that it is a fair coin.\n\n\n\nQuestion for the class - whats the evidence?\nThe 10 coin flips and the observation of 7 heads is the evidence"
  },
  {
    "objectID": "sessions/week3_lecture.html#the-evidence",
    "href": "sessions/week3_lecture.html#the-evidence",
    "title": "Hypothesis Testing",
    "section": "The evidence",
    "text": "The evidence\n\n\n\nYou have a coin.\nYou think it’s a fair coin.\nYou toss it 10 times.\nIt comes up heads 7 times.\n\n\n\n\nThe observations are the evidence."
  },
  {
    "objectID": "sessions/week3_lecture.html#what-question-can-you-ask",
    "href": "sessions/week3_lecture.html#what-question-can-you-ask",
    "title": "Hypothesis Testing",
    "section": "What question can you ask?",
    "text": "What question can you ask?\n\n\n\nYou have a coin.\nYou think it’s a fair coin.\nYou toss it 10 times.\nIt comes up heads 7 times.\n\n\n\nIs it a fair coin?\n\n\nWhat’s the probability that it’s fair?\n\n\nIf the coin is fair, how likely would it be to see 7 heads out of 10 flips?\n\n\n\nAll these questions are difficult to evaluate."
  },
  {
    "objectID": "sessions/week3_lecture.html#what-question-should-you-ask",
    "href": "sessions/week3_lecture.html#what-question-should-you-ask",
    "title": "Hypothesis Testing",
    "section": "What question should you ask?",
    "text": "What question should you ask?\n\n\n\nYou have a coin.\nYou think it’s a fair coin.\nYou toss it 10 times.\nIt comes up heads 7 times.\n\n\nCorrect formulation:\nIf the coin is fair, how likely would it be to see 7 heads out of 10 flips or an even more extreme result?\n\n\nInterested in the probability of seeing the event or something even more extreme."
  },
  {
    "objectID": "sessions/week3_lecture.html#step-1",
    "href": "sessions/week3_lecture.html#step-1",
    "title": "Hypothesis Testing",
    "section": "Step 1",
    "text": "Step 1\nDefine the null and alternative hypothesis\n\n\\(H_0\\) - the null hypothesis\n\nthis is the “status quo”\nit is assumed to be true\n\n\n\n\\(H_1\\) - the alternative hypothesis\n\nyour hypothesis\nit requires some evidence (i.e. data) to verify\nit directly contradicts the null hypothesis\n\n\nBe careful in how the two are defined. Logically H_1 needs to directly contradict H_0.\nFor example if my H_0 was everyone likes chocolate. H_1 couldn’t be some people prefer biscuits - since in this situation both H_0 and H_1 can be true at the same time. H_1 would have to be that not everyone likes chocolate."
  },
  {
    "objectID": "sessions/week3_lecture.html#step-2",
    "href": "sessions/week3_lecture.html#step-2",
    "title": "Hypothesis Testing",
    "section": "Step 2",
    "text": "Step 2\nSet your significance level \\(\\alpha\\)\nThe significance level is the threshold below which you reject the null hypothesis.\n\nDecide what “too unlikely” means\nCommon choice is 5% significance\n\n\\(\\alpha = 0.05\\)\nThis means that if we see evidence that would have less than a 5% chance of occurring under the null hypothesis, then we reject the null hypothesis."
  },
  {
    "objectID": "sessions/week3_lecture.html#warning",
    "href": "sessions/week3_lecture.html#warning",
    "title": "Hypothesis Testing",
    "section": "WARNING",
    "text": "WARNING\nDecide what “too unlikely” means before you do the test\n\n\n\n\n\n\n\notherwise considered ‘HARKing’\n\nHypothesising\nAfter\nthe\nResults\nare\nKnown\n\n\n\n\nIt’s important to decide on your significance level prior, as otherwise you might find an event which is significant at the 10% threshold and not at 5% - which could lead to bad scientific practise.\nThere’s a balance - often we don’t know what will be interesting - so you do some exploratory data analysis. But you shouldn’t be fishing around for stuff - needs to be grounded in theory and literature."
  },
  {
    "objectID": "sessions/week3_lecture.html#step-3",
    "href": "sessions/week3_lecture.html#step-3",
    "title": "Hypothesis Testing",
    "section": "Step 3",
    "text": "Step 3\nIdentify the evidence\n\nThis could mean collecting the data\nOr identifying a suitable existing dataset\n\nCrucial that it’s suitable - think about biased/ unrepresentative data\n\n\n\nIn quant methods this normally means finding an open public dataset to use.\nThe evidence needs to be suitable to answer the question - think about biased/representative data from last week. Something they will be assessed on in the assessment."
  },
  {
    "objectID": "sessions/week3_lecture.html#step-4",
    "href": "sessions/week3_lecture.html#step-4",
    "title": "Hypothesis Testing",
    "section": "Step 4",
    "text": "Step 4\nCalculate the p-value\nThe p-value is the probability of seeing the evidence, or something even more extreme, if the null hypothesis is true.\n\nCalculated according to the appropriate statistical test\nThe choice of test is determined by the research question and the data\n\n\nWe’ll come back to different types of statistical test later in the lecture"
  },
  {
    "objectID": "sessions/week3_lecture.html#step-5",
    "href": "sessions/week3_lecture.html#step-5",
    "title": "Hypothesis Testing",
    "section": "Step 5",
    "text": "Step 5\nCompare p-value with significance level\n\np-value \\(&gt; \\alpha\\)\n\nEvidence not that unlikely.\nNot enough evidence to reject \\(H_0\\).\n\np-value \\(\\leq \\alpha\\)\n\nEvidence very unlikely.\nReject \\(H_0\\) and accept \\(H_1\\)."
  },
  {
    "objectID": "sessions/week3_lecture.html#the-steps",
    "href": "sessions/week3_lecture.html#the-steps",
    "title": "Hypothesis Testing",
    "section": "The steps",
    "text": "The steps\nIn order to evaluate our hypothesis we just have to do the five steps:\n\nDefine the null and alternative hypothesis\nSet you significance level\nIdentify the evidence\nCalculate the p-value\nCompare p-value with hypothesis level"
  },
  {
    "objectID": "sessions/week3_lecture.html#type-i-error",
    "href": "sessions/week3_lecture.html#type-i-error",
    "title": "Hypothesis Testing",
    "section": "Type I error",
    "text": "Type I error\nThe true null hypothesis is incorrectly rejected.\nThe null hypothesis is true, but you get a false positive leading to you rejecting the null hypothesis.\nThis is also called a false positive.\n\n\nExample: In court a defendant is found guilty despite being innocent."
  },
  {
    "objectID": "sessions/week3_lecture.html#type-ii-error",
    "href": "sessions/week3_lecture.html#type-ii-error",
    "title": "Hypothesis Testing",
    "section": "Type II error",
    "text": "Type II error\nThe false null hypothesis is incorrectly accepted.\nThe null hypothesis is false, but you get a false negative result, leading you to accepting the null hypothesis.\nThis is also called a false negative.\n\n\nExample: In court a defendant is found innocent despite being guilty."
  },
  {
    "objectID": "sessions/week3_lecture.html#example-mammograms",
    "href": "sessions/week3_lecture.html#example-mammograms",
    "title": "Hypothesis Testing",
    "section": "Example: Mammograms",
    "text": "Example: Mammograms\n\n\n\nNHS offers breast cancer screening for all people with breasts between the ages of 50 and 70.\n\nThe idea is to screen all those in the population who are at high risk of breast cancer - in the hope they pick up results better.\nWhilst the tests are good they’re not 100% accurate."
  },
  {
    "objectID": "sessions/week3_lecture.html#example-screening-outcomes",
    "href": "sessions/week3_lecture.html#example-screening-outcomes",
    "title": "Hypothesis Testing",
    "section": "Example: Screening outcomes",
    "text": "Example: Screening outcomes\nThe hypothesis:\n\\(H_0:\\) The individual doesn’t have breast cancer.\n\\(H_1:\\) The individual does have breast cancer."
  },
  {
    "objectID": "sessions/week3_lecture.html#example-evaluating-the-evidence",
    "href": "sessions/week3_lecture.html#example-evaluating-the-evidence",
    "title": "Hypothesis Testing",
    "section": "Example: Evaluating the evidence",
    "text": "Example: Evaluating the evidence\nFrom NHS digital.\nType I error: false positive\n\nIn 2020-2021, 4.0% of those screened had an abnormal results and were referred for assessment.\nOf these, 77.1% were found to not have breast cancer at follow up assessment.\nLeading to a false positive rate of 3.1%.\n\n\nThis false positive rate is calculated as 4% times 77.1%\nFalse positive is bad as you might pursue a healthcare treatment which is actually unecessary for the patient."
  },
  {
    "objectID": "sessions/week3_lecture.html#example-evaluating-the-evidence-1",
    "href": "sessions/week3_lecture.html#example-evaluating-the-evidence-1",
    "title": "Hypothesis Testing",
    "section": "Example: Evaluating the evidence",
    "text": "Example: Evaluating the evidence\nFrom NHS digital.\nType II error: false negative\n\nThose who had a negative screening but did in fact have breast cancer.\nHarder to calculate the false negative rate, as they might be diagnosed with breast cancer at any later point in time.\nStudies suggest the false negative rate could be as high as 20%.\n\n\nFalse negative - in healthcare context this is the worst outcome - as goes undiagnosed and hence untreated."
  },
  {
    "objectID": "sessions/week3_lecture.html#matrix-of-errors",
    "href": "sessions/week3_lecture.html#matrix-of-errors",
    "title": "Hypothesis Testing",
    "section": "Matrix of errors",
    "text": "Matrix of errors\n\n\n\n\nPossible outcomes form a matrix. Have two good outcomes, and two bad outcomes which we want to look out for (these are the Type I and Type II errors).\nThis is something to bear in mind when we look at evaluating our hypotheses. How good is our data, so how reliable is our outcome?\nOften this language is used when evaluating the outcomes of an ML classification model. Where we report the quality of a supervised model in terms of its rate of false positives and false negatives."
  },
  {
    "objectID": "sessions/week3_lecture.html#understanding-the-literature-and-the-context",
    "href": "sessions/week3_lecture.html#understanding-the-literature-and-the-context",
    "title": "Hypothesis Testing",
    "section": "Understanding the literature and the context",
    "text": "Understanding the literature and the context\nThe hypothesis should not come out of thin air.\n\nShould consider:\n\nWhat do you know about the context?\nWhat research have other people done?"
  },
  {
    "objectID": "sessions/week3_lecture.html#asking-ethical-hypothesis-questions",
    "href": "sessions/week3_lecture.html#asking-ethical-hypothesis-questions",
    "title": "Hypothesis Testing",
    "section": "Asking ethical hypothesis questions",
    "text": "Asking ethical hypothesis questions\nIt’s important to not make unethical assumptions in choosing the hypothesis.\n\nExample\nPolice profiling - assumes a correlation between appearance and crime\n\nUse contextual knowledge\nIs this causation?\nOr correlation linked to other factors?\n\n\nThere is an assumption that theres a link between how someone looks and what crime they are likley to commit - leading to potentially racist police profiling - as assuming a link between ethnicity and likelihood to commit a crime."
  },
  {
    "objectID": "sessions/week3_lecture.html#correlation-vs.-causation",
    "href": "sessions/week3_lecture.html#correlation-vs.-causation",
    "title": "Hypothesis Testing",
    "section": "Correlation vs. Causation",
    "text": "Correlation vs. Causation\nCorrelation: Two variables are statistically related, as one changes so does the other.\nCausation: One variable influences the other variable to occur.\n\n\nCausation implies correlation.\nBUT correlation does not imply causation!\n\nLots of things can be correlated BUT it doesn’t mean one event caused another."
  },
  {
    "objectID": "sessions/week3_lecture.html#aliens-and-librarians",
    "href": "sessions/week3_lecture.html#aliens-and-librarians",
    "title": "Hypothesis Testing",
    "section": "Aliens and librarians",
    "text": "Aliens and librarians\n\n\n\nImage credit: [Spurious Correlations](https://www.tylervigen.com/spurious/correlation/19598_google-searches-for-report-ufo-sighting_correlates-with_the-number-of-librarians-in-hawaii)\n\n\n\nIt’s a dumb example but it’s quite easy to find nonsense correlations between random variables. Can generate your own at the website spurious correlations."
  },
  {
    "objectID": "sessions/week3_lecture.html#correlation-is-not-causation",
    "href": "sessions/week3_lecture.html#correlation-is-not-causation",
    "title": "Hypothesis Testing",
    "section": "Correlation IS NOT causation",
    "text": "Correlation IS NOT causation\nYou might not know whether events are correlated, or causing each other\n\n\nBUT\n\n\n\nyou should use your contextual understanding to come up with plausible (and ethical) initial questions.\n\nSometimes it’s hard to figure out if two variables are correlated or causational. There can be all sorts of complexities - like confounding factors, which can be hard to spot. The point is to do your best to come up with plausible and ethical intial questions.\nAnd then the point of hypothesis testing is figuring out whether the evidence you have is siginificant enough to support you hypothesis.\nWill return to looking at the mathematical correlation between variables in week 5."
  },
  {
    "objectID": "sessions/week3_lecture.html#the-point-of-the-scientific-method",
    "href": "sessions/week3_lecture.html#the-point-of-the-scientific-method",
    "title": "Hypothesis Testing",
    "section": "The point of the scientific method",
    "text": "The point of the scientific method\n\n\n\n\n\n\nIt’s a process\n\nquestion\ntest\nevaluate\nREPEAT!\n\n\n\nReturning to the idea of the scientific method is that it’s iterative.\nMaybe we initially made the wrong assumptions but we can repeat, and update our ideas."
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-1",
    "href": "sessions/week3_lecture.html#example---step-1",
    "title": "Hypothesis Testing",
    "section": "Example - step 1",
    "text": "Example - step 1\nDefine the null and alternative hypothesis\n\n\\(H_0\\): The mean height of male and female students is the same.\n\\(H_1\\): The mean height of male and female students is different."
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-2",
    "href": "sessions/week3_lecture.html#example---step-2",
    "title": "Hypothesis Testing",
    "section": "Example - step 2",
    "text": "Example - step 2\nSet your significance level\n\n\\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-3",
    "href": "sessions/week3_lecture.html#example---step-3",
    "title": "Hypothesis Testing",
    "section": "Example - step 3",
    "text": "Example - step 3\nIdentify the evidence\n\n\nI’ve collected data from 198 students, as follows:\n\n\n\nGroup\nSample Size\nMean (cm)\nstd (cm)\n\n\n\n\nFemale students\n95\n170\n5\n\n\nMale students\n103\n180\n6"
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-4",
    "href": "sessions/week3_lecture.html#example---step-4",
    "title": "Hypothesis Testing",
    "section": "Example - step 4",
    "text": "Example - step 4\nCalculate the p-value\n\nAha!\n\n\nHow do we do this? We need to know what statistical test to use!"
  },
  {
    "objectID": "sessions/week3_lecture.html#parametric-vs.-non-parametric-tests",
    "href": "sessions/week3_lecture.html#parametric-vs.-non-parametric-tests",
    "title": "Hypothesis Testing",
    "section": "Parametric vs. Non-parametric tests",
    "text": "Parametric vs. Non-parametric tests\nParametric tests\n\nAssumptions about the distribution:\n\nNormal distribution\nIndependent and unbiased samples\nEqual/comparable variances\nContinuous data\n\n\n\nNon-parametric tests\n\nTypically less assumptions on the distribution\nContinuous or discrete data\n\n\nTwo main categories of test."
  },
  {
    "objectID": "sessions/week3_lecture.html#deciding-on-a-test",
    "href": "sessions/week3_lecture.html#deciding-on-a-test",
    "title": "Hypothesis Testing",
    "section": "Deciding on a test",
    "text": "Deciding on a test\n\nIt can be hard to figure out what test to use.\nGoing to cover some common tests\nMight also find these useful:\n\nflowchart\ntable\n\n\n\nSometimes its clear what test to use, but sometimes it’s not. All sorts of obscure statistical tests from different disciplines.\nFocusing on a few tests which might be useful for the types of data we’re likely to come across.\nAdditional resources for identifying tests - typically based on flowchart of whether assumptions are met or not."
  },
  {
    "objectID": "sessions/week3_lecture.html#students-t-test",
    "href": "sessions/week3_lecture.html#students-t-test",
    "title": "Hypothesis Testing",
    "section": "Student’s T-test",
    "text": "Student’s T-test\n\n\nStudent’s T-test is used to compare the mean of a dataset.\n\nparametric statistical test\nassumes the data is normally distributed\n\n\n\n\n\nThis is William Sealy Gosset - he was **not** a student.\nImage credit: https://en.wikipedia.org/wiki/William_Sealy_Gosset#/media/File:William_Sealy_Gosset.jpg\n\n\n\n\nWilliam Gosset was working for Guinness brewing company when he came up with the t-test - but his company wanted his to publish under a pseudonym - hence ‘student’. He was comparing the chemical properties of different samples of barley."
  },
  {
    "objectID": "sessions/week3_lecture.html#students-t-test-steps",
    "href": "sessions/week3_lecture.html#students-t-test-steps",
    "title": "Hypothesis Testing",
    "section": "Student’s T-test: steps",
    "text": "Student’s T-test: steps\nCalculate:\n\ntest statistic (called the t value)\n\nThe test statistic is a number that summarises the data so as to determine whether to reject the null hypothesis.\n\ndegrees of freedom\n\nThe number of degrees of freedom is the number of values in the final calculation that are free to vary.\n\n\n\nWhich we use to identify the p value - typically using a ‘look up table’.\n\nUse the test statistic and the degrees of freedom to get the p-value. Might remember look up tables from maths at school - instead we’ll be using python."
  },
  {
    "objectID": "sessions/week3_lecture.html#students-t-test-types",
    "href": "sessions/week3_lecture.html#students-t-test-types",
    "title": "Hypothesis Testing",
    "section": "Student’s T-test: types",
    "text": "Student’s T-test: types\n\n\n\nImage credit: https://www.geeksforgeeks.org/data-science/t-test/\n\n\n\n3 main types of test depending on what we’re interested in."
  },
  {
    "objectID": "sessions/week3_lecture.html#students-t-test-one-sample",
    "href": "sessions/week3_lecture.html#students-t-test-one-sample",
    "title": "Hypothesis Testing",
    "section": "Student’s T-test: one sample",
    "text": "Student’s T-test: one sample\nTests whether the population mean is equal to a specific value or not\n\nThe test statistic is calculated as:\n\\[\\begin{align}\nt = \\frac{\\bar{x} - \\mu_{0}}{s / \\sqrt{n}}\n\\end{align}\\]\nwhere\n\n\\(\\bar{x}\\) is the sample mean\n\\(\\mu_{0}\\) is the hypothesised population mean\n\\(s\\) is the sample standard deviation\n\\(n\\) is the sample size"
  },
  {
    "objectID": "sessions/week3_lecture.html#students-t-test-one-sample-degrees-of-freedom",
    "href": "sessions/week3_lecture.html#students-t-test-one-sample-degrees-of-freedom",
    "title": "Hypothesis Testing",
    "section": "Student’s T-test: one sample, degrees of freedom",
    "text": "Student’s T-test: one sample, degrees of freedom\nThe number of degrees of freedom is the number of values in the final calculation that are free to vary.\n\\[\\begin{align}\ndf = n-1\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week3_lecture.html#students-t-test-two-sample",
    "href": "sessions/week3_lecture.html#students-t-test-two-sample",
    "title": "Hypothesis Testing",
    "section": "Student’s T-test: two sample",
    "text": "Student’s T-test: two sample\nTests if the population means for two different groups are equal or not.\n\nThe test statistic is:\n\\[\\begin{align}\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\end{align}\\]\n\n\\(\\bar{x}_1, \\bar{x}_2\\) are the sample means of groups 1 and 2\n\\(n_1, n_2\\) are the sample sizes of groups 1 and 2\n\\(s_p\\) is the pooled standard deviation\n\n\n\n\\[\\begin{align}\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\end{align}\\]\nwith \\(s_1, s_2\\) the sample standard deviations."
  },
  {
    "objectID": "sessions/week3_lecture.html#students-t-test-two-sample-degrees-of-freedom",
    "href": "sessions/week3_lecture.html#students-t-test-two-sample-degrees-of-freedom",
    "title": "Hypothesis Testing",
    "section": "Student’s T-test: two sample, degrees of freedom",
    "text": "Student’s T-test: two sample, degrees of freedom\nThe number of degrees of freedom is the number of values in the final calculation that are free to vary.\nIn the two-sample Student’s T-test the degrees of freedom are:\n\\[\\begin{align}\ndf = n_1 + n_2 - 2\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week3_lecture.html#students-t-test-paired",
    "href": "sessions/week3_lecture.html#students-t-test-paired",
    "title": "Hypothesis Testing",
    "section": "Student’s T-test: paired",
    "text": "Student’s T-test: paired\nTests if the difference between paired measurements for a population is zero or not - normally used with longitudinal data.\n\nThe test statistic is:\n\\[\\begin{align}\nt = \\frac{\\bar{d}}{s_d / \\sqrt{n}}\n\\end{align}\\]\nwhere\n\n\\(\\bar{d}\\) is the mean of the paired differences\n\\(s_d\\) is the standard deviation of the paired differences\n\\(n\\) is the number of pairs\nthe number of degress of freedom is \\(n-1\\)\n\n\nFor example used when I have data from different years, and want to figure out if there’s a difference across the years.\nYou need to be able to pair the data - i.e. the same things being observed at time point 1 and time point 2."
  },
  {
    "objectID": "sessions/week3_lecture.html#how-many-tails",
    "href": "sessions/week3_lecture.html#how-many-tails",
    "title": "Hypothesis Testing",
    "section": "How many tails?",
    "text": "How many tails?\nTests can be one-tailed or two-tailed - which you want is determined when you define the hypothesis.\n\n\n\nOne tailed: if you only care is the mean is significant in one direction\nTwo tailed: if you care about the mean being different regardless of direction\n\nSoemthing else to consider - this becomes relevant when we look up the p-value"
  },
  {
    "objectID": "sessions/week3_lecture.html#kolmogorov-smirnov",
    "href": "sessions/week3_lecture.html#kolmogorov-smirnov",
    "title": "Hypothesis Testing",
    "section": "Kolmogorov-Smirnov",
    "text": "Kolmogorov-Smirnov\n\n\n\n\n\n\n\nCompares two probability distributions\nCan be used to test whether an observed sample came from a given distribution\nOr to test whether two samples both came from the same distribution\n\n\n\nNamed after two Russian soviet mathematicians working in the mid 20th century"
  },
  {
    "objectID": "sessions/week3_lecture.html#k-s-test-one-sample-test",
    "href": "sessions/week3_lecture.html#k-s-test-one-sample-test",
    "title": "Hypothesis Testing",
    "section": "K-S test: one sample test",
    "text": "K-S test: one sample test\nTests if a sample dataset came from a known distribution.\nThe Kolmogorov–Smirnov test statistic is:\n\\[\\begin{align}\nD_n = \\sup_x \\, | F_n(x) - F(x) |\n\\end{align}\\]\nwhere\n\n\\(F_n(x)\\) is the empirical distribution function (EDF) of the sample\n\\(F(x)\\) is the cumulative distribution function (CDF) of the reference distribution\n\n\n\nNote\n‘\\(sup\\)’ is the suprenum - think of it as the smallest upper bound.\n\n\n‘sup’ is a maths - don’t need to know what it means - just intuitively its measuirng the smallest upper bound on the difference between the two distirbutions.\nThinking back to last week we might use the K-S test to determine if a dataset is drawn from a noraml distirbution by comapring to the normal cumulative distribution function."
  },
  {
    "objectID": "sessions/week3_lecture.html#k-s-empirical-distribution-function",
    "href": "sessions/week3_lecture.html#k-s-empirical-distribution-function",
    "title": "Hypothesis Testing",
    "section": "K-S: empirical distribution function",
    "text": "K-S: empirical distribution function\nThe empirical distribution function (EDF) is:\n\\[\\begin{align}\nF_{n}(x) = \\frac{1}{n} \\sum_{i=1}^{n} 1_{(-\\infty ,x]}(X_{i})\n\\end{align}\\]\nwhere\n\n\\(n\\) is the number of observations\n\\(X_i\\) are the ordered sample values\n\\(1_{(-\\infty ,x]}(X_{i})\\) is an indicator function (1 if \\(X_i \\leq x\\), else 0)"
  },
  {
    "objectID": "sessions/week3_lecture.html#k-s-test-two-sample-test",
    "href": "sessions/week3_lecture.html#k-s-test-two-sample-test",
    "title": "Hypothesis Testing",
    "section": "K-S test: two sample test",
    "text": "K-S test: two sample test\nTests if the underlying distributions of two sample datasets are the same.\nFor the two-sample test:\n\\[\\begin{align}\nD_{n,m} = \\sup_x \\, | F_n(x) - G_m(x) |\n\\end{align}\\]\nwhere\n\n\\(F_n(x)\\) and \\(G_m(x)\\) are the EDFs of the two samples."
  },
  {
    "objectID": "sessions/week3_lecture.html#k-s-test-decision-rule",
    "href": "sessions/week3_lecture.html#k-s-test-decision-rule",
    "title": "Hypothesis Testing",
    "section": "K-S test: decision rule",
    "text": "K-S test: decision rule\nThe hypotheses would be:\n\n\\(H_0\\): the distributions are the same\n\\(H_1\\): the distributions differ\n\n\nLarger values of the test statistic \\(D\\) is stronger evidence against \\(H_0\\).\n\nHow do we use the K-S test is practise?"
  },
  {
    "objectID": "sessions/week3_lecture.html#kernel-density-estimate-kde",
    "href": "sessions/week3_lecture.html#kernel-density-estimate-kde",
    "title": "Hypothesis Testing",
    "section": "Kernel density estimate (KDE)",
    "text": "Kernel density estimate (KDE)\n\nUsed to generate a smooth probability density function for a random variable dataset\nUseful for understanding the underlying distribution of a sample\nThink of it as getting a smooth function to describe a histogram of data\nThere are no assumptions about the prior distribution\n\n\nRemember from last week - the probability density function (PDF) describes the likelihood of different outcomes for a continuous random variable"
  },
  {
    "objectID": "sessions/week3_lecture.html#kde-of-simulated-heights",
    "href": "sessions/week3_lecture.html#kde-of-simulated-heights",
    "title": "Hypothesis Testing",
    "section": "KDE of simulated heights",
    "text": "KDE of simulated heights\nIt’s easy to fit a KDE to data in Python:\n\nimport numpy as np\nimport pandas as pd \nfrom scipy.stats import gaussian_kde\n\n# Supposing you have some data \ndata = pd.read_csv('/path_to_data')\n\n# Kernel Density Estimation\nkde = gaussian_kde(data)\nx_vals = np.linspace(100, 200, 100)\ny_vals = kde(x_vals)"
  },
  {
    "objectID": "sessions/week3_lecture.html#kde-of-simulated-heights-1",
    "href": "sessions/week3_lecture.html#kde-of-simulated-heights-1",
    "title": "Hypothesis Testing",
    "section": "KDE of simulated heights",
    "text": "KDE of simulated heights"
  },
  {
    "objectID": "sessions/week3_lecture.html#kde-use-case",
    "href": "sessions/week3_lecture.html#kde-use-case",
    "title": "Hypothesis Testing",
    "section": "KDE use case",
    "text": "KDE use case\n\nfit the KDE to two sample datasets\ncompare visually\ncarry out a non-parametric test - such as Kolmogorov-Smirnov\n\n\nHow might we use a KDE in practise?"
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-1-2-3",
    "href": "sessions/week3_lecture.html#example---step-1-2-3",
    "title": "Hypothesis Testing",
    "section": "Example - step 1, 2, 3",
    "text": "Example - step 1, 2, 3\nDefine the null and alternative hypothesis\n\\(H_0\\): The mean height of male and female students is the same.\n\\(H_1\\): The mean height of male and female students is different.\n\nSet your significance level\n\\(\\alpha = 0.05\\)\n\n\nIdentify the evidence\nGroup 1 – female students\n\\(\\bar{x}_1 = 170\\), \\(s_1 = 5\\), \\(n_1\\) = 95\nGroup 2 – male students\n\\(\\bar{x}_2 = 180\\), \\(s_2 = 6\\), \\(n_2\\) = 103\n\nAny questions about the steps up to this point?"
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-4-1",
    "href": "sessions/week3_lecture.html#example---step-4-1",
    "title": "Hypothesis Testing",
    "section": "Example - step 4",
    "text": "Example - step 4\nCalculate the p-value\n\nUse Student’s T-test: two sample\n\nDon’t care if students are taller or shorter - so use two-tailed test\nCalculate the t value\nCalculate the degrees of freedom\nGet the p value"
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-4---calculate-the-t-value",
    "href": "sessions/week3_lecture.html#example---step-4---calculate-the-t-value",
    "title": "Hypothesis Testing",
    "section": "Example - step 4 - calculate the t value",
    "text": "Example - step 4 - calculate the t value\nSubstituting values:\n\\[\\begin{align}\ns_p &= \\sqrt{\\frac{(95-1)\\cdot 5^2 + (103-1)\\cdot 6^2}{95+103-2}} \\approx 5.55\n\\end{align}\\]\nNow compute \\(t-value\\):\n\\[\\begin{align}\nt &= \\frac{170 - 180}{5.55 \\cdot \\sqrt{\\tfrac{1}{95} + \\tfrac{1}{103}}} \\approx -12.7\n\\end{align}\\]\n\nDon’t need to know the formulas because in practise we’ll be using Python functions to do the calculations for us.\nUsing the right python function is something we’ll cover in todays tutorial."
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-4---calculate-the-degrees-of-freedom",
    "href": "sessions/week3_lecture.html#example---step-4---calculate-the-degrees-of-freedom",
    "title": "Hypothesis Testing",
    "section": "Example - step 4 - calculate the degrees of freedom",
    "text": "Example - step 4 - calculate the degrees of freedom\nFor Student’s T-Test we need degrees of freedom:\n\\[\\begin{align}\ndf = n_1 + n_2 - 2 = 95 + 103 - 2 = 196\n\\end{align}\\]"
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-4---calculate-the-p-value",
    "href": "sessions/week3_lecture.html#example---step-4---calculate-the-p-value",
    "title": "Hypothesis Testing",
    "section": "Example - step 4 - calculate the p value",
    "text": "Example - step 4 - calculate the p value\nTraditionally this uses look up tables - we’re going to use Python.\n\nfrom scipy import stats\n\nt_val = -12.7\ndf = 18\n\n# Two-tailed p-value\np_value_two_tailed = 2 * (1 - stats.t.cdf(abs(t_val), df))\n\nprint(\"Two-tailed p-value:\", p_value_two_tailed)\n\nTwo-tailed p-value: 2.0151302848603336e-10"
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-4---simplified",
    "href": "sessions/week3_lecture.html#example---step-4---simplified",
    "title": "Hypothesis Testing",
    "section": "Example - step 4 - simplified",
    "text": "Example - step 4 - simplified\nWe can do all the previous calulations in one step using the `scipy’ library - we’ll practise this in the tutorial."
  },
  {
    "objectID": "sessions/week3_lecture.html#example---step-5",
    "href": "sessions/week3_lecture.html#example---step-5",
    "title": "Hypothesis Testing",
    "section": "Example - step 5",
    "text": "Example - step 5\nCompare p-value with hypothesis level\nNow we need to compare the p-value to our siginificance level.\np-value = \\(2.0151302848603336e-10 &lt; 0.05\\) = alpha\n\n…we reject \\(H_0\\)!\n\n\nAnd conclude that male and female students have significantly different heights."
  },
  {
    "objectID": "sessions/week3_lecture.html#questions",
    "href": "sessions/week3_lecture.html#questions",
    "title": "Hypothesis Testing",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/week4.html",
    "href": "sessions/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "This week will introduce some concepts from mathematics to help with understanding equations and modelling data.",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#introduction",
    "href": "sessions/week4.html#introduction",
    "title": "Week 4",
    "section": "",
    "text": "This week will introduce some concepts from mathematics to help with understanding equations and modelling data.",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#learning-objectives",
    "href": "sessions/week4.html#learning-objectives",
    "title": "Week 4",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nDefine concept of linear maps.\nCompute linear algebra equations using vectors and matrices.\n\nDescribe how linear algebra relates to solving linear regression.",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#lecture",
    "href": "sessions/week4.html#lecture",
    "title": "Week 4",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#quiz",
    "href": "sessions/week4.html#quiz",
    "title": "Week 4",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#practical",
    "href": "sessions/week4.html#practical",
    "title": "Week 4",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#further-resources",
    "href": "sessions/week4.html#further-resources",
    "title": "Week 4",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4_practical.html",
    "href": "sessions/week4_practical.html",
    "title": "Practical 4: Practising practical Linear Algebra",
    "section": "",
    "text": "This week is focussed on the key ideas from linear algebra which are the building blocks of regression, computational modelling and machine learning.\nIn the tutorial we’re going to explore the mathematical concepts from the lecture through an example."
  },
  {
    "objectID": "sessions/week4_practical.html#a-problem-with-bike-parts",
    "href": "sessions/week4_practical.html#a-problem-with-bike-parts",
    "title": "Practical 4: Practising practical Linear Algebra",
    "section": "A problem with bike parts",
    "text": "A problem with bike parts\n\nBackground\nA factory specialises in producing parts for bicycles: handlebars, brakes and wheels.\n\n\\(x_1\\) — the number of handlebars they can make\n\n\\(x_2\\) — the number of brakes they can make\n\\(x_3\\) — the number of wheels they can make\n\nThese require resources to be made: aluminium, rubber and paint.\nFor each handlebar they need 8 units of aluminium, 4 units of rubber and 4 units of paint. For each brake they need 1 unit of aluminium, 1 unit of rubber and 1 unit of paint. For each wheel they need 10 units of aluminium, 23 units of rubber, and 5 units of paint.\n\nThe factory is having problems with the supply of raw materials. On a given day they have: - \\(b_1=400\\) units of aluminium,\n- \\(b_2=750\\) units of rubber,\n- \\(b_3=200\\) units of paint.\nWhat is the optimal number of each bike part that they can produce?\n\n\nLinear equations\nWe can express the resource constraints as a system of equations. If they are producing \\(x_1, x_2, x_3\\) units of each part we need:\n\\[\n8x_1 + x_2 + 10x_3\n\\]\nof aluminium.\n\\[\n4x_1 + x_2 + 23x_3\n\\]\nof rubber.\n\\[\n4x_1 + x_2 + 5x_3\n\\]\nof paint.\nGiven the contraints of raw materials we can have:\n\\[\n8x_1 + x_2 + 10x_3 = b_1 = 400\n\\\\\n4x_1 + x_2 + 23x_3 = b_2 = 750\n\\\\\n4x_1 + x_2 + 5x_3 = b_3 = 200\n\\]\n\n\nMatrix form\nThis can be written compactly as:\n\\[\n\\begin{pmatrix}\n8 & 1 & 10 \\\\\n4 & 1 & 23 \\\\\n4 & 1 & 5\n\\end{pmatrix}\n\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\end{pmatrix}\n=\n\\begin{pmatrix}b_1\\\\b_2\\\\b_3\\end{pmatrix}.\n\\]\nThat is, \\[\nMx =b\n\\]\nWhere \\(M\\) is the coefficient matrix,\n\\[\nM =\n\\begin{pmatrix}\n8 & 1 & 10 \\\\\n4 & 1 & 23 \\\\\n4 & 1 & 5\n\\end{pmatrix}\n\\]\n\\(x\\) is the vector of the number of each part, i.e. the variables in our equation:\n\\[\nx = \\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}\n\\]\nand \\(b\\) is the resource constraint:\n\\[\nb = \\begin{pmatrix}b_1\\\\b_2\\\\b_3\\end{pmatrix}.\n\\]\n\n\nWriting this is Python\nLet’s start by importing the libraries we need in Python.\n\nimport numpy as np\nimport plotly as plotly\nimport plotly.graph_objects as go\n\nWe can write this same information in Python by creating variables.\n\n# Coefficient matrix\n# M \nM = np.array([[8, 1, 10],\n              [4, 1, 23],\n              [4, 1, 5]], dtype=float)\n\n# Resource availability\n# [b_1, b_2, b_3]\nb = np.array([400, 750, 200], dtype=float)  \n\nIn Python matrices are represented as an array of arrays ([] within other []) - each array [] is a row of the matrix."
  },
  {
    "objectID": "sessions/week4_practical.html#solving-the-equations",
    "href": "sessions/week4_practical.html#solving-the-equations",
    "title": "Practical 4: Practising practical Linear Algebra",
    "section": "Solving the equations",
    "text": "Solving the equations\nWe can use in-built functions in Python to solve the system of linear equations. The library np.linalg provides us with all the matrix operations we need.\n\nChecking the determinant\nFrom the lecture we saw that in order to solve a system of linear equations the corresponding coefficient matrix must be invertible - and this requires the determinant to be non-zero. So let’s start by checking the determinant of M is non-zero.\n\n# Calculate the determinant of M\ndet_M = np.linalg.det(M)\n\nprint(det_M)\n\n-71.99999999999994\n\n\nThe det(M) is non-zero! This means that matrix M is invertible.\n\n\nInverting the matrix\nNow let’s calculate the inverse of M.\n\n\n\n\n\n\nNote\n\n\n\nRemember: The inverse of a matrix M is denoted M^-1 and has the property that M * M^-1 = I, where I is the identity matrix.\n\n\n\n# Invert M\nM_inv = np.linalg.inv(M)\n\nprint(M_inv)\n\n[[ 0.25       -0.06944444 -0.18055556]\n [-1.          0.          2.        ]\n [-0.          0.05555556 -0.05555556]]\n\n\n\n\nThe identity matrix\nLet’s check that \\(MM^{-1}= I\\), where I is the 3x3 identity matrix:\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\]\n\n# Multiply M by its inverse\n\nMM_inv = np.dot(M, M_inv)\nprint(MM_inv)\n\n[[ 1.00000000e+00 -5.55111512e-17  5.55111512e-17]\n [ 0.00000000e+00  1.00000000e+00 -2.77555756e-17]\n [ 0.00000000e+00 -2.77555756e-17  1.00000000e+00]]\n\n\nHmmmm… that’s weird. It’s not quite the identity matrix.\nThe reason it’s not quite the dientity matrix is because we are using floating point values (dtype=float) in Python - which means there’s always going to be a small error - i.e. epsilon - \\(\\epsilon\\))\nRounding the matrix we see that it is very close to the identity.\n\nprint(MM_inv.round(2))\n\n[[ 1. -0.  0.]\n [ 0.  1. -0.]\n [ 0. -0.  1.]]\n\n\nPhew - it is the identity matrix!\n\n\nLet’s solve it!\nAs we saw in the lectures, we can get \\(x\\) as follows:\n\\[\nx = M^{-1}b\n\\]\nWe use the function np.dot() to take the dot product.\n\n# do M^-1 * b to get x_bar\nx = np.dot(M_inv, b)\n\nprint(np.round(x,2))\n\n[11.81  0.   30.56]\n\n\nSo we see that the optimal use of resources is to make 11 handlebrars, 0 brakes and 30 wheels. Potentially some terrifying brake-less bikes!\n\n\nLet’s solve it again!\nSo above we calculated each step of the process. Alternatively we can solve the equations using numpys linear algebra solver, which skips most of the steps.\n\n## Solve the equations using numpy's linear algebra solver\nx = np.linalg.solve(M, b)\n\nprint(np.round(x, 2))\n\n[11.81  0.   30.56]"
  },
  {
    "objectID": "sessions/week4_practical.html#visualising",
    "href": "sessions/week4_practical.html#visualising",
    "title": "Practical 4: Practising practical Linear Algebra",
    "section": "Visualising",
    "text": "Visualising\nIn 2-dimensions and 3-dimensions we can visualise the solution to our set of equations. For two equations the solution will be the point where the two lines intersect. In three dimensiosn the solution is the intersection of the three planes described by the equation.\nWe can plot our equations in 3D space to see this. We we-write the equations as\n\n# Define grid for x1 (handlebars) and x2 (brakes)\nx1 = np.linspace(0, 30, 50)\nx2 = np.linspace(0, 30, 50)\nX1, X2 = np.meshgrid(x1, x2)\n\n# Constraint surfaces (solving for x3 = wheels)\nZ_al = (b[0] - 8*X1 - 1*X2) / 10 # from aluminium\nZ_rb = (b[1] - 4*X1 - 1*X2) / 23 # from rubber\nZ_pn = (b[2] - 4*X1 - 1*X2) / 5 # from paint\n\n# Interactive 3D plot\nfig = go.Figure()\n\nfig.add_surface(x=x1, y=x2, z=Z_al, opacity=0.5, colorscale=\"Blues\", name=\"Aluminium\")\nfig.add_surface(x=x1, y=x2, z=Z_rb, opacity=0.5, colorscale=\"Oranges\", name=\"Rubber\")\nfig.add_surface(x=x1, y=x2, z=Z_pn, opacity=0.5, colorscale=\"Greens\", name=\"Paint\")\n\n# Add solution point\nfig.add_trace(go.Scatter3d(x=[x[0]], y=[x[1]], z=[x[2]],\nmode=\"markers\", marker=dict(size=6, color=\"red\"),\nname=\"Solution\"))\n\nfig.update_layout(\nscene=dict(\nxaxis_title=\"Handlebars\",\nyaxis_title=\"Brakes\",\nzaxis_title=\"Wheels\"\n))\n\nfig.show()\n\n                            \n                                            \n\n\nAnd now we can plot the intersecting planes."
  },
  {
    "objectID": "sessions/week4_practical.html#conclusions",
    "href": "sessions/week4_practical.html#conclusions",
    "title": "Practical 4: Practising practical Linear Algebra",
    "section": "Conclusions",
    "text": "Conclusions\nHere we solved a set of linear equations using algebra. Solving sets of liunear equations is closely related to regression, a topic we will learn about in week 6."
  },
  {
    "objectID": "sessions/week4_practical.html#extension",
    "href": "sessions/week4_practical.html#extension",
    "title": "Practical 4: Practising practical Linear Algebra",
    "section": "Extension",
    "text": "Extension\nIf you have time you can explore implementing an ordinary least squares regression.\nWe have data from the factory - every day for a year they recorded how many handlebars, brakes and wheels they made and the corresponding amount of each resource used. We can load the data as:\n\nimport pandas as pd \n\ndf = pd.read_csv('L4_data/factory_data.csv')\n\ndf.head()\n\n\n\n\n\n\n\n\nx1_handlebars\nx2_brakes\nx3_wheels\naluminium_used\nrubber_used\npaint_used\n\n\n\n\n0\n38\n12\n28\n601.004775\n797.467413\n300.493185\n\n\n1\n28\n29\n31\n560.707839\n862.138639\n286.595862\n\n\n2\n14\n18\n18\n312.346520\n490.527555\n180.463972\n\n\n3\n7\n16\n20\n261.929106\n497.427667\n134.626443\n\n\n4\n20\n18\n4\n201.302208\n207.646869\n105.584988\n\n\n\n\n\n\n\nYou might want to use statsmodels OLS package which you can read about here."
  },
  {
    "objectID": "sessions/week4_practical.html#youre-done",
    "href": "sessions/week4_practical.html#youre-done",
    "title": "Practical 4: Practising practical Linear Algebra",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the linear algebra practical! If you are still working on it, take your time.\nDon’t worry here about understanding every detail of the maths - focus on each of the operations we looked at. Do you know what is being calculated here? Why is that a useful thing to calculate? If you’re struggling to understand this just ask!\nIt takes time to learn - remember practice makes perfect!"
  },
  {
    "objectID": "sessions/week6.html#introduction",
    "href": "sessions/week6.html#introduction",
    "title": "Week 6 - Prof D’s Regression Sessions Vol 1",
    "section": "Introduction",
    "text": "Introduction\nThis week will introduce Linear Regression. The most useful and widely used model in all of statistics. Regression underpins more ‘advanced’ methods like machine learning, but for most situations, it is likely to be the only model you will require.\nBecause of its popularity, regression is also one of the most widely misused models in the spatial data scientist’s toolbox. Therefore, understanding the basics is absolutely fundamental. If you understand the basics, then you stand a better chance of understanding what the more sophisticated methods related to standard linear regression bring to the table.\nLike baking a cake, getting a regression model right is theoretically no more challenging than following a baking recipe - almost anyone can follow the instructions. However, like baking an amazing cake, skill, experience and understanding gained through hours of experimentation, failures, the odd amazing success and lots of perseverance are what make the difference between a great model and a soggy bottom!",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#learning-objectives",
    "href": "sessions/week6.html#learning-objectives",
    "title": "Week 6 - Prof D’s Regression Sessions Vol 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will:\n\nUnderstand what basic bivariate regression model is and how it might be used to explore / describe the relationship between two continuous variables\nUnderstand the absolutely fundamental importance of visualising your data and your model first\nBe able to interpret the various outputs from a statistical regression model - knowing what each means and how not to rely just on things like p-values and R-Squared to determine how good your model is\nAppreciate how the coefficients in a model will likely vary depending on your study population and how representative it is of the wider population you are making inferences about. In doing this, you will understand why ‘degrees of freedom’ are important\nBe ready to go beyond the basics and take your regression modelling further in next week’s Regression Session.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#lecture",
    "href": "sessions/week6.html#lecture",
    "title": "Week 6 - Prof D’s Regression Sessions Vol 1",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#quiz",
    "href": "sessions/week6.html#quiz",
    "title": "Week 6 - Prof D’s Regression Sessions Vol 1",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#practical",
    "href": "sessions/week6.html#practical",
    "title": "Week 6 - Prof D’s Regression Sessions Vol 1",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nMain Practical Page\nIf Quarto has rendered this properly, you might be able to download a notebook here",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#further-resources",
    "href": "sessions/week6.html#further-resources",
    "title": "Week 6 - Prof D’s Regression Sessions Vol 1",
    "section": "Further resources",
    "text": "Further resources\n\nApplied Regression Example\nEarly in 2025, I produced a very rapid piece of regression analysis to contribute to the debate around Brighton and Hove City Council’s proposed changes to the secondary school admission process. This was put together very quickly and as such it’s not perfect. However, it did help shift the public debate in the city and is one example of how regression analysis can be used in a piece of ‘public-facing scholarly writing’ - you can read it here: https://adamdennett.github.io/BH_Schools_Consultation/absence.html\nA follow-one piece (that I am in the process of revising as the methodology is potentially a bit sketchy) can be viewed here - https://adamdennett.github.io/BH_Schools_Consultation/attainment_extra.html\n\n\nFurther Reading\nField, Andy P. 2026. Discovering Statistics Using R and RStudio. London: SAGE Publications. - Andy Field is always my go-to - https://profandyfield.com/discoverse/dsur/\nYou might want to try the Tidyverse and Tidy Models - https://www.tidymodels.org/",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6_practical.html#preamble",
    "href": "sessions/week6_practical.html#preamble",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Preamble",
    "text": "Preamble\nOver the next 3 weeks we are going DEEP into linear regression. The QM Regression Sessions will be tough, but we will be eased through the journey in the company of LTJ Bukem, Blame, Conrad and others. Plug into the Progression Sessions, let them sooth your mind and allow you to get fully into the Regression Sessions!"
  },
  {
    "objectID": "sessions/week6_practical.html#introduction",
    "href": "sessions/week6_practical.html#introduction",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Introduction",
    "text": "Introduction\nThis week’s practical is focused on understanding the most basic form of regression analysis, but in an applied context discovering what these models help you understand about your data in great detail. You will essentially re-create some of the analysis you saw in the lecture, but for slightly different variables and for different places.\nMost of what you will be doing in this practical won’t actually be modelling - although this will come at the end - but you will be getting to know your data so that your final models make sense. This is how it should be!! Going back to our bake-off analogy, the modelling is really just the final bake after hours of preparation."
  },
  {
    "objectID": "sessions/week6_practical.html#aims",
    "href": "sessions/week6_practical.html#aims",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Aims",
    "text": "Aims\nBy the end of this week’s regression session you should:\n\nBe able to use R or Python to process data in order to carry out a scientific investigation\nFeel comfortable in plotting and visualising that processed data to assess relationships between x and y variables\nUnderstand how to use built-in statistical software functions in R and Python to run specify and a basic regression model and produce statistical outputs from those models\nInterpret those model outputs critically in order to evaluate the strenght and direction of the relationships you are observing.\n\n\n\n\n\n\n\nNote\n\n\n\nBelow you will find code-blocks that will allow you to run your analysis in either R or Python. It’s up to you which you decide to use - you might even want to try both to compare (Of course, R will be better)!\nThe Juypter notebook attached to this practical might work, but you are probably better off just firing up RStudio or VS code, creating a new notebook and editing from scratch on your local machine."
  },
  {
    "objectID": "sessions/week6_practical.html#tasks",
    "href": "sessions/week6_practical.html#tasks",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you complete each of the tasks below, 1-4. All of these will be covered in the code examples as you work your way down the page, but this is the full workflow:\n\n\n1. Downloading: Download and read data into your software environment:\n\nSchool locational and attribute information, downloaded from here - https://get-information-schools.service.gov.uk/Downloads - or read it in using the code supplied below. This will include the following datasets:\nedubase_schools = this is the ‘all establishment’ data for every school in England and Wales\n\nSchool-level Key Stage 4 (14-16 GCSE level) attainment and other attribute information. I have put this on Moodle here, but it can also be downloaded from here - https://www.compare-school-performance.service.gov.uk/download-data.\nDownload to a local drive on your computer from Moodle, unzip it and then read it in using the code supplied below. The zip folders contain the following data files in one folder and the associated metadata in the other folder:\n\nengland_ks4final = data on the 2022/23 academic year, school-level, KS4 outcomes and associated statistics\nengland_abs = data on the 2022/23 academic year, school-level absence rates\nengland_census = additional school-level pupil information for the 2022/23 academic year\nengland_school_information = additional school-level institution information such as Ofsted rating etc. for the 2022/23 academic year\nengland_ks4_pupdest = pupil destination data (where students go once they leave school at 16)\n\n\n2. Data Munging: Join, filter, select and subset:\n\nCreate a master file for England called england_school_2022_23. This will be made by joining together all of the school-level data above into a single file and then reducing it in size using:\n\n\nfilter - so that it just contains open secondary schools\n\nselect - so that only a few key variables relating to attainment, progress, deprivation, absence and school quality remain, alongside key locational and other attributes.\n\n\nCreate two local authority data subsets from this main file:\n\nA subset containing all of the secondary schools in one of the 32 London Boroughs (not including the City of London)\nA subset containing all of the secondary schools in any other local authority in England. Any local authority you like - go wild!\n\n\n\nAt the end of this you will have x3 datasets - one national and two local authority subsets\n3. Exploratory Data Analysis\n\nChoose:\n\none attainment related dependent variable from your set\none possible explanatory variable which you think might help explain levels of attainment in schools\n\n\nCreate a series of graphs and statistical outputs to allow you to understand the structure and composition of your data. You should produce:\n\nA histogram for the dependent and independent variables you are analysing for both your two subsets and the national dataset (x6 histograms in total). You might want to augment your histograms to include:\n\nmean and median lines\nlines for the upper and lower quartiles\nindications of outliners\na kernel density smoothing\n\n\nAlternative plots such as violin plots or box plots to compare with the histogram\nPoint Maps of the locations of your schools in your two subsets - points sized by either of your variables\nA scatter plot of your dependent (attainment) and independent (disadvantage) variables (national and two local subsets) - on these plots you might want to include:\n\na regression line of best fit\nthe R-squared, slope and intercept parameters\n\n\n\n\n4. Explanatory Data Analysis - Attainment and Factors Affecting it in Different Parts of England\n\nCarry out a bi-variate regression analysis to explore the relationship between your chosen attainment dependent variable and your single explanatory independent variable, for your two subsets and the national dataset. Questions to Answer:\n\nAfter visualising the relationships using scatter plots, do you think you need to transform your variables (e.g. using a log transformation) before putting into a regression model? Maybe plot scatter plots of your logged variables too!\nWhat are the slope and intercept coefficients in each model? Are they statistically significant and how do vary between your two subsets? How do they compare with the national picture?\nHow do the R-squared values differ and how reliable do you think they are (what do the degrees of freedom tell you)?\nWhat observations can you make about your two local authority study areas relative to the national picture?"
  },
  {
    "objectID": "sessions/week6_practical.html#task-1---download-your-data-and-read-into-your-software-environment",
    "href": "sessions/week6_practical.html#task-1---download-your-data-and-read-into-your-software-environment",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Task 1 - Download your data and read into your software environment",
    "text": "Task 1 - Download your data and read into your software environment\n\nI’ve made this task easy for you by giving you the code to download all of this data. You’ll need to download a selection of files - click on either tab to see the code for each.\nIn order to run this code, you should first download and unzip the data from Moodle, before editing the code below so that you can read the data from a local folder on your computer:\n\n\n\n\n\n\n\nNote\n\n\n\nI would encourage you NOT to just copy and paste this code blindly, but try to understand what it is doing at each stage.\nYou may notice that there are some bits that are redundant (some of the NA code that I was messing with before deciding on a single way of removing NAs).\nWhat am I doing to the URN field? Why might I be doing that?\nWhat is the regex function doing and why might I be searching out those characters? What might happen to the data if I didn’t handle those first?\n\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\nimport numpy as np\nimport janitor\nfrom pathlib import Path\n\n# little function to define the file root on different machines\ndef find_qm_root(start_path: Path = Path.cwd(), anchor: str = \"QM\") -&gt; Path:\n    \"\"\"\n    Traverse up from the start_path until the anchor folder (e.g., 'QM')      is found. Returns the path to the anchor folder.\n    \"\"\"\n    for parent in [start_path] + list(start_path.parents):\n        if parent.name == anchor:\n            return parent\n    raise FileNotFoundError(f\"Anchor folder '{anchor}' not found in path      hierarchy.\")\n  \nqm_root = find_qm_root()\n\n\n# Read CSV file\nedubase_schools = pd.read_csv(\n    \"https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1\",\n    encoding=\"cp1252\",\n    low_memory=False,\n    dtype={\"URN\": str}\n)\nedubase_schools = edubase_schools.clean_names()\n#py_edubase_schools.dtypes\n#dtype_df = py_edubase_schools.dtypes.reset_index()\n#dtype_df.columns = [\"column_name\", \"dtype\"]\n\n########################################\n# Define base path and common NA values - NOTE YOU WILL NEED TO DOWNLOAD THIS DATA AND PUT INTO YOUR OWN base_path LOCATION ON YOUR MACHINE - not this one as this is my path!!!\n#base_path = Path(\"E:/QM/sessions/L6_data/Performancetables_130242/2022-2023\")\nbase_path = qm_root / \"sessions\" / \"L6_data\" / \"Performancetables_130242\" / \"2022-2023\"\n##################################################\nna_values_common = [\"\", \"NA\", \"SUPP\", \"NP\", \"NE\"]\nna_values_extended = na_values_common + [\"SP\", \"SN\"]\nna_values_attainment = na_values_extended + [\"LOWCOV\", \"NEW\"]\nna_values_mats = na_values_common + [\"SUPPMAT\"]\nna_all = [\"\", \"NA\", \"SUPP\", \"NP\", \"NE\", \"SP\", \"SN\", \"LOWCOV\", \"NEW\", \"SUPPMAT\", \"NaN\"]\n\n# Absence data\nengland_abs = pd.read_csv(base_path / \"england_abs.csv\", na_values=na_all, dtype={\"URN\": str})\n\n# Census data\nengland_census = pd.read_csv(base_path / \"england_census.csv\", na_values=na_all, dtype={\"URN\": str})\nengland_census.iloc[:, 4:23] = england_census.iloc[:, 4:23].apply(lambda col: pd.to_numeric(col.astype(str).str.replace('%', '', regex=False), errors=\"coerce\"))\n\n# KS4 MATs performance data\nengland_ks4_mats_performance = pd.read_csv(base_path / \"england_ks4-mats-performance.csv\", na_values=na_all, encoding=\"cp1252\", low_memory=False, dtype={\"URN\": str})\nengland_ks4_mats_performance[\"TRUST_UID\"] = england_ks4_mats_performance[\"TRUST_UID\"].astype(str)\nengland_ks4_mats_performance[\"P8_BANDING\"] = england_ks4_mats_performance[\"P8_BANDING\"].astype(str)\nengland_ks4_mats_performance[\"INSTITUTIONS_INMAT\"] = england_ks4_mats_performance[\"INSTITUTIONS_INMAT\"].astype(str)\n\ncols_to_convert = england_ks4_mats_performance.columns[10:]\nexclude = [\"P8_BANDING\", \"INSTITUTIONS_INMAT\"]\ncols_to_convert = [col for col in cols_to_convert if col not in exclude]\nengland_ks4_mats_performance[cols_to_convert] = england_ks4_mats_performance[cols_to_convert].apply(lambda col: pd.to_numeric(col.astype(str).str.replace('%', '', regex=False), errors=\"coerce\"))\n\n# KS4 pupil destination data\nengland_ks4_pupdest = pd.read_csv(base_path / \"england_ks4-pupdest.csv\", na_values=na_all, dtype={\"URN\": str})\nengland_ks4_pupdest.iloc[:, 7:82] = england_ks4_pupdest.iloc[:, 7:82].apply(lambda col: pd.to_numeric(col.astype(str).str.replace('%', '', regex=False), errors=\"coerce\"))\n\n# KS4 final attainment data\nengland_ks4final = pd.read_csv(base_path / \"england_ks4final.csv\", na_values=na_all, dtype={\"URN\": str})\nstart_col = \"TOTPUPS\"\nend_col = \"PTOTENT_E_COVID_IMPACTED_PTQ_EE\"\ncols_range = england_ks4final.loc[:, start_col:end_col].columns\n# Strip % signs and convert to numeric\nengland_ks4final[cols_range] = england_ks4final[cols_range].apply(\n    lambda col: pd.to_numeric(col.astype(str).str.replace('%', '', regex=False), errors=\"coerce\")\n)\n\n# School information data\nengland_school_information = pd.read_csv(\n    base_path / \"england_school_information.csv\",\n    na_values=na_all,\n    dtype={\"URN\": str},\n    parse_dates=[\"OFSTEDLASTINSP\"],\n    dayfirst=True  # Adjust if needed\n)\n\n\n\nCodelibrary(tidyverse)\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(here)\n\n# Read CSV file\nedubase_schools &lt;- read_csv(\"https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1\") %&gt;% \n  clean_names() %&gt;% \n  mutate(urn = as.character(urn))\n#str(r_edubase_schools)\n\n# Define base path and common NA values - NOTE YOU WILL NEED TO DOWNLOAD THIS DATA AND PUT INTO YOUR OWN base_path LOCATION ON YOUR MACHINE\nbase_path &lt;- here(\"sessions\", \"L6_data\", \"Performancetables_130242\", \"2022-2023\")\nna_common &lt;- c(\"\", \"NA\", \"SUPP\", \"NP\", \"NE\")\nna_extended &lt;- c(na_common, \"SP\", \"SN\")\nna_attainment &lt;- c(na_extended, \"LOWCOV\", \"NEW\")\nna_mats &lt;- c(na_common, \"SUPPMAT\")\nna_all &lt;- c(\"\", \"NA\", \"SUPP\", \"NP\", \"NE\", \"SP\", \"SN\", \"LOWCOV\", \"NEW\", \"SUPPMAT\")\n\n# Absence data\nengland_abs &lt;- read_csv(file.path(base_path, \"england_abs.csv\"), na = na_all) |&gt;\n  mutate(URN = as.character(URN))\n\n# Other School Census data\nengland_census &lt;- read_csv(file.path(base_path, \"england_census.csv\"), na = na_all) |&gt;\n  mutate(URN = as.character(URN)) |&gt;\n  mutate(across(5:23, ~ parse_number(as.character(.))))\n\n# KS4 MATs performance data\n#First read\nengland_ks4_mats_performance &lt;- read_csv(\n  file.path(base_path, \"england_ks4-mats-performance.csv\"),\n  na = na_all\n) |&gt;\n  mutate(\n    TRUST_UID = as.character(TRUST_UID),\n    P8_BANDING = as.character(P8_BANDING),\n    INSTITUTIONS_INMAT = as.character(INSTITUTIONS_INMAT)\n  )\n\n# Then Identify columns to convert (excluding character columns)\ncols_to_convert &lt;- england_ks4_mats_performance |&gt;\n  select(-(1:10), -P8_BANDING, -INSTITUTIONS_INMAT) |&gt;\n  names()\n\n# Then apply parse_number safely to convert characters to numbers\nengland_ks4_mats_performance &lt;- england_ks4_mats_performance |&gt;\n  mutate(across(all_of(cols_to_convert), ~ parse_number(as.character(.))))\n\n\n# KS4 pupil destination data\nengland_ks4_pupdest &lt;- read_csv(file.path(base_path, \"england_ks4-pupdest.csv\"), na = na_all) |&gt;\n  mutate(URN = as.character(URN)) |&gt;\n  mutate(across(8:82, ~ parse_number(as.character(.))))\n\n# KS4 final attainment data\nengland_ks4final &lt;- read_csv(file.path(base_path, \"england_ks4final.csv\"), na = na_all) |&gt;\n  mutate(URN = as.character(URN)) |&gt;\n  mutate(across(TOTPUPS:PTOTENT_E_COVID_IMPACTED_PTQ_EE, ~ parse_number(as.character(.))))\n\n# School information data\nengland_school_information &lt;- read_csv(\n  file.path(base_path, \"england_school_information.csv\"),\n  na = na_all,\n  col_types = cols(\n    URN = col_character(),\n    OFSTEDLASTINSP = col_date(format = \"%d-%m-%Y\")\n  )\n)"
  },
  {
    "objectID": "sessions/week6_practical.html#task-2---data-munging",
    "href": "sessions/week6_practical.html#task-2---data-munging",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Task 2 - Data Munging",
    "text": "Task 2 - Data Munging\n\nUse Python or R to join all of the datasets you loaded in the previous step into a single master file that we will called england_school_2022_23 using the unique school reference number (URN) as the key.\nCreate the master file:\n\n\n\n\n\n\n\n\nCode# Left join england_ks4final with england_abs, census, and school information\nengland_school_2022_23 = (\n    england_ks4final\n    .merge(england_abs, on=\"URN\", how=\"left\")\n    .merge(england_census, on=\"URN\", how=\"left\")\n    .merge(england_school_information, on=\"URN\", how=\"left\")\n    .merge(edubase_schools, left_on=\"URN\", right_on=\"urn\", how=\"left\")\n)\n\n\n\nCode# Left join england_ks4final with england_abs\nengland_school_2022_23 &lt;- england_ks4final %&gt;%\n  left_join(england_abs, by = \"URN\") %&gt;%\n  left_join(england_census, by = \"URN\") %&gt;%\n  left_join(england_school_information, by = \"URN\") %&gt;%\n  left_join(edubase_schools, by = c(\"URN\" = \"urn\"))\n\n\n\n\nIterative Checking and initial exploratory analysis\n\nIt’s never too soon to start exploring your data - even (or indeed especially) in the data ‘munging’ (processing, cleaning etc.) stage.\nYou will probably find yourself iteratively plotting, filtering, plotting again, switching variables, plotting again, over and over again until you are happy with the data you have. This is all part of the Exploratory Data Analysis Process and vital before applying any other methods to your data!\nIf you recall from the lecture, the histogram of attainment 8 in the whole of England dataset looked a bit odd:\n\n\n\n\n\n\n\n\n\nCodeimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Compute statistics\nmedian_value = england_school_2022_23[\"ATT8SCR\"].median(skipna=True)\nmean_value = england_school_2022_23[\"ATT8SCR\"].mean(skipna=True)\nsd_value = england_school_2022_23[\"ATT8SCR\"].std(skipna=True)\n\n# Set up the figure\nplt.figure(figsize=(10, 6))\n\n# Histogram with density\nsns.histplot(\n    data=england_school_2022_23,\n    x=\"ATT8SCR\",\n    stat=\"density\",\n    binwidth=1,\n    color=\"#4E3C56\",\n    alpha=0.4,\n    linewidth=0.5,\n    edgecolor=\"white\"\n)\n\n# Overlay normal distribution curve\nx_vals = np.linspace(\n    england_school_2022_23[\"ATT8SCR\"].min(),\n    england_school_2022_23[\"ATT8SCR\"].max(),\n    500\n)\nnormal_density = (\n    1 / (sd_value * np.sqrt(2 * np.pi))\n) * np.exp(-0.5 * ((x_vals - mean_value) / sd_value) ** 2)\nplt.plot(x_vals, normal_density, color=\"#2E6260\", linewidth=1)\n\n# Add vertical lines for median and mean\nplt.axvline(median_value, color=\"black\", linestyle=\"dotted\", linewidth=1)\nplt.axvline(mean_value, color=\"#F9DD73\", linestyle=\"solid\", linewidth=1)\n\n# Annotate median and mean\nplt.text(median_value, plt.gca().get_ylim()[1]*0.95, f\"Median = {median_value:.1f}\",\n         color=\"black\", ha=\"center\", va=\"top\", fontsize=10)\nplt.text(mean_value, plt.gca().get_ylim()[1]*0.85, f\"Mean = {mean_value:.1f}\",\n         color=\"#F9DD73\", ha=\"center\", va=\"top\", fontsize=10)\n\n# Customize labels and theme\nplt.title(\"Attainment 8 - All Schools England and Wales, 2022/23 Academic Year\")\nplt.xlabel(\"Attainment 8 Score\")\nplt.ylabel(\"Density\")\nsns.despine()\nplt.tight_layout()\n\n# Show the plot\n# plt.show()\n\n\n\n\n\n\n\n\n\n\nCodemedian_value &lt;- median(england_school_2022_23$ATT8SCR, na.rm = TRUE)\nmean_value   &lt;- mean(england_school_2022_23$ATT8SCR, na.rm = TRUE)\nsd_value   &lt;- sd(england_school_2022_23$ATT8SCR, na.rm = TRUE)\n\nggplot(england_school_2022_23, aes(x = ATT8SCR)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = \"#4E3C56\", alpha = 0.4) +\n  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value), color = \"#2E6260\", linewidth = 1) +\n  geom_vline(xintercept = median_value, color = \"black\", linetype = \"dotted\", size = 1) +\n  geom_vline(xintercept = mean_value, color = \"#F9DD73\", linetype = \"solid\", size = 1) +\n  annotate(\"text\",\n           x = median_value, y = Inf,\n           label = paste0(\"Median = \", round(median_value, 1)),\n           vjust = 1.3, color = \"black\", size = 3.5) +\n  annotate(\"text\",\n           x = mean_value, y = Inf,\n           label = paste0(\"Mean = \", round(mean_value, 1)),\n           vjust = 30.5, color = \"#F9DD73\", size = 3.5) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +\n  labs(\n    title = \"Attainment 8 - All Schools England and Wales, 2022/23 Academic Year\",\n    x = \"Attainment 8 Score\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nYou may also recall from the lecture that this odd distribution was related to different types of school in the system. We can create a similar visualisation here:\n\n\n\n\n\n\n\n\n\nCodeimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up the figure\nplt.figure(figsize=(12, 6))\n\n# Boxplot grouped by MINORGROUP\nsns.boxplot(\n    x='ATT8SCR',\n    data=england_school_2022_23,\n    color=\"#EDD971\",\n    fliersize=0,\n    linewidth=1\n)\n\n# Jittered points\nsns.stripplot(\n    x='ATT8SCR',\n    data=england_school_2022_23,\n    hue='MINORGROUP',\n    dodge=False,\n    jitter=True,\n    alpha=0.5,\n    size=4,\n    palette='turbo'\n)\n\n# Customize the plot\nplt.title(\"Attainment 8 - All Schools 2022/23 Academic Year\")\nplt.xlabel(\"Attainment 8 Score\")\nplt.ylabel(\"\")\nplt.legend(title=\"School Type\", loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=2)\nsns.despine()\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCodeggplot(england_school_2022_23, aes(x = ATT8SCR, y = \"\")) +\n  geom_boxplot(fill = \"#EDD971\", alpha = 0.3, outlier.shape = NA) +    \n  geom_jitter(aes(colour = MINORGROUP), height = 0.2, alpha = 0.3, size = 1) + \n  scale_colour_viridis_d(option = \"turbo\") +  # applies the default casaviz discrete palette\n  labs(\n    title = \"Attainment 8 - All Schools 2022/23 Academic Year\",\n    x = \"Attainment 8 Score\",\n    y = NULL,\n    colour = \"School Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.key.size = unit(10, \"mm\"),   # increase dot space\n    legend.text = element_text(size = 10)  # optional: larger legend labels\n  ) + guides(colour = guide_legend(override.aes = list(size = 4)))\n\n\n\n\n\n\n\n\n\n\nFurther filtering and subsetting\n\nHaving joined all of these data frames into a single master dataframe for the whole of England and observed some issues, we will need to filter out some of the rows we don’t want and the columns that we don’t need\nIn the two folders you downloaded from Moodle, one contains all of the data we need, the other contains all of the metadata which tells us what the variables are\nIf you open up, for example ks4_meta.xlsx these are the variables related to attainment for each school.\nSimilarly abbreviations.xlsx attaches some meaning to some of the codes associated with some of these variables, with other metadata files containing other info. Take some time to look at these so you understand the range of variables we’ll be exploring.\n\n\n\n\n\n\n\nNote\n\n\n\nOne of the key filtering asks carried out in the code below is to filter the schools so that we only retain the maintained schools and the academies in the dataset"
  },
  {
    "objectID": "sessions/week6_practical.html#task-2-continued---filtering-selecting-and-subsetting",
    "href": "sessions/week6_practical.html#task-2-continued---filtering-selecting-and-subsetting",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Task 2 (continued) - Filtering, Selecting and Subsetting",
    "text": "Task 2 (continued) - Filtering, Selecting and Subsetting\nThe Code Below already has the syntax to allow you to filter your dataset and drop special schools, independent schools and colleges.\nIt also contains the start of some code to allow you to select a smaller set of variables from the 700-odd in the main file.\n\nAdapt this Python or R code so that it contains these variables plus the additional variables listed below (see the note about how you may need to adjust the names depending on whether using Python or R). - You should end up with 34 in total\nOnce you have created this filtered and reduced file called england_filtered, write it out as a csv (e.g. england_filtered.csv) and save to your local working directory - we will use it again in future practicals\nNow you have your england_filtered file, use this code below as the template for creating two new data subsets:\n\nlondon_borough_sub\nengland_lad_sub These two new files should create a subset which has filtered on the name or code of a London borough and another local authority in England\n\n\n\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\n# Assuming england_filtered is a pandas DataFrame\nengland_filtered.to_csv(\"england_filtered.csv\", index=False)\n\n\n\n\n\nCodelibrary(tidyverse)\nwrite_csv(england_filtered, \"england_filtered.csv\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are unsure of the names of the local authorities in England, use the la_and_region_codes_meta.csv file in your metadata folder to help you. You might want to name your “sub” files according to the names of the local authorities you choose.\n\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\n\n## check all of your variable names first as R and python will call them something different\ncolumn_df = pd.DataFrame({'Column Names': england_school_2022_23.columns.tolist()})\n\n# Define excluded MINORGROUP values\nexcluded_groups = [\"Special school\", \"Independent school\", \"College\", None]\n\n# Filter and select columns\nengland_filtered = (\n    england_school_2022_23[\n        ~england_school_2022_23[\"MINORGROUP\"].isin(excluded_groups) &\n        (england_school_2022_23[\"phaseofeducation_name_\"] == \"Secondary\") &\n        (england_school_2022_23[\"establishmentstatus_name_\"] == \"Open\")\n    ][[\"URN\", \"SCHNAME_x\", \"LEA\", \"TOWN_x\", \"TOTPUPS\", \"ATT8SCR\", \"OFSTEDRATING\", \"MINORGROUP\", \"PTFSM6CLA1A\"]]\n)\n\n\n\nCode# Filter data\nengland_filtered &lt;- england_school_2022_23 %&gt;%\n  filter(!MINORGROUP %in% c(\"Special school\", \"Independent school\", \"College\", NA)) %&gt;%   filter(phase_of_education_name == \"Secondary\") %&gt;% \n  filter(establishment_status_name == \"Open\") %&gt;% \n  select(URN, SCHNAME.x, TOWN.x, TOTPUPS, ATT8SCR, OFSTEDRATING, MINORGROUP, PTFSM6CLA1A)\n\n\n\n\n\n\n\n\n\n\nImportantOther variables to include in your reduced dataset\n\n\n\nOther variables to include in your reduced dataset:\n\nLocal Authority (LEA) Code and Name - LEA and LANAME\n\nTotal Number of Pupils on Roll (all ages) - TOTPUPS\n\nAverage Attainment 8 score per pupil for English element - ATT8SCRENG\n\nAverage Attainment 8 score per pupil for mathematics element - ATT8SCRMAT\n\nAverage Attainment 8 score per disadvantaged pupil - ATT8SCR_FSM6CLA1A\n\nAverage Attainment 8 score per non-disadvantaged pupil - ATT8SCR_NFSM6CLA1A\n\nAverage Attainment 8 score per boy - ATT8SCR_BOYS\n\nAverage Attainment 8 score per girl - ATT8SCR_GIRLS\n\nProgress 8 measure after adjustment for extreme scores - P8MEA\n\nAdjusted Progress 8 measure - disadvantaged pupils - P8MEA_FSM6CLA1A\n\nAdjusted Progress 8 measure - non-disadvantaged pupils - P8MEA_NFSM6CLA1A\n\n% of pupils at the end of key stage 4 who are disadvantaged - PTFSM6CLA1A\n\n% of pupils at the end of key stage 4 who are not disadvantaged - PTNOTFSM6CLA1A\n\n% pupils where English not first language - PNUMEAL\n\n% pupils with English first language - PNUMENGFL\n\n% of pupils at the end of key stage 4 with low prior attainment at the end of key stage 2 - PTPRIORLO\n\n% of pupils at the end of key stage 4 with high prior attainment at the end of key stage 2 - PTPRIORHI\n\nTotal boys on roll (including part-time pupils) - NORB\n\nTotal girls on roll (including part-time pupils) - NORG\n\nPercentage of pupils eligible for FSM at any time during the past 6 years - PNUMFSMEVER\n\nPercentage of overall absence - PERCTOT\n\nPercentage of enrolments who are persistent absentees - PPERSABS10\n\nSchool Type e.g. Voluntary Aided school - SCHOOLTYPE\n\nReligious character - RELCHAR\n\nAdmissions Policy - ADMPOL\n\nAdmissions Policy new definition from 2019 - ADMPOL_PT\n\nGovernment Office Region Name - gor_name\n\nMixed or Single Sex - gender_name\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you joined your datasets earlier, you will find that some variables appear in more than one of the datasets. This may cause a problem now as R and Python will try and distinguish them in the join by appending a .x or .y (R) or _x or _y (Python) so you may get an error when joining, particularly with the SCHOOLTYPE and ADMPOL variables. For these try SCHOOLTYPE.x and ADMPOL.y (underscore for Python) instead."
  },
  {
    "objectID": "sessions/week6_practical.html#task-3---further-exploratory-analysis-and-visual-modelling",
    "href": "sessions/week6_practical.html#task-3---further-exploratory-analysis-and-visual-modelling",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Task 3 - Further Exploratory Analysis and Visual Modelling",
    "text": "Task 3 - Further Exploratory Analysis and Visual Modelling\n\nUsing the example code below as a guide, select your own \\(Y\\) variable related to attainment from your smaller filtered dataset. You have a range of attainment or progress related variables in there\nSelect your own \\(X\\) variable that might help explain attainment, this might relate to disadvantage, prior attainment, absence or anything else that is a continuous variable (DO NOT SELECT A CATEGORICAL VARIABLE AT THIS TIME).\nOnce you have selected your two variables, plot them against each other on a scatter plot to see if there is an obvious relationship - again, use the code below as a guide.\n\n\n\n\n\n\n\nTipAI tip\n\n\n\nAI - yes, that dreaded thing that’s invading everything. Well, it’s hear to stay and while it’s here, you might as well use it to help you with your coding. Of course, I don’t want you to blindly copy things and not understand what’s going on, but if you get stuck with syntax and getting plots to look how you want - feed your favourite AI with your code and get it to help you out.\n\n\nExploratory Data Analysis Plots\n\n\n\n\n\n\nTipQuestions to Answer\n\n\n\nJust from looking at your plots: - How do you expect the intercepts (baseline Y) to differ between your London and non-London local authority (if at all)? Which is higher and which is lower? - How do you think the slope values (influence of X on Y) are different (if at all)?\n\n\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Combine the datasets\ncombined_data = pd.concat([camden_sub, leeds_sub])\n\n# Create a function to make and customize the plot\ndef make_plot():\n    g = sns.FacetGrid(combined_data, col=\"LANAME\", height=5, aspect=1)\n    g.map_dataframe(\n        sns.scatterplot,\n        x=\"PTFSM6CLA1A\",\n        y=\"ATT8SCR\",\n        color=\"steelblue\",\n        alpha=0.7,\n        s=60\n    )\n    g.set_axis_labels(\"% KS4 Pupils Disadvantaged (PTFSM6CLA1A)\", \"Attainment 8 Score (ATT8SCR)\")\n    g.fig.suptitle(\"Attainment 8 vs % Disadvantaged by Local Authority\", fontsize=16)\n    g.fig.tight_layout()\n    g.fig.subplots_adjust(top=0.85)\n    plt.show()\n\n# Call the function once\nmake_plot()\n\n\n\n\n\n\n\n\n\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\n\n# Combine the datasets and label them\ncombined_data &lt;- bind_rows(\n  camden_sub %&gt;% mutate(Area = \"Camden\"),\n  leeds_sub %&gt;% mutate(Area = \"Leeds\")\n)\n\n# Create faceted scatter plot\nggplot(combined_data, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +\n  geom_point(alpha = 0.7, size = 3, color = \"steelblue\") +\n  facet_wrap(~ Area) +\n  labs(\n    title = \"Attainment 8 vs % Disadvantaged by Local Authority\",\n    x = \"% KS4 Pupils Disadvantaged (PTFSM6CLA1A)\",\n    y = \"Attainment 8 Score (ATT8SCR)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis Maps\n\nMaps are great, they help you understand all kinds of things if your data have a spatial identifier. Try some maps out here!\nYou might find R is a bit better at mapping things. Just saying.\n\n\n\n\n\n\n\n\n\nCodeimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\nimport contextily as ctx\n\n# Reproject to Web Mercator for contextily\nsf_df_mercator = sf_df_bng.to_crs(epsg=3857)\n\n# Scale marker size\nmin_val = sf_df_mercator['TOTPUPS'].min()\nmax_val = sf_df_mercator['TOTPUPS'].max()\nsf_df_mercator['size_scale'] = 4 + (sf_df_mercator['TOTPUPS'] - min_val) * (12 - 4) / (max_val - min_val)\n\n# Color mapping\nnorm = Normalize(vmin=sf_df_mercator['ATT8SCR'].min(), vmax=sf_df_mercator['ATT8SCR'].max())\ncmap = plt.colormaps['magma']\nsf_df_mercator['color'] = sf_df_mercator['ATT8SCR'].apply(lambda x: cmap(norm(x)))\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 10))\nsf_df_mercator.plot(ax=ax, color=sf_df_mercator['color'], markersize=sf_df_mercator['size_scale']**2, alpha=0.8)\n\n&lt;Axes: &gt;\n\nCode# Add basemap\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n# Final touches\nax.set_title(\"Static Map of Schools Colored by ATT8SCR and Sized by TOTPUPS\", fontsize=14)\n\nText(0.5, 1.0, 'Static Map of Schools Colored by ATT8SCR and Sized by TOTPUPS')\n\nCodeax.set_axis_off()\n\n# Colorbar\nsm = cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\ncbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.03, pad=0.01)\ncbar.set_label('ATT8SCR', fontsize=12)\n\nplt.tight_layout()\nplt.savefig(\"static_school_map_with_basemap.png\", dpi=300)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCodelibrary(sf)\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(viridis)\n\n# Convert to sf object with EPSG:27700\nsf_df &lt;- st_as_sf(leeds_sub, coords = c(\"easting\", \"northing\"), crs = 27700)\n\n# Transform to EPSG:4326\nsf_df &lt;- st_transform(sf_df, crs = 4326)\n\n# Extract lat/lon for leaflet\nsf_df &lt;- sf_df %&gt;%\n  mutate(\n    lon = st_coordinates(.)[,1],\n    lat = st_coordinates(.)[,2]\n  )\n\n\n# Define size and color scales\nsize_scale &lt;- rescale(sf_df$TOTPUPS, to = c(1, 15))  # radius from 4 to 12\n\n# Create a color palette based on ATT8SCR\npal &lt;- colorNumeric(palette = \"magma\", domain = sf_df$ATT8SCR)\n\nleaflet(sf_df) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\") %&gt;%\n  addCircleMarkers(\n    lng = ~lon,\n    lat = ~lat,\n    radius = size_scale,\n    stroke = FALSE,\n    fillOpacity = 0.8,\n    color = ~pal(ATT8SCR),\n    popup = ~paste0(\n      \"&lt;strong&gt;\", SCHNAME.x, \"&lt;/strong&gt;&lt;br&gt;\",\n      \"Pupils: \", TOTPUPS\n    )\n  ) %&gt;%\n  addLegend(\n    \"bottomright\",\n    pal = pal,\n    values = ~ATT8SCR,\n    title = \"ATT8SCR\",\n    opacity = 0.8\n  )\n\n\n\n\n\n\n\n\nExploratory Data Analysis - Distributions and slicing and dicing\n\nYou can also explore the scatter plot for your whole of England dataset.\nOne really interesting thing to do is to slice and dice your data according to some of the categorical variables.\nIn the R visualisation, you can see how we can start to slide and dice by regions, for example. Maybe try some of your other categorical variables.\n\n\n\n\n\n\n\nNote\n\n\n\nYou will notice that in the plots below I have used a log10() transformation, but you could also use the natural log() or a range of other possible transformations. If you want to understand a bit more about transformations for normalisng your data, you should explore Tukey’s Ladder of Powers - https://onlinestatbook.com/2/transformations/tukey.html - in R (and probably in Python too) there are packages to apply appropriate Tukey transformations so your data gets a little closer to a normal distribution.\n\n\n\n\n\n\n\n\n\n\nCodeimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Load your data into a DataFrame named england_filtered\n# england_filtered = pd.read_csv(\"your_data.csv\")\n\n# Apply log10 transformation (replace 0s to avoid log(0))\nengland_filtered['log_PTFSM6CLA1A'] = np.log10(england_filtered['PTFSM6CLA1A'].replace(0, np.nan))\nengland_filtered['log_ATT8SCR'] = np.log10(england_filtered['ATT8SCR'].replace(0, np.nan))\n\n# Drop rows with NaNs from log(0)\nengland_filtered = england_filtered.dropna(subset=['log_PTFSM6CLA1A', 'log_ATT8SCR'])\n\n# Set theme\nsns.set_theme(style=\"darkgrid\")\n\n# Create jointplot\ng = sns.jointplot(\n    x=\"log_PTFSM6CLA1A\",\n    y=\"log_ATT8SCR\",\n    data=england_filtered,\n    kind=\"reg\",\n    truncate=False,\n    color=\"m\",\n    height=7\n)\n\n# Label axes\ng.set_axis_labels(\"log10(PTFSM6CLA1A)\", \"log10(ATT8SCR)\")\n\n&lt;seaborn.axisgrid.JointGrid object at 0x7f2342ad6320&gt;\n\nCodeplt.show()\n\n\n\n\n\n\n\n\n\n\nCodelibrary(ggside)\nlibrary(ggplot2)\n\nggplot(england_filtered, aes(x = log10(PTFSM6CLA1A), y = log10(ATT8SCR), colour = gor_name)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_xsidedensity(aes(y = after_stat(density)), alpha = 0.5, size = 1, position = \"stack\") +\n  geom_ysidedensity(aes(x = after_stat(density)), alpha = 0.5, size = 1, position = \"stack\") + \n  theme(ggside.panel.scale.x = 0.4,ggside.panel.scale.y = 0.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodeimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the theme\nsns.set_theme(style=\"darkgrid\")\n\n# Create side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Histogram for PTFSM6CLA1A\nsns.histplot(data=england_filtered, x=\"PTFSM6CLA1A\", ax=axes[0], kde=True, color=\"skyblue\")\n\n&lt;Axes: xlabel='PTFSM6CLA1A', ylabel='Count'&gt;\n\nCodeaxes[0].set_title(\"Histogram of PTFSM6CLA1A\")\n\nText(0.5, 1.0, 'Histogram of PTFSM6CLA1A')\n\nCodeaxes[0].set_xlabel(\"PTFSM6CLA1A\")\n\nText(0.5, 0, 'PTFSM6CLA1A')\n\nCodeaxes[0].set_ylabel(\"Frequency\")\n\nText(0, 0.5, 'Frequency')\n\nCode# Histogram for ATT8SCR\nsns.histplot(data=england_filtered, x=\"ATT8SCR\", ax=axes[1], kde=True, color=\"salmon\")\n\n&lt;Axes: xlabel='ATT8SCR', ylabel='Count'&gt;\n\nCodeaxes[1].set_title(\"Histogram of ATT8SCR\")\n\nText(0.5, 1.0, 'Histogram of ATT8SCR')\n\nCodeaxes[1].set_xlabel(\"ATT8SCR\")\n\nText(0.5, 0, 'ATT8SCR')\n\nCodeaxes[1].set_ylabel(\"Frequency\")\n\nText(0, 0.5, 'Frequency')\n\nCodeplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\n\n# Histogram for PTFSM6CLA1A\nggplot(england_filtered, aes(x = PTFSM6CLA1A)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"white\", alpha = 0.5) +\n  geom_density(aes(y = ..count..), color = \"skyblue\", size = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of PTFSM6CLA1A\",\n    x = \"PTFSM6CLA1A\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\nCode# Histogram for ATT8SCR\nggplot(england_filtered, aes(x = ATT8SCR)) +\n  geom_histogram(binwidth = 1, fill = \"salmon\", color = \"white\", alpha = 0.5) +\n  geom_density(aes(y = ..count..), color = \"salmon\", size = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of ATT8SCR\",\n    x = \"ATT8SCR\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipExplore Different Visualisations\n\n\n\nI’ve just given you a few very basic visualisation types here, but there are so many extensions and options available in packages like ggplot2 in R and Seaborn in Python so you can get visualising in very creative ways - have a look at some of these gallery examples in the links and maybe try experimenting with a few different ones! There are no excuses these days for 💩 data visualisations!\n\n\n\nCodeggplot(england_filtered, aes(x = PTFSM6CLA1A, y = ATT8SCR, fill = gor_name)) +\n  geom_violin(\n    position = position_dodge(width = 0.8),\n    alpha = 0.6,\n    draw_quantiles = c(0.25, 0.5, 0.75)\n  ) +\n  facet_wrap(~ gor_name) +\n  theme_minimal() +\n  labs(\n    title = \"Violin Plot of Attainment 8 Score with Quantiles by Region\",\n    x = \"% Disadvantaged\",\n    y = \"Attainment 8 Score\",\n    fill = \"Region\"\n  )\n\n\n\n\n\n\n\n\nCodelibrary(ggdist)\n\nggplot(england_filtered, aes(x = PTFSM6CLA1A, y = gor_name, fill = gor_name)) + \n  stat_slab(aes(thickness = after_stat(pdf*n)), scale = 0.7) +\n  stat_dotsinterval(side = \"bottom\", scale = 1, slab_linewidth = NA) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_ggdist() +\n  labs(\n    title = \"Rain Cloud Plot of % Disadvantage in Schools by Region\",\n    x = \"% Disadvantaged\",\n    y = \"Region\",\n    fill = \"Region\"\n  )"
  },
  {
    "objectID": "sessions/week6_practical.html#task-4---explanatory-analysis---running-your-regression-model",
    "href": "sessions/week6_practical.html#task-4---explanatory-analysis---running-your-regression-model",
    "title": "Prof D’s Regression Sessions - Vol 1",
    "section": "Task 4 - Explanatory Analysis - Running Your Regression Model",
    "text": "Task 4 - Explanatory Analysis - Running Your Regression Model\n\nNow you have carried out some appropriate exploratory data analysis, you should run a simple bi-variate regression model for your two local authority areas and compare it with a similar analysis run on the national dataset\n\n\n\n\n\n\n\nTip\n\n\n\n\nDepending on your variables and whether or not there appears to be a linear relationship or a log-log or level-log relationship, if you transform a variable for the national data or one of your subsets, you could also transform it for the others, otherwise it will be impossible to compare\n\n\n\n\nThe Code below is an example using my variables, but you should use the variables you have selected.\n\nRunning the Model for the first (London) local authority\n\n\n\n\n\n\n\n\nCodeimport numpy as np\nimport statsmodels.api as sm\n\n# Log-transform the variables\ncamden_sub['log_ATT8SCR'] = np.log(camden_sub['ATT8SCR'])\ncamden_sub['log_PTFSM6CLA1A'] = np.log(camden_sub['PTFSM6CLA1A'])\n\n# Define independent and dependent variables\nX = sm.add_constant(camden_sub['log_PTFSM6CLA1A'])  # adds intercept\ny = camden_sub['log_ATT8SCR']\n\n# Fit the model\ncamden_model1 = sm.OLS(y, X).fit()\n#camden_summary = extract_model_summary(camden_model1, 'Camden Model')\n\n# Print summary\nprint(camden_model1.summary())\n\n/opt/hostedtoolcache/Python/3.10.18/x64/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:430: UserWarning: `kurtosistest` p-value may be inaccurate with fewer than 20 observations; only n=10 observations were given.\n  return hypotest_fun_in(*args, **kwds)\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.576\nModel:                            OLS   Adj. R-squared:                  0.523\nMethod:                 Least Squares   F-statistic:                     10.88\nDate:                Sat, 25 Oct 2025   Prob (F-statistic):             0.0109\nTime:                        15:20:30   Log-Likelihood:                 9.4809\nNo. Observations:                  10   AIC:                            -14.96\nDf Residuals:                       8   BIC:                            -14.36\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               6.1206      0.678      9.023      0.000       4.556       7.685\nlog_PTFSM6CLA1A    -0.5784      0.175     -3.298      0.011      -0.983      -0.174\n==============================================================================\nOmnibus:                        2.491   Durbin-Watson:                   2.528\nProb(Omnibus):                  0.288   Jarque-Bera (JB):                0.897\nSkew:                           0.002   Prob(JB):                        0.639\nKurtosis:                       1.533   Cond. No.                         84.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nCode# Fit linear model and get predicted values\ncamden_model1 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = camden_sub)\nsummary(camden_model1)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A), data = camden_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.12111 -0.10124  0.01187  0.06663  0.13871 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        6.1206     0.6783   9.023 1.82e-05 ***\nlog(PTFSM6CLA1A)  -0.5784     0.1753  -3.298   0.0109 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1048 on 8 degrees of freedom\nMultiple R-squared:  0.5763,    Adjusted R-squared:  0.5233 \nF-statistic: 10.88 on 1 and 8 DF,  p-value: 0.01088\n\n\n\n\n\nRunning the Model for the second local authority\n\n\n\n\n\n\n\n\nCodeimport numpy as np\nimport statsmodels.api as sm\n\n\n# Log-transform safely: replace non-positive values with NaN\nleeds_sub['log_ATT8SCR'] = np.where(leeds_sub['ATT8SCR'] &gt; 0, np.log(leeds_sub['ATT8SCR']), np.nan)\nleeds_sub['log_PTFSM6CLA1A'] = np.where(leeds_sub['PTFSM6CLA1A'] &gt; 0, np.log(leeds_sub['PTFSM6CLA1A']), np.nan)\n\n# Drop rows with NaNs in either column\nleeds_clean = leeds_sub.dropna(subset=['log_ATT8SCR', 'log_PTFSM6CLA1A'])\n\n# Define independent and dependent variables\nX = sm.add_constant(leeds_clean['log_PTFSM6CLA1A'])  # adds intercept\ny = leeds_clean['log_ATT8SCR']\n\n# Fit the model\nleeds_model1 = sm.OLS(y, X).fit()\n#leeds_summary = extract_model_summary(leeds_model1, 'Leeds Model')\n\n# Print summary\nprint(leeds_model1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.491\nModel:                            OLS   Adj. R-squared:                  0.477\nMethod:                 Least Squares   F-statistic:                     35.71\nDate:                Sat, 25 Oct 2025   Prob (F-statistic):           6.77e-07\nTime:                        15:20:30   Log-Likelihood:                 26.542\nNo. Observations:                  39   AIC:                            -49.08\nDf Residuals:                      37   BIC:                            -45.76\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               4.5473      0.123     36.940      0.000       4.298       4.797\nlog_PTFSM6CLA1A    -0.2201      0.037     -5.976      0.000      -0.295      -0.145\n==============================================================================\nOmnibus:                        0.518   Durbin-Watson:                   2.333\nProb(Omnibus):                  0.772   Jarque-Bera (JB):                0.654\nSkew:                          -0.200   Prob(JB):                        0.721\nKurtosis:                       2.508   Cond. No.                         22.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nCode# Fit linear model and get predicted values\nleeds_model1 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = leeds_sub)\nsummary(leeds_model1)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A), data = leeds_sub)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.310438 -0.085062 -0.003004  0.116129  0.205687 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.54727    0.12310  36.940  &lt; 2e-16 ***\nlog(PTFSM6CLA1A) -0.22006    0.03682  -5.976 6.77e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1258 on 37 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4911,    Adjusted R-squared:  0.4774 \nF-statistic: 35.71 on 1 and 37 DF,  p-value: 6.767e-07\n\n\n\n\n\nRunning the Model for all Schools in England\n\n\n\n\n\n\n\n\nCodeimport numpy as np\nimport statsmodels.api as sm\n\n\n# Log-transform safely: replace non-positive values with NaN\nengland_filtered['log_ATT8SCR'] = np.where(england_filtered['ATT8SCR'] &gt; 0, np.log(england_filtered['ATT8SCR']), np.nan)\nengland_filtered['log_PTFSM6CLA1A'] = np.where(england_filtered['PTFSM6CLA1A'] &gt; 0, np.log(england_filtered['PTFSM6CLA1A']), np.nan)\n\n# Drop rows with NaNs in either column\nengland_clean = england_filtered.dropna(subset=['log_ATT8SCR', 'log_PTFSM6CLA1A'])\n\n# Define independent and dependent variables\nX = sm.add_constant(england_clean['log_PTFSM6CLA1A'])  # adds intercept\ny = england_clean['log_ATT8SCR']\n\n# Fit the model\nengland_model1 = sm.OLS(y, X).fit()\n#england_summary = extract_model_summary(england_model1, 'England Model')\n\n# Print summary\nprint(england_model1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.468\nModel:                            OLS   Adj. R-squared:                  0.468\nMethod:                 Least Squares   F-statistic:                     2611.\nDate:                Sat, 25 Oct 2025   Prob (F-statistic):               0.00\nTime:                        15:20:31   Log-Likelihood:                 1668.0\nNo. Observations:                2968   AIC:                            -3332.\nDf Residuals:                    2966   BIC:                            -3320.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               4.4778      0.013    349.542      0.000       4.453       4.503\nlog_PTFSM6CLA1A    -0.2071      0.004    -51.095      0.000      -0.215      -0.199\n==============================================================================\nOmnibus:                      105.763   Durbin-Watson:                   1.313\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              199.209\nSkew:                           0.267   Prob(JB):                     5.53e-44\nKurtosis:                       4.151   Cond. No.                         17.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nCode# Fit linear model and get predicted values\nengland_filtered_clean &lt;- england_filtered[\n  !is.na(england_filtered$ATT8SCR) & \n  !is.na(england_filtered$PTFSM6CLA1A) &\n  england_filtered$ATT8SCR &gt; 0 &\n  england_filtered$PTFSM6CLA1A &gt; 0, \n]\n\nengland_model1 &lt;- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered_clean)\nsummary(england_model1)\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65843 -0.08607 -0.00897  0.07800  0.55670 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.477823   0.012811  349.54   &lt;2e-16 ***\nlog(PTFSM6CLA1A) -0.207090   0.004053  -51.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.138 on 2966 degrees of freedom\nMultiple R-squared:  0.4681,    Adjusted R-squared:  0.468 \nF-statistic:  2611 on 1 and 2966 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCombining your outputs for comparison\nDon’t forget to run some basic regression modelling checks on all of your models:\n\n\n\n\n\n\nNote\n\n\n\nIf you are doing this in Python, it appears there isn’t a nice little package with the code for diagnostic plots already created. However, the statsmodels pages do have some code we can borrow - https://www.statsmodels.org/dev/examples/notebooks/generated/linear_regression_diagnostics_plots.html - run this first before using the functions below\n\n\nLinearity\n\n\n\n\n\n\n\n\nCodecls = LinearRegDiagnostic(england_model1)\n\ncls.residual_plot();\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCodelibrary(performance)\n\ncheck_model(england_model1, \n            check = c(\"linearity\"))\n\n\n\n\n\n\n\n\n\n\nHomoscedasticity (Constant variance)?\n\n\n\n\n\n\n\n\nCodecls.scale_location_plot();\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCodecheck_model(england_model1, \n            check = c(\"homogeneity\"))\n\n\n\n\n\n\n\n\n\n\nNormality of Residuals\n\n\n\n\n\n\n\n\nCodecls.qq_plot()\n\n&lt;Axes: title={'center': 'Normal Q-Q'}, xlabel='Theoretical Quantiles', ylabel='Standardized Residuals'&gt;\n\nCodeplt.show()\n\n\n\n\n\n\n\n\n\n\nCodecheck_model(england_model1, \n            check = c(\"qq\"))\n\n\n\n\n\n\n\n\n\n\nInfluence of Outliers\n\n\n\n\n\n\n\n\nCodecls.leverage_plot(high_leverage_threshold=True, cooks_threshold=\"dof\")\n\n&lt;Axes: title={'center': 'Residuals vs Leverage'}, xlabel='Leverage', ylabel='Standardized Residuals'&gt;\n\nCodeplt.show()\n\n\n\n\n\n\n\n\n\n\nCodecheck_model(england_model1, \n            check = c(\"outliers\"))\n\n\n\n\n\n\n\n\n\n\nCombining your outputs for comparison\n\n\n\n\n\n\nNote\n\n\n\nYou might notice that R is a bit better at things like manipulating model outputs than Python!\n\n\n\n\n\n\n\n\n\n\nCodeimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col \n\nresults_table = summary_col(\n    results=[camden_model1, leeds_model1, england_model1],\n    model_names=['Camden Model', 'Leeds Model', 'England Model'],\n    stars=True,\n    float_format=\"%0.3f\",\n    # You can customize what model statistics show up here (like R2, N, F-stat)\n    info_dict={'N':lambda x: \"{0:d}\".format(int(x.nobs))}\n)\n\n# \n# # Round for readability\nprint(results_table)\n\n\n======================================================\n                Camden Model Leeds Model England Model\n------------------------------------------------------\nconst           6.121***     4.547***    4.478***     \n                (0.678)      (0.123)     (0.013)      \nlog_PTFSM6CLA1A -0.578**     -0.220***   -0.207***    \n                (0.175)      (0.037)     (0.004)      \nR-squared       0.576        0.491       0.468        \nR-squared Adj.  0.523        0.477       0.468        \nN               10           39          2968         \n======================================================\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01\n\n\n\n\n\nCodelibrary(jtools)\nexport_summs(camden_model1, leeds_model1, england_model1, error_format = \"\", error_pos = \"same\", model.names = c(\"Camden Model\", \"Leeds Model\", \"England Model\"))\n\n\n\n\n\n\n\n\nCamden Model\nLeeds Model\nEngland Model\n\n\n\n(Intercept)\n6.12 *** \n4.55 *** \n4.48 *** \n\n\nlog(PTFSM6CLA1A)\n-0.58 *   \n-0.22 *** \n-0.21 *** \n\n\nN\n10        \n39        \n2968        \n\n\nR2\n0.58     \n0.49     \n0.47     \n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipQuestions to Answer\n\n\n\nComparing your three regression models:\n1. How do the intercept (baseline) attainment values differ between your three models? remember, if you (natural)logged your variables, you will need to take the exponential of the coefficient to interpret it back on the original scale.\n2. What do the differences in the slope values tell you?\n3. Are your intercept and slope values statistically significant? If so, at what level?\n4. How do the R-squared values differ between your models?\n5. What can you say about your degrees of freedom and the reliability of your R-squared and other coefficients between your models?\n6. What is the overall story of the relationship between your attainment variable and your explanatory variable considering both your local examples and comparison with schools across the rest of England?\n\n\nWhat does my model show?\n\nBaseline attainment (intercept) is much higher in Camden than in Leeds or the whole of England. Exp(6.12) = 455.32. So the Attainment 8 Score in Camden would be 455 (an impossible score) when % of disadvantaged children in a school are 0. This compares to exp(4.547) = 94.35 in Leeds and exp(4.478) = 88.06 in England as a whole. All of these intercepts are statistically significant (i.e. observed values not a chance relationship). While unrealistic (potentially due to the low degrees of freedom), the Camden intercept shows us that after controlling for disadvantage, baseline attainment in Camden is better than in Leeds or the rest of England.\n\nThe slope value of -0.207 for England shows that across the country. As this is a log-log model, it is an Elasticity: A 1% increase in % of disadvantaged students in a school is associated with a 0.21% decrease in attainment. HOWEVER, as this is a log-log model, the effect is far stronger at one end of the distribution than the other.\n\nWhat this means is that a change from 1% to 10% of disadvantaged students in a school there is a 37.9% decrease in attainment, however, change from 21% to 30% leads to just a 7.1% decrease in attainment.\n\n\n\n\n\n\n\n\n\nTipAI can help here!\n\n\n\nOne of the things that large language models are quite good at is interpreting regression coefficients in the context of real data.\nThey don’t always get it right, so USE WITH EXTREME CAUTION AND ALWAYS DOUBLE CHECK WITH YOUR OWN UNDERSTANDING, however, they can help explain what YOUR model coefficients mean in relation to YOUR data in an incredibly helpful way. If you get stuck, try feeding an LLM (ChatGTP, Gemini etc.) with your regression outputs and a description of your data and see what it can tell you\n\n\nThis effect is similar for Leeds, but apparently far more severe in Camden where the slope is much steeper. However, Camden doesn’t experience very low levels of disadvantage in its schools (unlike in Leeds and the rest of the county), with the lowest level around 35%. With low numbers of schools in the Borough, this observation is potentially unreliable.\n\nThe slopes and intercept coefficients in all models are all apparently statistically significant - but as we have seen with Camden, this does not necessarily mean our observations are generalisable! This is a good lesson in the difference between statistical significance (p-values are not a panacea!) and why you should always interrogate your model fully when interpreting outputs.\nAgain, R-squared values might fool you into thinking the Camden model is ‘better’ than the other models, however, the R-squared is partially an artefact of the low degrees of freedom in the model.\nSee above.\nOverall, we can make some crucial observations that encourage further research. While the model for England suggests a moderately strong log-log relationship between levels of disadvantage in a school and attainment, looking at this relationship for a London Borough (Camden) - and another Local Authority in England (Leeds) it’s the case that this is unlikely to be a universal relationship.\n\nThe baseline of attainment in Camden, controlling for disadvantage, is much higher than in the rest of the country. Students get higher levels of attainment, even when controlling for disadvantage. The log log relationship is crucial as it shows that on average, at low levels of disadvantage small increases have a more severe impact on attainment level, whereas at higher levels, similar changes have much reduced effects.\nYour Turn\nTo finish off the practical, see if you can answer the questions above for your example."
  },
  {
    "objectID": "sessions/week7_lecture.html#this-weeks-session-aims",
    "href": "sessions/week7_lecture.html#this-weeks-session-aims",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "This week’s Session Aims",
    "text": "This week’s Session Aims\n\nBuild on the basics you learned last week and learn how to extend regression modelling into multiple dimensions\nUnderstand the main benefits, risks and steps to take when gradually building the complexity of your models\nUnderstand who to interpret the outputs correctly\nGain an appreciation of how regression help explain the phenomena we are observing\nHave hands-on practice at building and interpreting your own complex models"
  },
  {
    "objectID": "sessions/week7_lecture.html#recap---last-weeks-model",
    "href": "sessions/week7_lecture.html#recap---last-weeks-model",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Recap - Last Week’s Model",
    "text": "Recap - Last Week’s Model\n\n\n\n\n\n\n\n\n\n\nWork of Gorard suggested link between levels of disadvantage in a school and attainment.\nNot a linear relationship but a log-log elasticity"
  },
  {
    "objectID": "sessions/week7_lecture.html#recap---last-weeks-model-1",
    "href": "sessions/week7_lecture.html#recap---last-weeks-model-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Recap - Last Week’s Model",
    "text": "Recap - Last Week’s Model\n\n\n\n\n\n\n\n\n\n\nSchool-level relationship between attainment and disadvantage unreliable at the local authority level with small changes influencing coefficients where degrees of freedom are low"
  },
  {
    "objectID": "sessions/week7_lecture.html#an-alternative-model",
    "href": "sessions/week7_lecture.html#an-alternative-model",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "An Alternative Model",
    "text": "An Alternative Model\n\n\n\n\n\n\n\n\n\n\nOther research suggests overall levels of absence might be even more important\nHow can we tell for sure?"
  },
  {
    "objectID": "sessions/week7_lecture.html#an-alternative-model-1",
    "href": "sessions/week7_lecture.html#an-alternative-model-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "An Alternative Model",
    "text": "An Alternative Model\n\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PERCTOT), data = england_filtered)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.16241 -0.07336  0.00471  0.07247  1.03789 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.00521    0.01712  292.40   &lt;2e-16 ***\nlog(PERCTOT) -0.53898    0.00778  -69.27   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1223 on 3231 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.5976,    Adjusted R-squared:  0.5975 \nF-statistic:  4799 on 1 and 3231 DF,  p-value: &lt; 2.2e-16\n\n\n\nOverall Absence = bigger coefficient (-0.54 vs -0.21) & improves the \\(R^2\\) to (60% vs 45% for the % disadvantage model)\nSuggests that Overall Absence explains more variation in Attainment 8"
  },
  {
    "objectID": "sessions/week7_lecture.html#multiple-linear-regression",
    "href": "sessions/week7_lecture.html#multiple-linear-regression",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nOne way to really tell which variable is more important is to put them in a model together and observe how the affect each other\nMultiple linear regression extends bivariate regression to include multiple independent variables\nAs with bivariate regression, variables should be chosen based on theory and prior research rather than just throwing the variables in because you have them\nHowever, it’s permissible to explore relationships and experiment and iterate between data exploration and theory - both can inform each other"
  },
  {
    "objectID": "sessions/week7_lecture.html#multiple-linear-regression-1",
    "href": "sessions/week7_lecture.html#multiple-linear-regression-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nWhen we start adding more variables into the model, an alternative to the mode generic notation is to include the variables in the equation explicitly, for example:\n\n\\[\\log(\\text{Attainment8}) = \\beta_0 + \\beta_1\\log(\\text{PctDisadvantage}) +\\\\\\\\ \\beta_2log(\\text{PctPersistentAbsence}) + \\epsilon\\]\n\nThere’s no real limit to the number of variables you can include in a model - however:\n\nFewer is easier to interpret\nMore variables will reduce your degrees of freedom"
  },
  {
    "objectID": "sessions/week7_lecture.html#multiple-linear-regression-2",
    "href": "sessions/week7_lecture.html#multiple-linear-regression-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression"
  },
  {
    "objectID": "sessions/week7_lecture.html#multiple-linear-regression-3",
    "href": "sessions/week7_lecture.html#multiple-linear-regression-3",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT), \n    data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.26481 -0.06531 -0.00040  0.06352  0.77403 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       5.063088   0.015126  334.72   &lt;2e-16 ***\nlog(PTFSM6CLA1A) -0.111473   0.003578  -31.16   &lt;2e-16 ***\nlog(PERCTOT)     -0.406199   0.008045  -50.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1072 on 3230 degrees of freedom\nMultiple R-squared:  0.6906,    Adjusted R-squared:  0.6904 \nF-statistic:  3605 on 2 and 3230 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat is the model telling us?"
  },
  {
    "objectID": "sessions/week7_lecture.html#multiple-linear-regression-4",
    "href": "sessions/week7_lecture.html#multiple-linear-regression-4",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n19500.8\n19500.8\n19525.1\n0.69\n0.69\n0.11\n0.11"
  },
  {
    "objectID": "sessions/week7_lecture.html#multiple-linear-regression-5",
    "href": "sessions/week7_lecture.html#multiple-linear-regression-5",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nThe coefficients for both variables are statistically significant and negative, indicating that both variables contribute to explaining the variation in Attainment 8 scores\nBut t-values indicate that the % Overall Absence variable (-50.49) has a stronger effect on Attainment 8 scores than the % Disadvantaged Students variable (-31.16)\nThe \\(R^2\\) value is now 0.69, indicating that the model is potentially good and explains 69% of the variation in Attainment 8 scores. Degrees of freedom are good and the overall model is statistically significant.\nBut does is satisfy the assumptions of linear regression?"
  },
  {
    "objectID": "sessions/week7_lecture.html#diagnostics-1",
    "href": "sessions/week7_lecture.html#diagnostics-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Diagnostics 1",
    "text": "Diagnostics 1\n\n\n\n\n\n\n\n\n\n\nPretty good - the residuals are randomly scattered around zero, suggesting a linear relationship"
  },
  {
    "objectID": "sessions/week7_lecture.html#diagnostics-2",
    "href": "sessions/week7_lecture.html#diagnostics-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Diagnostics 2",
    "text": "Diagnostics 2\n\n\n\n\n\n\n\n\n\n\nThe residuals are normally distributed - the Q-Q plot shows most points on the line"
  },
  {
    "objectID": "sessions/week7_lecture.html#diagnostics-3",
    "href": "sessions/week7_lecture.html#diagnostics-3",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Diagnostics 3",
    "text": "Diagnostics 3\n\n\n\n\n\n\n\n\n\n\nThe residuals are randomly scattered around zero, suggesting constant variance."
  },
  {
    "objectID": "sessions/week7_lecture.html#linear-regression---diagnostics",
    "href": "sessions/week7_lecture.html#linear-regression---diagnostics",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Linear Regression - Diagnostics",
    "text": "Linear Regression - Diagnostics\n\nLinearity: 😎\nHomoscedasticity: 😎\nNormality of residuals: 😎\nNo multicollinearity: 😕\nIndependence of residuals: 😕"
  },
  {
    "objectID": "sessions/week7_lecture.html#multicollinearity",
    "href": "sessions/week7_lecture.html#multicollinearity",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity occurs when two or more independent variables in a regression model are highly correlated with each other\nThis can lead to unreliable estimates of the coefficients, inflated standard errors, and difficulty in interpreting the results"
  },
  {
    "objectID": "sessions/week7_lecture.html#multicollinearity-1",
    "href": "sessions/week7_lecture.html#multicollinearity-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\n\n\n\n\n\n\n\n\n\nA quick and easy way to check for the correlation between your independent variables is generate a standard correlation matrix/plot\nDifficult to say what is too much correlation - but over 0.7 is often considered problematic\nHowever, pairwise correlations might miss n-way correlations"
  },
  {
    "objectID": "sessions/week7_lecture.html#variance-inflaction-factor---vif",
    "href": "sessions/week7_lecture.html#variance-inflaction-factor---vif",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Variance Inflaction Factor - VIF",
    "text": "Variance Inflaction Factor - VIF\n\n\n\n\n\n\n\n\n\n\nA more useful diagnostic tool is the Variance Inflation Factor (VIF), which measures how much the variance of a regression coefficient is increased due to multicollinearity\nA VIF value of 1 indicates no correlation, while a VIF value above 5 or 10 suggests high multicollinearity"
  },
  {
    "objectID": "sessions/week7_lecture.html#variance-inflaction-factor---vif-1",
    "href": "sessions/week7_lecture.html#variance-inflaction-factor---vif-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Variance Inflaction Factor - VIF",
    "text": "Variance Inflaction Factor - VIF\nWhy does it even matter if the variance is inflated?\n\nVariance = Uncertainty in our model\nIf 2 or more variables are highly correlated and both appear to affect the dependent variable, it can be difficult to determine which variable is actually having the effect\nThe model makes an arbitrary split\nSmall changes in the data could make the split go either way - e.g. attribute more of the effect to % Disadvantaged Students or % Overall Absence (if VIF high) - thus inflating the uncertainty / variance"
  },
  {
    "objectID": "sessions/week7_lecture.html#variance-inflaction-factor---vif-2",
    "href": "sessions/week7_lecture.html#variance-inflaction-factor---vif-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Variance Inflaction Factor - VIF",
    "text": "Variance Inflaction Factor - VIF\n\nDespite some positive correlation (0.48) between % Disadvantaged Students and % Overall Absence, the VIF is very low ~1.5\nNo problems with multicollinearity\nSo we have passed that test and can happily use both in the model"
  },
  {
    "objectID": "sessions/week7_lecture.html#linear-regression---diagnostics-1",
    "href": "sessions/week7_lecture.html#linear-regression---diagnostics-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Linear Regression - Diagnostics",
    "text": "Linear Regression - Diagnostics\n\nLinearity: 😎\nHomoscedasticity: 😎\nNormality of residuals: :😎:\nNo multicollinearity: 😎\nIndependence of residuals: 😕"
  },
  {
    "objectID": "sessions/week7_lecture.html#independence-of-residuals",
    "href": "sessions/week7_lecture.html#independence-of-residuals",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Independence of Residuals",
    "text": "Independence of Residuals\n\nResiduals (errors) should not be correlated with each other\n(auto)correlation (clustering) in the errors = model missing something!\ncan lead to biased estimates and incorrect conclusions\noften occurs when a temporal or spatial component to the data\nor when another important variable is missing or omitted"
  },
  {
    "objectID": "sessions/week7_lecture.html#spatial-or-temporal-autocorrelation",
    "href": "sessions/week7_lecture.html#spatial-or-temporal-autocorrelation",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Spatial or Temporal (auto)Correlation",
    "text": "Spatial or Temporal (auto)Correlation\n\nIn our case, we have a spatial component to the data - schools are located in different areas of England and Wales\nIf schools in the same area have similar characteristics, it is likely that the residuals will be correlated\nThe easiest way to check this is to plot the values of the residuals for the schools on a map"
  },
  {
    "objectID": "sessions/week7_lecture.html#residual-autocorrelation",
    "href": "sessions/week7_lecture.html#residual-autocorrelation",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Residual (auto)Correlation",
    "text": "Residual (auto)Correlation\n\n\n\n\n\n\n\nResiduals show clear spatial autocorrelation\nLondon (and other city schools) perform much better than model predicts"
  },
  {
    "objectID": "sessions/week7_lecture.html#residual-autocorrelation-1",
    "href": "sessions/week7_lecture.html#residual-autocorrelation-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Residual (auto)Correlation",
    "text": "Residual (auto)Correlation\n\nALWAYS MAP YOUR RESIDUALS IF MODELLING SPATIAL DATA\nCorrelated residuals often the sign of an omitted variable (e.g “London”)\nIn the GIS Course you will learn a lot more about testing for spatial autocorrelation (using Moran’s I) and how spatial variables (lags, error terms) or geographically weighted regression can be used to deal with spatial autocorrelation\nHere we will try something much simpler - adding London (and other regions) to our model"
  },
  {
    "objectID": "sessions/week7_lecture.html#linear-regression---diagnostics-2",
    "href": "sessions/week7_lecture.html#linear-regression---diagnostics-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Linear Regression - Diagnostics",
    "text": "Linear Regression - Diagnostics\n\nLinearity: 😎\nHomoscedasticity: 😎\nNormality of residuals: :😎:\nNo multicollinearity: 😎\nIndependence of residuals: 😬"
  },
  {
    "objectID": "sessions/week7_lecture.html#dummy-variables",
    "href": "sessions/week7_lecture.html#dummy-variables",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Dummy Variables",
    "text": "Dummy Variables"
  },
  {
    "objectID": "sessions/week7_lecture.html#dummy-variables-1",
    "href": "sessions/week7_lecture.html#dummy-variables-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\nIn linear regression, independent variables can also be categorical\nCategorical variables are often referred to as DUMMY variables\nDummy because the are a numerical stand-in (1 or 0) for a qualitative concept\nIn our model Region could be a dummy variable to see the “London” effect is real\n\nDummy could also be any other categorical variable like Ofsted rating (e.g. Good, Outstanding etc.)"
  },
  {
    "objectID": "sessions/week7_lecture.html#dummy-variables-2",
    "href": "sessions/week7_lecture.html#dummy-variables-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + \n    gor_name, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24928 -0.05827  0.00088  0.06062  0.73259 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       5.010071   0.015920 314.710  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)                 -0.132708   0.003801 -34.910  &lt; 2e-16 ***\nlog(PERCTOT)                     -0.366326   0.008373 -43.752  &lt; 2e-16 ***\ngor_nameEast of England           0.006536   0.008173   0.800 0.423949    \ngor_nameLondon                    0.100077   0.007890  12.683  &lt; 2e-16 ***\ngor_nameNorth East                0.079556   0.010500   7.577 4.61e-14 ***\ngor_nameNorth West                0.008348   0.007786   1.072 0.283680    \ngor_nameSouth East                0.004422   0.007724   0.573 0.566995    \ngor_nameSouth West                0.031877   0.008486   3.757 0.000175 ***\ngor_nameWest Midlands             0.033315   0.008067   4.130 3.72e-05 ***\ngor_nameYorkshire and the Humber  0.040358   0.008426   4.789 1.75e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1025 on 3222 degrees of freedom\nMultiple R-squared:  0.7182,    Adjusted R-squared:  0.7173 \nF-statistic:   821 on 10 and 3222 DF,  p-value: &lt; 2.2e-16\n\n\n\nThere are 9 regions in England and here “London” has been set as the contrast or reference variable (which all others are compared to)\nChanging the contrast compares each region to a different reference"
  },
  {
    "objectID": "sessions/week7_lecture.html#dummy-variables-3",
    "href": "sessions/week7_lecture.html#dummy-variables-3",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\n\n\n\n\n\n\n\n\n\nGovernment Office Region (GOR) dummy - (London as ref)"
  },
  {
    "objectID": "sessions/week7_lecture.html#dummy-variables-4",
    "href": "sessions/week7_lecture.html#dummy-variables-4",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + \n    gor_name, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24928 -0.05827  0.00088  0.06062  0.73259 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       5.014493   0.015398 325.660  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)                 -0.132708   0.003801 -34.910  &lt; 2e-16 ***\nlog(PERCTOT)                     -0.366326   0.008373 -43.752  &lt; 2e-16 ***\ngor_nameLondon                    0.095655   0.007131  13.413  &lt; 2e-16 ***\ngor_nameEast Midlands            -0.004422   0.007724  -0.573 0.566995    \ngor_nameEast of England           0.002114   0.007119   0.297 0.766538    \ngor_nameNorth East                0.075134   0.009836   7.638 2.88e-14 ***\ngor_nameNorth West                0.003926   0.006840   0.574 0.566020    \ngor_nameSouth West                0.027455   0.007428   3.696 0.000223 ***\ngor_nameWest Midlands             0.028893   0.007181   4.023 5.87e-05 ***\ngor_nameYorkshire and the Humber  0.035935   0.007533   4.770 1.92e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1025 on 3222 degrees of freedom\nMultiple R-squared:  0.7182,    Adjusted R-squared:  0.7173 \nF-statistic:   821 on 10 and 3222 DF,  p-value: &lt; 2.2e-16\n\n\n\nSouth East as contrast - London’s log(ATT8SCR) is 0.09 higher\nThis equals (exp(0.095655) - 1) * 100 = 10% - London’s average Attainment 8 is 10% higher than the South East"
  },
  {
    "objectID": "sessions/week7_lecture.html#dummy-variables-5",
    "href": "sessions/week7_lecture.html#dummy-variables-5",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\n\n\n\n\n\n\nHard to tell definitively from visual map, but residuals have changed and less clustering (Moran’s I would be more definitive - see GIS course)"
  },
  {
    "objectID": "sessions/week7_lecture.html#dummy-variables-6",
    "href": "sessions/week7_lecture.html#dummy-variables-6",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\nThere is no correct contrast to choose for your dummy variable, you may need to try different ones to see which makes most intuitive sense\nDepending on the contrast, some dummy regions may or may not be statistically significant in comparison\nThe inclusion of a regional dummy in this model has improved the \\(R^2\\) to 72%\nHowever, you may have noticed some of the coefficients have changed and this brings us to confounding and mediation"
  },
  {
    "objectID": "sessions/week7_lecture.html#variable-reclassification",
    "href": "sessions/week7_lecture.html#variable-reclassification",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Variable Reclassification",
    "text": "Variable Reclassification\nA Quick Aside: - One potentially useful strategy in regression modelling is the reclassification of an independent variable. - Continuous -&gt; Categorical or Weak Ordinal (e.g. various measurements of height into short, average and tall at different thresholds) - Categorical -&gt; Categorical with fewer categories (e.g. short, average, tall into below average and above average)"
  },
  {
    "objectID": "sessions/week7_lecture.html#variable-reclassification-1",
    "href": "sessions/week7_lecture.html#variable-reclassification-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Variable Reclassification",
    "text": "Variable Reclassification\n\nBenefits of reclassification:\n\nMoving from a continuous variable to a weak ordinal variable might make the patterns or observations easier to explain\ncomplex relationships (such as log-linear elasticities associated with absence from school) reduced to something more interpretable (if you miss more than 50% of school, your odds of passing your exams are drastically reduced)\nPrevious obscured patterns can emerge from the data\nCollapsing categorical variables (e.g. multiple religious school denominations into religious/non-religious) can aid interpretability and boost statistical power, increase degrees of freedom"
  },
  {
    "objectID": "sessions/week7_lecture.html#variable-reclassification-2",
    "href": "sessions/week7_lecture.html#variable-reclassification-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Variable Reclassification",
    "text": "Variable Reclassification\n\nRisks of reclassification:\n\nLoss of statistical power due to loss of information\nNot always obvious where to make the cut when reclassifying a continuous variable and this can be crucial to the interpretation (e.g. is Low &lt;20 or &lt;30?)"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding",
    "href": "sessions/week7_lecture.html#confounding",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\n\nConfound: “to fail to discern differences between : mix up”"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding-1",
    "href": "sessions/week7_lecture.html#confounding-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding-2",
    "href": "sessions/week7_lecture.html#confounding-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding-3",
    "href": "sessions/week7_lecture.html#confounding-3",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding\n\nConfounding is the change in the effect on the dependent variable of the independent variables in the presence of each other\nCan occur when we know some independent variables might effect each other (such as disadvantage on absence) even when their joint presence doesn’t cause variance issues\nLooking for confounding effects is a crucial part of model building - how do the coefficients of your model change when you introduce other variables?"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding-4",
    "href": "sessions/week7_lecture.html#confounding-4",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\n\nModel 1Model 2Model 3\n\n\n\n(Intercept)4.48 *** 5.01 *** 5.06 *** \n\nlog(PTFSM6CLA1A)-0.21 ***          -0.11 *** \n\nlog(PERCTOT)         -0.54 *** -0.41 *** \n\nN3248        3233        3233        \n\nR20.45     0.60     0.69     \n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\nIn the presence of Overall Absence, the effect of disadvantage is confounded (and vice versa)\nBoth variables together, however, explain much more variation in Attainment 8 - both part of the story"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding-5",
    "href": "sessions/week7_lecture.html#confounding-5",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding-6",
    "href": "sessions/week7_lecture.html#confounding-6",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding\n\nThe coefficient for log(PTFSM6CLA1A) - disadvantage - shrinks dramatically, from -0.21 down to -0.11 (a 44.5% reduction in effect)\nWhile the coefficient for log(PERCTOT) - Overall Absence - also shrinks from -0.54 to -0.41 this is just a 24.6% reduction\nThe confounding effect of Overall Absence on disadvantage is almost double the confounding effect of disadvantage on Overall Absence\nThis means that the relationship between disadvantage and Attainment 8 is much more biased by the omission of overall absence from the model than the other way around"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding-7",
    "href": "sessions/week7_lecture.html#confounding-7",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding\n\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT) + \n    gor_name, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24928 -0.05827  0.00088  0.06062  0.73259 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       5.014493   0.015398 325.660  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)                 -0.132708   0.003801 -34.910  &lt; 2e-16 ***\nlog(PERCTOT)                     -0.366326   0.008373 -43.752  &lt; 2e-16 ***\ngor_nameLondon                    0.095655   0.007131  13.413  &lt; 2e-16 ***\ngor_nameEast Midlands            -0.004422   0.007724  -0.573 0.566995    \ngor_nameEast of England           0.002114   0.007119   0.297 0.766538    \ngor_nameNorth East                0.075134   0.009836   7.638 2.88e-14 ***\ngor_nameNorth West                0.003926   0.006840   0.574 0.566020    \ngor_nameSouth West                0.027455   0.007428   3.696 0.000223 ***\ngor_nameWest Midlands             0.028893   0.007181   4.023 5.87e-05 ***\ngor_nameYorkshire and the Humber  0.035935   0.007533   4.770 1.92e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1025 on 3222 degrees of freedom\nMultiple R-squared:  0.7182,    Adjusted R-squared:  0.7173 \nF-statistic:   821 on 10 and 3222 DF,  p-value: &lt; 2.2e-16\n\n\n\nIncluding a regional dummy slightly reduces the effect of Overall Absence and slightly increases the effect of disadvantage, relative to model 3"
  },
  {
    "objectID": "sessions/week7_lecture.html#confounding-8",
    "href": "sessions/week7_lecture.html#confounding-8",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Confounding",
    "text": "Confounding\nInterpretation:\n\nPart of the negative effect of disadvantage was being suppressed or masked by the regional differences (probably London where Attainment 8 is higher on average and so are levels of disadvantage)\nPart of the negative effect of Overall Absence is due to its correlation with regional factors - but which ones?\nWe can explore this question through one final twist in our modelling recipe - we can look at the interaction effects between individual regions and disadvantage / Overall Absence"
  },
  {
    "objectID": "sessions/week7_lecture.html#mediating",
    "href": "sessions/week7_lecture.html#mediating",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Mediating",
    "text": "Mediating\n\n\n\nA mediating variable lies on the causal pathway between the primary independent variable and the the outcome\nGabber Music (\\(X\\)) \\(\\rightarrow\\) Hakken Dancing (\\(Y\\))\nGabber Music (\\(X\\)) \\(\\rightarrow\\) Consumption of Mind-Altering Drugs (\\(M\\)) \\(\\rightarrow\\) Hakken Dancing (\\(Y\\))\n\nhttps://en.wikipedia.org/wiki/Hakken"
  },
  {
    "objectID": "sessions/week7_lecture.html#mediating-1",
    "href": "sessions/week7_lecture.html#mediating-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Mediating",
    "text": "Mediating\n\nStep 1: Gabber Music (\\(X\\)) \\(\\rightarrow\\) Consumption of Mind-Altering Drugs (\\(M\\)) - Gabber music is so awful that being in a field where it is playing for too long forces people to consume vast amounts of mind altering drugs to cope\nStep 2: Consumption of Mind-Altering Drugs (\\(M\\)) \\(\\rightarrow\\) Hakken Dancing (\\(Y\\)) - Mind-altering drugs induce involuntary limb movements and repetitive motions in time with the relentless gabber beat\nStep 3: Gabber Music (\\(X\\)) \\(\\rightarrow\\) Hakken Dancing (\\(Y\\)) - The effect of gabber music on hakken dancing is significantly reduced once we control for the presence of mind altering drugs, although there is still some independent effect (some mad Dutch people who don’t take drugs just love dancing to gabber!).\nMind-altering drugs act as a partial mediator of the relationship between Gabber Music and Hakken Dancing"
  },
  {
    "objectID": "sessions/week7_lecture.html#mediating-2",
    "href": "sessions/week7_lecture.html#mediating-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Mediating",
    "text": "Mediating\nDisadvantage \\(\\rightarrow\\) Absence \\(\\rightarrow\\) Attainment\n\nStep 1: Disadvantage to Absence (X \\(\\rightarrow\\) M): Plausible that high rates of disadvantage lead to higher rates of unauthorised absence (due to factors like poor health, transport issues, lack of engagement, etc.).\nStep 2: Absence to Attainment (M \\(\\rightarrow\\) Y): Model consistently show a large, highly significant negative effect of Absence on Attainment\nStep 3: Disadvantage to Attainment (X \\(\\rightarrow\\) Y): The effect of Disadvantage is significantly reduced but still exists\nSome of the detrimental effect of poverty is channelled through the mechanism of higher unauthorised absence"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects",
    "href": "sessions/week7_lecture.html#interaction-effects",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\nPut simply, an interaction term allows the model to answer the question: “Does the effect of disadvantage/Overall Absence on a school’s Attainment 8 score change from region to region?”\nOperationally, running interactions in the model simply requires variables to be multiplied together rather than summed\nPractically, the combinations of variables need some careful attention (or the assistance of a helpful Artificial Intelligence helper) to interpret correctly!"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-1",
    "href": "sessions/week7_lecture.html#interaction-effects-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\nThe equation for a model where we interact Overall Absence with Regions would look like:\n\\[\\log(\\text{Attainment8}) = \\beta_0 + \\beta_1 \\log(\\text{PctPersistentAbsence}) +\\\\\\\\ \\sum_{j=1}^{k} \\gamma_j \\text{Region}_j + \\sum_{j=1}^{k} \\delta_j (\\log(\\text{PctPersistentAbsence}) \\times \\text{Region}_j) + \\epsilon\\]"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-2",
    "href": "sessions/week7_lecture.html#interaction-effects-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\nWhere (if we set the South East as the reference dummy)\n\n\\(\\beta_0\\) is the intercept (predicted \\(\\log(\\text{ATT8SCR})\\) for the South East when - \\(\\log(\\text{PERCTOT})=0\\)).\n\\(\\beta_1\\) is the main effect of unauthorised absence (the effect for the South East).\n\\(\\gamma_j\\) is the difference in overall attainment for region \\(j\\) compared to the South East.\n\\(\\delta_j\\) is the difference in the slope (effect of absence) for region \\(j\\) compared to the South East.\n\\(\\epsilon\\) is the error term."
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-3",
    "href": "sessions/week7_lecture.html#interaction-effects-3",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\n\n\n1% increase in the overall absence rate (log(PERCTOT)) predicts a 0.58% decrease in Attainment 8\nControlling for overall absence, all other regions worse attainment than SE (reference). SE uniquely sensitive to absence\nInteraction effect: in London, NE & Yorkshire strong positive coefficient = negative effect of overall absence on attainment is significantly less severe than in SE"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-4",
    "href": "sessions/week7_lecture.html#interaction-effects-4",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-5",
    "href": "sessions/week7_lecture.html#interaction-effects-5",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\n\n\n\n\n\n\n\n\n\nInteracting disadvantage with region shows negative effect of disadvantage not as bad in regions other than SE - however…"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-6",
    "href": "sessions/week7_lecture.html#interaction-effects-6",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\n\n\n\n\n\n\n\n\n\nAs overall absence confounds disadvantage, so interacting both with region changes these effects"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-7",
    "href": "sessions/week7_lecture.html#interaction-effects-7",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\nMain Effect (Reference Group):\n\nThe coefficient for log(PTFSM6CLA1A) is -0.190520. This is the effect of disadvantage in the South East.\nThe coefficient for log(PERCTOT) is -0.289960. This is the effect of Overall Absence in the South East.\n\nEven when controlling for regional differences, unauthorised absence has a stronger negative effect on Attainment 8 than disadvantage"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-8",
    "href": "sessions/week7_lecture.html#interaction-effects-8",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\nInteraction Effects (Reference Group value + Other Region coefficient):\n\nlog(PTFSM6CLA1A):gor_nameLondon: significant positive coefficient (+0.078684) - negative effect of disadvantage is less severe in London than in the South East\nsame for all other regions - disadvantage has a more severe impact in the SE than anywhere else in England. SE schools particularly bad at mitigating effects of disadvantage"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-9",
    "href": "sessions/week7_lecture.html#interaction-effects-9",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\nInteraction Effects (Reference Group value + Other Region coefficient):\n\ngor_nameLondon:log(PERCTOT): The significant negative coefficient (-0.069792) - negative effect of Overall Absence is more severe in London than in the South East\nsame for most other regions - Overall Absence as less severe impact in SE than anywhere else in England."
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-10",
    "href": "sessions/week7_lecture.html#interaction-effects-10",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\n\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A) * log(PERCTOT) + \n    gor_name, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24743 -0.05845  0.00082  0.06076  0.72702 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       4.989742   0.064333  77.561  &lt; 2e-16 ***\nlog(PTFSM6CLA1A)                 -0.125059   0.019674  -6.357 2.35e-10 ***\nlog(PERCTOT)                     -0.353934   0.032374 -10.933  &lt; 2e-16 ***\ngor_nameLondon                    0.095751   0.007137  13.417  &lt; 2e-16 ***\ngor_nameEast Midlands            -0.004420   0.007725  -0.572 0.567250    \ngor_nameEast of England           0.002028   0.007123   0.285 0.775845    \ngor_nameNorth East                0.075416   0.009864   7.646 2.72e-14 ***\ngor_nameNorth West                0.004092   0.006853   0.597 0.550493    \ngor_nameSouth West                0.027349   0.007434   3.679 0.000238 ***\ngor_nameWest Midlands             0.028953   0.007184   4.030 5.70e-05 ***\ngor_nameYorkshire and the Humber  0.036074   0.007542   4.783 1.80e-06 ***\nlog(PTFSM6CLA1A):log(PERCTOT)    -0.003798   0.009586  -0.396 0.691945    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1025 on 3221 degrees of freedom\nMultiple R-squared:  0.7182,    Adjusted R-squared:  0.7172 \nF-statistic: 746.2 on 11 and 3221 DF,  p-value: &lt; 2.2e-16\n\n\n\nIt is also possible to interact our continuous variables. However…"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-11",
    "href": "sessions/week7_lecture.html#interaction-effects-11",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\nInteraction term is statistically insignificant\nThere is no evidence that the effect of concentrations of disadvantage on attainment changes based on the levels of unauthorised absence, or vice-versa\nNegative impact of disadvantage is roughly the same whether a school has a low or high unauthorised absence rate, and the negative impact of unauthorised absence is roughly the same regardless of the level of disadvantage\nGood to experiment as informative, but this interaction term can be removed from the model"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-12",
    "href": "sessions/week7_lecture.html#interaction-effects-12",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\nFinal Note: Adding interaction effects will seriously increase your VIF as you are multiplying variables that are already in the model\nHigh VIF in a model with interaction terms does not affect the overall model.\nHigh VIF could inflate the standard errors of the coefficients making some otherwise significant variables seem insignificant\nMean-Centring is a technique that could fix this, but we won’t look at it today"
  },
  {
    "objectID": "sessions/week7_lecture.html#interaction-effects-13",
    "href": "sessions/week7_lecture.html#interaction-effects-13",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Interaction Effects",
    "text": "Interaction Effects\n\n\n\n\n\n\nTip\n\n\n\nInterpreting interaction effects is HARD (as you can probably tell from me trying to explain these!)\nAI is incredibly adept at interpreting outputs from regression models and describing the outputs in plain English. ChatGPT, Google Gemini, Claude and various others can be of great help here\nIt doesn’t always get it right - never just feed an AI your outputs and trust its interpretation without questioning what it’s telling you\nUsed correctly, however - particularly if you give it context with the data you are using and the patterns you are observing, it’s a tool that can really assist your understanding."
  },
  {
    "objectID": "sessions/week7_lecture.html#some-final-thoughts-on-building-regression-models",
    "href": "sessions/week7_lecture.html#some-final-thoughts-on-building-regression-models",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Some final thoughts on building regression models",
    "text": "Some final thoughts on building regression models\n\nAim is to try and explain as much variation in dependent variable as possible with the fewest number of independent variables\nMore variables isn’t necessarily better - statistically insignificant variables might be worth dropping from your model\nAs you add more variables, confounding might change the interpretation of earlier variables and may even stop them being statistically significant\nAlgorithms like the Step-Wise algorithm can automate the process of variable selection - worth experimenting with, but no substitute for doing your own research!\nBuilding a good model takes time, experimentation, iteration and a lot of care and attention"
  },
  {
    "objectID": "sessions/week7_lecture.html#bringing-it-all-together---our-final-regression-show-stopper",
    "href": "sessions/week7_lecture.html#bringing-it-all-together---our-final-regression-show-stopper",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Bringing it all together - our final regression show-stopper!",
    "text": "Bringing it all together - our final regression show-stopper!\n\n\n\nIn carefully building our regression model step-by-step, we have evolved our understanding:\n\nfrom a basic model for Brighton where it appeared that the % disadvantaged students in a school explained around 65% of the variation in Attainment 8 scores and a 10% decrease in disadvantage equating to a 6-point increase in Attainment 8\nto understanding that this initial local model was poor:\n\nfew degrees of freedom\nunobserved confounding and mediation"
  },
  {
    "objectID": "sessions/week7_lecture.html#bringing-it-all-together---our-final-regression-show-stopper-1",
    "href": "sessions/week7_lecture.html#bringing-it-all-together---our-final-regression-show-stopper-1",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Bringing it all together - our final regression show-stopper!",
    "text": "Bringing it all together - our final regression show-stopper!\n\n\n\n\n\n\n\n\n\n\n\n\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n0.83\n0.83\n0.08\n0.08"
  },
  {
    "objectID": "sessions/week7_lecture.html#bringing-it-all-together---our-final-regression-show-stopper-2",
    "href": "sessions/week7_lecture.html#bringing-it-all-together---our-final-regression-show-stopper-2",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Bringing it all together - our final regression show-stopper!",
    "text": "Bringing it all together - our final regression show-stopper!\n\n\n\nAdditional variables (some discussed, so not discussed in this lecture) have allowed us to explain almost 85% of the variation in Attainment 8 at the school level across England\nCareful interrogation of the coefficients as we have successively built up the complexity of the model reveals how the variables interact with each other\n\nThe influence of disadvantage is partially confounded by overall absence\nbut overall absence is a mediating variable in the relationship between disadvantage and attainment"
  },
  {
    "objectID": "sessions/week7_lecture.html#bringing-it-all-together---our-final-regression-show-stopper-3",
    "href": "sessions/week7_lecture.html#bringing-it-all-together---our-final-regression-show-stopper-3",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "Bringing it all together - our final regression show-stopper!",
    "text": "Bringing it all together - our final regression show-stopper!\n\n\n\nDummy variables can be your friend and allow us to compare how different groups perform in the data\nIneracting variables can add even more explanatory power, but will take effort to interpret - AI can be your friend here if you are struggling\nIf you follow the recipe, check you regression diagnostics at each stage and continually VISUALISE your data as you go, a good multiple regression model can be one of the most powerful explanatory tools in your toolbox!"
  },
  {
    "objectID": "sessions/week7_lecture.html#this-weeks-practical",
    "href": "sessions/week7_lecture.html#this-weeks-practical",
    "title": "Prof D’s Regression Sessions - Vol 2",
    "section": "This week’s practical",
    "text": "This week’s practical\n\nRecreating much of what you have seen in this lecture yourself!\nExtending last week’s model and interpreting the outputs in Python or R\nThis is another long practical - you won’t complete it in class today, so you should spend time at home (while listening to the Progression Sessions, preferably) working through this at your own pace.\nIt is important that you are able to build your best model before next week’s lecture and practical session!"
  },
  {
    "objectID": "sessions/week8.html",
    "href": "sessions/week8.html",
    "title": "Week XXX",
    "section": "",
    "text": "This week will introduce XXX.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "8. Multilevel Regression"
    ]
  },
  {
    "objectID": "sessions/week8.html#introduction",
    "href": "sessions/week8.html#introduction",
    "title": "Week XXX",
    "section": "",
    "text": "This week will introduce XXX.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "8. Multilevel Regression"
    ]
  },
  {
    "objectID": "sessions/week8.html#learning-objectives",
    "href": "sessions/week8.html#learning-objectives",
    "title": "Week XXX",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nXXX\nXXX\nXXX",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "8. Multilevel Regression"
    ]
  },
  {
    "objectID": "sessions/week8.html#lecture",
    "href": "sessions/week8.html#lecture",
    "title": "Week XXX",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "8. Multilevel Regression"
    ]
  },
  {
    "objectID": "sessions/week8.html#quiz",
    "href": "sessions/week8.html#quiz",
    "title": "Week XXX",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "8. Multilevel Regression"
    ]
  },
  {
    "objectID": "sessions/week8.html#practical",
    "href": "sessions/week8.html#practical",
    "title": "Week XXX",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "8. Multilevel Regression"
    ]
  },
  {
    "objectID": "sessions/week8.html#further-resources",
    "href": "sessions/week8.html#further-resources",
    "title": "Week XXX",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "8. Multilevel Regression"
    ]
  },
  {
    "objectID": "sessions/week8_practical.html",
    "href": "sessions/week8_practical.html",
    "title": "Practical XXX: XXX",
    "section": "",
    "text": "This week is focussed on XXX."
  },
  {
    "objectID": "sessions/week8_practical.html#to-add-a-callout-note",
    "href": "sessions/week8_practical.html#to-add-a-callout-note",
    "title": "Practical XXX: XXX",
    "section": "To add a callout-note",
    "text": "To add a callout-note\n\n\n\n\n\n\nNote\n\n\n\nSuggestions for a Better Learning Experience:\n\nXXX"
  },
  {
    "objectID": "sessions/week8_practical.html#to-add-python-code-without-running-them",
    "href": "sessions/week8_practical.html#to-add-python-code-without-running-them",
    "title": "Practical XXX: XXX",
    "section": "To add Python code without running them …",
    "text": "To add Python code without running them …\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())"
  },
  {
    "objectID": "sessions/week8_practical.html#to-add-and-run-python-code",
    "href": "sessions/week8_practical.html#to-add-and-run-python-code",
    "title": "Practical XXX: XXX",
    "section": "To add and run Python code",
    "text": "To add and run Python code\n\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())\n\n   Area code          Area name Area type  Population 2011  Population 2021  \\\n0  K04000001  England and Wales  National       56075912.0       59597542.0   \n1  E92000001            England   Country       53012456.0       56490048.0   \n2  W92000004              Wales   Country        3063456.0        3107494.0   \n3  E12000001         North East    Region        2596886.0        2647013.0   \n4  E12000002         North West    Region        7052177.0        7417397.0   \n\n   Percentage change  \n0                6.3  \n1                6.6  \n2                1.4  \n3                1.9  \n4                5.2"
  },
  {
    "objectID": "sessions/week8_practical.html#to-add-a-photo---replace-the-path.-using-relative-path-is-also-okay.",
    "href": "sessions/week8_practical.html#to-add-a-photo---replace-the-path.-using-relative-path-is-also-okay.",
    "title": "Practical XXX: XXX",
    "section": "To add a photo - replace the path. Using relative path is also okay.",
    "text": "To add a photo - replace the path. Using relative path is also okay."
  },
  {
    "objectID": "sessions/week8_practical.html#to-add-some-questions",
    "href": "sessions/week8_practical.html#to-add-some-questions",
    "title": "Practical XXX: XXX",
    "section": "To add some “questions”",
    "text": "To add some “questions”\nThe qmd file will be rendered as two files in sessions folder, including a html and ipynb format. The html file will contain both question and answer, while the ipynb file will contain only the question.\nFor the effect, please check HTML and ipynb.\n\nQuestionAnswerAnswer\n\n\nif ??\n    ??\nelse:\n    ??\n\n\n\n\n\nif 'Moscow' in ['Moscow', 'Beijing']:\n    print(\"Moscow is in the cities list.\")\nelse:\n    print(\"Moscow is not in the cities list.\")\nMoscow is in the cities list."
  },
  {
    "objectID": "sessions/week8_practical.html#youre-done",
    "href": "sessions/week8_practical.html#youre-done",
    "title": "Practical XXX: XXX",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the first QM practical session! If you are still working on it, take you time.\nDon’t worry about understanding every detail of the Python code — what matters most is knowing which functions to use for a specific task, like checking minimum and maximum values or generating boxplots, and knowing how to debug when it goes wrong. Remember, practice makes perfect."
  },
  {
    "objectID": "sessions/weekX.html",
    "href": "sessions/weekX.html",
    "title": "Week XXX",
    "section": "",
    "text": "This week will introduce XXX."
  },
  {
    "objectID": "sessions/weekX.html#introduction",
    "href": "sessions/weekX.html#introduction",
    "title": "Week XXX",
    "section": "",
    "text": "This week will introduce XXX."
  },
  {
    "objectID": "sessions/weekX.html#learning-objectives",
    "href": "sessions/weekX.html#learning-objectives",
    "title": "Week XXX",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nXXX\nXXX\nXXX"
  },
  {
    "objectID": "sessions/weekX.html#lecture",
    "href": "sessions/weekX.html#lecture",
    "title": "Week XXX",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture"
  },
  {
    "objectID": "sessions/weekX.html#quiz",
    "href": "sessions/weekX.html#quiz",
    "title": "Week XXX",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page."
  },
  {
    "objectID": "sessions/weekX.html#practical",
    "href": "sessions/weekX.html#practical",
    "title": "Week XXX",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload"
  },
  {
    "objectID": "sessions/weekX.html#further-resources",
    "href": "sessions/weekX.html#further-resources",
    "title": "Week XXX",
    "section": "Further resources",
    "text": "Further resources"
  },
  {
    "objectID": "sessions/weekX_practical.html",
    "href": "sessions/weekX_practical.html",
    "title": "Practical XXX: XXX",
    "section": "",
    "text": "This week is focussed on XXX."
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-a-callout-note",
    "href": "sessions/weekX_practical.html#to-add-a-callout-note",
    "title": "Practical XXX: XXX",
    "section": "To add a callout-note",
    "text": "To add a callout-note\n\n\n\n\n\n\nNote\n\n\n\nSuggestions for a Better Learning Experience:\n\nXXX"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-python-code-without-running-them",
    "href": "sessions/weekX_practical.html#to-add-python-code-without-running-them",
    "title": "Practical XXX: XXX",
    "section": "To add Python code without running them …",
    "text": "To add Python code without running them …\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-and-run-python-code",
    "href": "sessions/weekX_practical.html#to-add-and-run-python-code",
    "title": "Practical XXX: XXX",
    "section": "To add and run Python code",
    "text": "To add and run Python code\n\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())\n\n   Area code          Area name Area type  Population 2011  Population 2021  \\\n0  K04000001  England and Wales  National       56075912.0       59597542.0   \n1  E92000001            England   Country       53012456.0       56490048.0   \n2  W92000004              Wales   Country        3063456.0        3107494.0   \n3  E12000001         North East    Region        2596886.0        2647013.0   \n4  E12000002         North West    Region        7052177.0        7417397.0   \n\n   Percentage change  \n0                6.3  \n1                6.6  \n2                1.4  \n3                1.9  \n4                5.2"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-a-photo---replace-the-path.-using-relative-path-is-also-okay.",
    "href": "sessions/weekX_practical.html#to-add-a-photo---replace-the-path.-using-relative-path-is-also-okay.",
    "title": "Practical XXX: XXX",
    "section": "To add a photo - replace the path. Using relative path is also okay.",
    "text": "To add a photo - replace the path. Using relative path is also okay."
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-some-questions",
    "href": "sessions/weekX_practical.html#to-add-some-questions",
    "title": "Practical XXX: XXX",
    "section": "To add some “questions”",
    "text": "To add some “questions”\nThe qmd file will be rendered as two files in sessions folder, including a html and ipynb format. The html file will contain both question and answer, while the ipynb file will contain only the question.\nFor the effect, please check HTML and ipynb.\n\nQuestionAnswerAnswer\n\n\nif ??\n    ??\nelse:\n    ??\n\n\n\n\n\nif 'Moscow' in ['Moscow', 'Beijing']:\n    print(\"Moscow is in the cities list.\")\nelse:\n    print(\"Moscow is not in the cities list.\")\nMoscow is in the cities list."
  },
  {
    "objectID": "sessions/weekX_practical.html#youre-done",
    "href": "sessions/weekX_practical.html#youre-done",
    "title": "Practical XXX: XXX",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the first QM practical session! If you are still working on it, take you time.\nDon’t worry about understanding every detail of the Python code — what matters most is knowing which functions to use for a specific task, like checking minimum and maximum values or generating boxplots, and knowing how to debug when it goes wrong. Remember, practice makes perfect."
  }
]