---
title: "Prof D's Regression Sessions - Vol 3"
subtitle: "In the mix - the linear mix(ed effects model)"
author: 
  - name: "Adam Dennett"
email: "a.dennett@ucl.ac.uk"
date-as-string: "1st August 2024"
other: "CASA0007 Quantitative Methods"
from: markdown+emoji
format:
  revealjs: 
    logo: "L6_images/CASA_logo.svg"
    template-partials: 
      - title-slide.html
    transition: none
    slide-number: TRUE
    preview-links: auto
    theme: casa-slides
    chalkboard: true
    
filters:
 - code-visibility
lightbox: auto
title-slide-attributes:
    data-background-image: "L6_images/regression.png"
    data-background-size: stretch
    data-background-opacity: "0.08"
    data-background-color: "#4e3c56"
---

```{r}
#| message: false
#| warning: false
#| include: false
library(casaviz)
library(tidyverse)
library(sf)
library(plotly)
library(leaflet)
library(rgl)
library(dplyr)
library(here)
library(stringr)
library(dplyr)
library(purrr)
library(janitor)
library(readxl)
library(tibble)
library(ggrepel)
library(gganimate)
library(interactions)
library(jtools)
#all england schools
edubase_schools <- read_csv("https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1") %>% 
  clean_names() %>% 
  filter(phase_of_education_name == "Secondary") %>% 
  filter(establishment_status_name == "Open") %>% 
  mutate(urn = as.character(urn))

#read in Brighton Secondary Schools Data
brighton_sec_schools <- read_csv("https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1") %>% 
  clean_names() %>% 
  filter(la_name == "Brighton and Hove") %>% 
  filter(phase_of_education_name == "Secondary") %>% 
  filter(establishment_status_name == "Open") %>%
  st_as_sf(., coords = c("easting", "northing")) %>% 
  st_set_crs(27700)

btn_urn_list <- brighton_sec_schools %>% 
  select(urn) 

england_abs <- read_csv(here("sessions","L6_data", "Performancetables_130242", "2022-2023", "england_abs.csv"), na = c("", "NA", "SUPP", "NP", "NE"))
england_census <- read_csv(here("sessions","L6_data", "Performancetables_130242", "2022-2023", "england_census.csv"), na = c("", "NA", "SUPP", "NP", "NE"))
england_ks4final <- read_csv(here("sessions","L6_data", "Performancetables_130242", "2022-2023", "england_ks4final.csv"), na = c("", "NA", "SUPP", "NP", "NE"))
england_school_information <- read_csv(here("sessions","L6_data", "Performancetables_130242", "2022-2023", "england_school_information.csv"), na = c("", "NA", "SUPP", "NP", "NE"))

la_codes <- read_csv(here("sessions","L6_data", "Performancetables_130249", "2022-2023", "la_and_region_codes_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()

england_ks4final <- england_ks4final %>%
  mutate(URN = as.character(URN)) %>%
  mutate(across(TOTPUPS:PTOTENT_E_COVID_IMPACTED_PTQ_EE, ~ parse_number(as.character(.))))

england_ks4final <- england_ks4final %>%
  filter(!is.na(URN))

england_abs <- england_abs %>%
  mutate(URN = as.character(URN))

england_census <- england_census %>%
  mutate(URN = as.character(URN))

england_school_information <- england_school_information %>%
  mutate(URN = as.character(URN))

# Left join england_ks4final with england_abs
england_school_2022_23 <- england_ks4final %>%
  left_join(england_abs, by = "URN") %>%
  left_join(england_census, by = "URN") %>%
  left_join(england_school_information, by = "URN")

data_types <- sapply(england_school_2022_23, class)
england_school_2022_23_meta <- data.frame(Field = names(data_types), DataType = data_types)

btn_sub <- england_school_2022_23 %>%
  filter(URN %in% btn_urn_list$urn)

#P8_BANDING
england_school_2022_23_not_special <- england_school_2022_23 %>%
  filter(MINORGROUP != "Special school" & ADMPOL.x == "NSE")

eng_sch_2022_23_not_special_plus <- england_school_2022_23_not_special %>% left_join(
  edubase_schools, by = join_by(URN == urn)
)


calculate_index_of_dissimilarity <- function(df, col_A, col_B) {
  # Ensure the dataframe has the necessary columns
  required_columns <- c(col_A, col_B)
  if (!all(required_columns %in% colnames(df))) {
    stop(paste("Dataframe must contain the columns:", paste(required_columns, collapse = ", ")))
  }
  
  # Calculate the total number of disadvantaged and non-disadvantaged pupils
  total_A <- sum(df[[col_A]], na.rm = TRUE)
  total_B <- sum(df[[col_B]], na.rm = TRUE)
  
  # Calculate the index of dissimilarity
  df$dissimilarity_component <- abs(df[[col_A]] / total_A - df[[col_B]] / total_B)
  index_of_dissimilarity <- 0.5 * sum(df$dissimilarity_component, na.rm = TRUE)
  
  return(index_of_dissimilarity)
}

calculate_gorard_segregation <- function(df, col_A, col_T) {
  # Ensure the dataframe has the necessary columns
  required_columns <- c(col_A, col_T)
  if (!all(required_columns %in% colnames(df))) {
    stop(paste("Dataframe must contain the columns:", paste(required_columns, collapse = ", ")))
  }
  
  # Calculate the total number of disadvantaged pupils and total pupils
  total_A <- sum(df[[col_A]], na.rm = TRUE)
  total_T <- sum(df[[col_T]], na.rm = TRUE)
  
  # Calculate the Gorard Segregation Index
  df$gorard_component <- abs(df[[col_A]] / total_A - df[[col_T]] / total_T)
  gorard_segregation <- 0.5 * sum(df$gorard_component, na.rm = TRUE)
  
  return(gorard_segregation)
}


# Apply the functions to each LEA and create a new dataframe with the results
results_df <- england_school_2022_23_not_special %>%
  group_by(LEA) %>%
  group_map(~ tibble(
    LEA = .y$LEA,
    index_of_dissimilarity = calculate_index_of_dissimilarity(.x, col_A = "TFSM6CLA1A", col_B = "TNOTFSM6CLA1A"),
    gorard_segregation = calculate_gorard_segregation(.x, col_A = "TFSM6CLA1A", col_T = "TPUP")
  )) %>%
  bind_rows()



```

```{r}
#| eval: false
#| include: false
library(plotly)

# Assuming merged_df has columns:
# index_of_dissimilarity, gorard_segregation, la_name, region_name

fig <- plot_ly(
  data = merged_df,
  x = ~index_of_dissimilarity,
  y = ~gorard_segregation,
  type = 'scatter',
  mode = 'markers',
  color = ~region_name,       # Color by region
  text = ~la_name,            # Hover shows Local Authority name
  hoverinfo = 'text+x+y',
  marker = list(size = 6, opacity = 0.7)
) %>%
  layout(
    title = "Scatter Plot of Index of Dissimilarity vs Gorard Segregation, all LEAs in England",
    xaxis = list(title = "Index of Dissimilarity"),
    yaxis = list(title = "Gorard Segregation Index")
  )

fig
```

```{r}
#| message: false
#| warning: false

base_path <- here("sessions", "L6_data", "Performancetables_130242", "2022-2023")
na_all <- c("", "NA", "SUPP", "NP", "NE", "SP", "SN", "LOWCOV", "NEW", "SUPPMAT")

england_filtered <- read_csv(file.path(base_path, "england_filtered.csv"), na = na_all) |> mutate(URN = as.character(URN))

#str(england_filtered)

england_filtered_clean <- england_filtered[
  !is.na(england_filtered$ATT8SCR) & 
  !is.na(england_filtered$PTFSM6CLA1A) &
  england_filtered$ATT8SCR > 0 &
  england_filtered$PTFSM6CLA1A > 0, 
]

```

# This week

## Recap

-   Last week we saw how multiple regression models can allow us to understand complex relationships between predictor and outcome variables
-   We began to explore how by increasing the complexity of our regression models, we can begin to observe how the effects of different variables can confound (obscrure) and mediate (partially cause) the effects of others, giving us a deeper understanding of our system of interest
-   We were able to see that with just a relatively small number of variables, we could explain most of the variation in school-level attainment scores in England

## OLS regression with interaction terms

```{r}
#| message: false
#| warning: false
#| width: 80%
# Fit linear model and get predicted values
model_data <- england_filtered %>%
  filter(!is.na(ATT8SCR), !is.na(PTFSM6CLA1A), !is.na(PERCTOT))

model_data <- model_data %>%
  filter(
    is.finite(ATT8SCR), ATT8SCR > 0,
    is.finite(PTFSM6CLA1A), PTFSM6CLA1A > 0,
    is.finite(PERCTOT), PERCTOT > 0
  )

lm_fit1 <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A) + log(PERCTOT), data = model_data, na.action = na.exclude)

#summary(lm_fit1)

model_data <- model_data %>%
  mutate(
    fitted_ATT8SCR = exp(fitted(lm_fit1)),
    resids_real = ATT8SCR - fitted_ATT8SCR
  )

model_data$gor_name <- relevel(factor(model_data$gor_name), ref = "South East")
model_data$log_PERCTOT <- log(model_data$PERCTOT)
model_data$log_PTFSM6CLA1A <- log(model_data$PTFSM6CLA1A)

#lm_fit3a <- lm(log(ATT8SCR) ~ log_PTFSM6CLA1A + log_PERCTOT * gor_name, data = model_data)
lm_fit3a <- lm(log(ATT8SCR) ~ log_PERCTOT * gor_name, data = model_data)

#plot_summs(lm_fit3a, robust = TRUE)

casa_palette <- as.character(casa_colours[1:9])

#scale_colour_casa()
# or another palette like casa_dark

interactions::interact_plot(
  lm_fit3a,
  pred = "log_PERCTOT",
  modx = "gor_name",
  plot.points = F,
  facet.modx = F,
  colors = casa_palette
)

england_model1 <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered_clean)

```

-   In our most sophisticated model we could even see how the relationship between our main predictor and our outcome might vary between levels of a categorical variable

## OLS regression with Interaction Terms

-   Interacting categorical variables with other continuous predictor variables allowed us to see how levels of continuous predictor slope coefficient vary according to the categorical predictor
-   In our example, we saw how in a simple bivariate model of Attainment 8 vs Disadvantage, the slope less severe in regions such as London, the West Midlands and Yorkshire and the Humber when compared to the South East.
-   The effects of concentrations of disadvantaged pupils in schools felt more severely in the South East.

## Drawbacks of OLS with Interaction

-   OLS assumes all observations (schools) are independent of each other, when in reality, they may have characteristics which mean they are not independent - i.e. they are in the same local authority or have similar Ofsted ratings which mean they share some characteristics with each other in terms of governance etc.
-   OLS assumes the effects (parameters) are constant (fixed) across the whole population / dataset
-   OLS assumes the errors are independent
-   OLS only has one global intercept with group level differences in the data only crudely represented with dummy variables

## Different Slopes and Intercepts

```{r}
filtered_data <- england_filtered_clean %>%
  filter(OFSTEDRATING != "Inadequate" & !is.na(OFSTEDRATING))
filtered_region <- england_filtered_clean %>%
  filter(OFSTEDRATING != "Inadequate" & !is.na(OFSTEDRATING) & !is.na(gor_name))
btn_sub <- filtered_region %>%
  filter(URN %in% btn_urn_list$urn)

# Base plot with england_school_2022_23
plot <- ggplot(filtered_data, aes(y = log(ATT8SCR), x = log(PTFSM6CLA1A), colour = OFSTEDRATING)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Add linear model line for england_school_2022_23
  labs(title = "Attainment 8 vs % Deprived Students, 2022-23",
       x = "log(% Deprived Students)",
       y = "log(Attainment 8 Score)",
       color = "Ofsted Rating") +
  theme_minimal()

# Add another layer with btn_sub points and labels with sticks
plot #+ 
#  geom_point(data = btn_sub, aes(y = log(ATT8SCR), x = log(PTFSM6CLA1A)), color = "black") +
#  geom_smooth(data = btn_sub, aes(y = log(ATT8SCR), x = log(PTFSM6CLA1A)), method = "lm", se = FALSE, color = "black") +  # Add linear model line for btn_sub
#  geom_text_repel(data = btn_sub, aes(y = log(ATT8SCR), x = log(PTFSM6CLA1A), label = SCHNAME.x), color = "black", size = 3, nudge_y = c(-1.5, 1.5), force = 10, box.padding = 0.5, max.overlaps = 10, direction = "both") 
```

## Different Slopes and Intercepts

```{r}
library(ggplot2)
library(dplyr)

# Step 1: Compute lm coefficients per Ofsted rating
lm_labels <- england_filtered_clean %>%
  group_by(OFSTEDRATING) %>%
  summarise(
    model = list(lm(log(ATT8SCR) ~ log(PTFSM6CLA1A))),
    .groups = "drop"
  ) %>%
  mutate(
    intercept = sapply(model, function(m) coef(m)[1]),
    slope = sapply(model, function(m) coef(m)[2]),
    label = paste0("y = ", round(slope, 2), "x + ", round(intercept, 2))
  )

# Step 2: Merge label positions (optional: use median values for placement)
label_positions <- england_filtered_clean %>%
  group_by(OFSTEDRATING) %>%
  summarise(
    x = 0,
    y = 3.2,
    .groups = "drop"
  )

lm_labels <- left_join(lm_labels, label_positions, by = "OFSTEDRATING")

# Step 3: Plot with annotations
ggplot(england_filtered_clean, aes(y = log(ATT8SCR), x = log(PTFSM6CLA1A), colour = OFSTEDRATING)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_text(data = lm_labels, aes(x = x, y = y, label = label), inherit.aes = FALSE, hjust = 0, size = 3.5) +
  labs(
    title = "Attainment 8 vs % Deprived Students, 2022-23",
    x = "log(% Deprived Students)",
    y = "log(Attainment 8 Score)",
    color = "Ofsted Rating"
  ) +
  theme_minimal() +
  facet_wrap(~ OFSTEDRATING)
```

```{r}
#| eval: false
#| include: false
filtered_data <- england_filtered_clean %>%
  filter(OFSTEDRATING != "Inadequate" & !is.na(OFSTEDRATING))
filtered_region <- england_filtered_clean %>%
  filter(OFSTEDRATING != "Inadequate" & !is.na(OFSTEDRATING) & !is.na(gor_name))
btn_sub <- filtered_region %>%
  filter(URN %in% btn_urn_list$urn)

# Base plot with england_school_2022_23
plot <- ggplot(filtered_data, aes(y = log(ATT8SCR), x = log(PERCTOT), colour = OFSTEDRATING)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Add linear model line for england_school_2022_23
  labs(title = "Attainment 8 vs Overall Absence %, 2022-23",
       x = "% Overall Absence",
       y = "Attainment 8 Score",
       color = "Ofsted Rating") +
  theme_minimal()

# Add another layer with btn_sub points and labels with sticks
plot + 
  geom_point(data = btn_sub, aes(y = log(ATT8SCR), x = log(PERCTOT)), color = "black") +
  geom_smooth(data = btn_sub, aes(y = log(ATT8SCR), x = log(PERCTOT)), method = "lm", se = FALSE, color = "black") +  # Add linear model line for btn_sub
  geom_text_repel(data = btn_sub, aes(y = log(ATT8SCR), x = log(PERCTOT), label = SCHNAME.x), color = "black", size = 3, nudge_y = c(-1.5, 1.5), force = 10, box.padding = 0.5, max.overlaps = 10, direction = "both") 
```

```{r}
#| eval: false
#| include: false
ggplot(england_filtered_clean, aes(y = log(ATT8SCR), x = log(PERCTOT), colour = OFSTEDRATING)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Add linear model line for england_school_2022_23
  labs(title = "Attainment 8 vs Overall Absence %, 2022-23",
       x = "% Overall Absence",
       y = "Attainment 8 Score",
       color = "Ofsted Rating") +
  theme_minimal() +
  facet_wrap(~ OFSTEDRATING)  # Facet plots for each Ofsted rating
```

## Different Slopes and Intercepts

```{r}

# Base plot with england_school_2022_23
library(ggplot2)

# Fit global linear model
global_model <- lm(P8MEA ~ PTFSM6CLA1A, data = filtered_data)
intercept <- round(coef(global_model)[1], 3)
slope <- round(coef(global_model)[2], 3)

# Create annotation text
annotation_text <- paste0("Global LM: y = ", intercept, " + ", slope, "x")

# Plot with individual colored lines and global black line
plot <- ggplot(filtered_data, aes(x = PTFSM6CLA1A, y = P8MEA, colour = OFSTEDRATING)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +  # Individual lines by OFSTEDRATING
  geom_smooth(method = "lm", se = FALSE, color = "black", aes(group = 1)) +  # Global line
  annotate("text", x = 10, 
           y = 2,
           label = annotation_text, hjust = 0, size = 4, color = "black") +
  labs(
    title = "Progress 8 vs % Deprived Students, 2022-23",
    x = "% Deprived Students",
    y = "Progress 8 Score",
    color = "Ofsted Rating"
  ) +
  theme_minimal()

# Add another layer with btn_sub points and labels with sticks
plot #+ 
#  geom_point(data = btn_sub, aes(y = P8MEA, x = PTFSM6CLA1A), color = "black") +
#  geom_smooth(data = btn_sub, aes(y = P8MEA, x = PTFSM6CLA1A), method = "lm", se = FALSE, color = "black") +  # Add linear model line for btn_sub
#  geom_text_repel(data = btn_sub, aes(y = P8MEA, x = PTFSM6CLA1A, label = SCHNAME.x), color = "black", size = 3, nudge_y = c(-1.5, 1.5), force = 10, box.padding = 0.5, max.overlaps = 10, direction = "both") 
```

## Different Slopes and Intercepts

```{r}
# Step 1: Compute lm coefficients per Ofsted rating
lm_labels <- england_filtered_clean %>%
  group_by(OFSTEDRATING) %>%
  summarise(
    model = list(lm(P8MEA ~ PTFSM6CLA1A)),
    .groups = "drop"
  ) %>%
  mutate(
    intercept = sapply(model, function(m) coef(m)[1]),
    slope = sapply(model, function(m) coef(m)[2]),
    label = paste0("y = ", round(intercept, 2), " + ", round(slope, 2), "x")
  )

# Step 2: Merge label positions (optional: use median values for placement)
label_positions <- england_filtered_clean %>%
  group_by(OFSTEDRATING) %>%
  summarise(
    x = 0,
    y = 2,
    .groups = "drop"
  )

lm_labels <- left_join(lm_labels, label_positions, by = "OFSTEDRATING")

ggplot(england_filtered_clean, aes(y = P8MEA, x = PTFSM6CLA1A, colour = OFSTEDRATING)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_text(data = lm_labels, aes(x = x, y = y, label = label), inherit.aes = FALSE, hjust = 0, size = 3.5) + 
  # Add linear model line for england_school_2022_23
  labs(title = "Progress 8 vs Overall Absence %, 2022-23",
       x = "% Overall Absence",
       y = "Attainment 8 Score",
       color = "Ofsted Rating") +
  theme_minimal() +
  facet_wrap(~ OFSTEDRATING)  # Facet plots for each Ofsted rating
  
```

## Different Slopes and Intercepts

-   Solution: can we just partition our data and run different models for different groups?
-   Answer: Well yes, you could, but it might also be useful to explore more systematically how slopes and intercepts vary between and within groups in your data - that is also useful information

In order to model these grouping factors more explicitly, we need a new type of model - a linear mixed effects model

## Jargon Alert!

-   Before we embark on our linear mixed effects journey, there are two key pieces of jargon you need to learn
-   ***Fixed Effects*** - A Fixed Effect is your traditional explanatory variable (or covariate) whose relationship with the outcome you are primarily interested in estimating for its own sake.
    -   Example: Proportion of disadvantaged students in a school. You want the single, best-estimated coefficient ($\beta$) for this factor.
-   ***Random Effects*** - A Random Effect is a grouping factor (or nesting factor) whose variability you want to account for, but whose specific individual levels you don't necessarily want to draw conclusions about.
    -   Example: The different Ofsted ratings (or individual Schools). You're interested in how much the relationship varies across these groups, not the specific effect of 'Longhill School'

## Fixed Effects - More Detail

-   Fixed effects estimate the mean relationship across all groups. They represent the "fixed" part of the model that applies to every observation.
    -   You estimate a single $\beta$ coefficient for a fixed effect, which represents the population average effect of that variable.
    -   Fixed effects can be either continuous (e.g., % of disadvantaged students in a school) or categorical (region, ofsted rating).
-   Depending on how you formulate your model, a fixed effect could be a random effect in a different context.

## Random Effects - More Detail

-   Random effects, (random intercepts and slopes) represent the deviations of each group's effect from the overall fixed-effect average.
    -   When you model a grouping factor (like Ofsted Rating) as a random effect, you are not estimating five separate coefficients (one for each rating).
    -   Instead, you estimate a variance term ($\sigma^2_{\text{group}}$) that quantifies how much the true group effects (intercepts and/or slopes) scatter around the overall mean effect.
    -   This approach is used when the groups (e.g., the 5 Ofsted categories or the hundreds of individual schools) are considered a random sample from a larger, unobserved population of groups.
    -   This allows the model to "borrow strength" across groups to stabilise estimates (the principle of shrinkage).

## Linear Mixed Effects Models

-   A Linear Mixed Effects (LME) model is a more general class of statistical model that features both a fixed-effects component (the population-average parameters) and a random-effects component (the group-specific deviations)
-   A common type of LME model is the multilevel or hierarchical linear model.
    -   Multilevel models focus specifically on nested data structures, e.g. students in classes $\rightarrow$ classes within schools $\rightarrow$ schools within local authorities $\rightarrow$ local authorities within regions
-   All multilevel models are linear mixed effects models, but linear mixed effects models can handle even more types of groupings (e.g. longitudinal - repeated observations over time)

## Linear Mixed Effects Models

![](L6_images/multilevel_data.webp)

Multilevel Data Structures (Source:Â [Gelman & Hill (2006)](http://www.stat.columbia.edu/~gelman/arm/)) and <https://paulrjohnson.net/blog/2022-11-01-multilevel-model-r-cheatsheet/>

```{r}
#| message: false
#| warning: false
library(GGally)

ggpairs(england_filtered_clean, , mapping = aes(color = OFSTEDRATING), columns = c("P8MEA", "PTFSM6CLA1A", "PERCTOT", "OFSTEDRATING"))


```

## The Null Model

-   The first step in a linear mixed effects model is to fit what is often called a null model and is also called the Variance Components Model

$$\text{lmer}(\text{P8MEA} \sim 1 + (1|\text{OFSTEDRATING}), \text{data} = \dots)$$

```{r}
library(lme4)

lm1 <- lm(P8MEA ~ PTFSM6CLA1A + OFSTEDRATING, data = england_filtered_clean)
summary(lm1)

lme1 <- lmer(P8MEA ~ PTFSM6CLA1A + (1|OFSTEDRATING), data = england_filtered_clean)
summary(lme1)
```
