{
  "hash": "060ed2aa674e24e9a7730af6fb9ab6ac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Prof D's Regression Sessions - Vol 1\"\nsubtitle: \"AKA - Applied Linear Regression - Basics\"\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n  ipynb: default\nfilters:\n  - qna\n  - multicode\n  - quarto # keep Quartoâ€™s built-ins as the last filter or it won't work\n\n---\n\n\n\n\n\n\n```{=html}\n<iframe data-testid=\"embed-iframe\" style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/1f2SbTAoE90QelFZlrpzxT?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n```\n\n## Preamble\n\nOver the next 3 weeks we are going **DEEP** into linear regression. The QM Regression Sessions will be tough, but we will be eased through the journey in the company of LTJ Bukem, Blame, Conrad and others. Plug into the Progression Sessions, let them sooth your mind and allow you to get fully into the Regression Sessions!\n\n## Introduction\n\nThis week's practical is focused on understanding the most basic form of regression analysis, but in an applied context discovering what these models help you understand about your data in great detail. You will essentially re-create some of the analysis you saw in the lecture, but for slightly different variables and for different places.\n\nMost of what you will be doing in this practical won't actually be modelling - although this will come at the end - but you will be getting to know your data so that your final models make sense. This is how it should be!! Going back to our bake-off analogy, the modelling is really just the final bake after hours of preparation.\n\n## Aims\n\nBy the end of this week's regression session you should:\n\n-   Be able to use R or Python to process data in order to carry out a scientific investigation\n-   Feel comfortable in plotting and visualising that processed data to assess relationships between x and y variables\n-   Understand how to use built-in statistical software functions in R and Python to run specify and a basic regression model and produce statistical outputs from those models\n-   Interpret those model outputs critically in order to evaluate the strenght and direction of the relationships you are observing. \n\n::: callout-note\nBelow you will find code-blocks that will allow you to run your analysis in either R or Python. It's up to you which you decide to use - you might even want to try both to compare (Of course, R will be better)!\n\nThe Juypter notebook attached to this practical *might* work, but you are probably better off just firing up RStudio or VS code, creating a new notebook and editing from scratch on your local machine.\n:::\n\n## Tasks\n\n::: callout-warning\nMake sure you complete each of the tasks below, 1-4. All of these will be covered in the code examples as you work your way down the page, but this is the full [workflow](https://dictionary.cambridge.org/dictionary/english/workflow):\n:::\n\n### 1. Downloading: Download and read data into your software environment:\n\n-   School locational and attribute information, downloaded from here - <https://get-information-schools.service.gov.uk/Downloads> - or read it in using the code supplied below. This will include the following datasets:\n\n-   **edubase_schools** = this is the 'all establishment' data for every school in England and Wales\n\n-   School-level Key Stage 4 (14-16 GCSE level) attainment and other attribute information. [I have put this on Moodle here](https://moodle.ucl.ac.uk/mod/folder/view.php?id=8199904), but it can also be downloaded from here - <https://www.compare-school-performance.service.gov.uk/download-data>.\n\n    Download to a local drive on your computer from Moodle, unzip it and then read it in using the code supplied below. The zip folders contain the following data files in one folder and the associated metadata in the other folder:\n\n    a.  **england_ks4final** = data on the 2022/23 academic year, school-level, KS4 outcomes and associated statistics\n\n    b.  **england_abs** = data on the 2022/23 academic year, school-level absence rates\n\n    c.  **england_census** = additional school-level pupil information for the 2022/23 academic year\n\n    d.  **england_school_information** = additional school-level institution information such as Ofsted rating etc. for the 2022/23 academic year\n\n    e.  **england_ks4_pupdest** = pupil destination data (where students go once they leave school at 16)\n\n### 2. Data Munging: Join, filter, select and subset:\n\n-   Create a master file for England called **england_school_2022_23.** This will be made by joining together all of the school-level data above into a single file and then reducing it in size using:\n    a.  **filter** - so that it just contains open secondary schools\n    b.  **select** - so that only a few key variables relating to attainment, progress, deprivation, absence and school quality remain, alongside key locational and other attributes.\n-   Create two local authority data **subsets** from this main file:\n    a.  A subset containing all of the secondary schools in one of the 32 London Boroughs (not including the City of London)\n    b.  A subset containing all of the secondary schools in any other local authority in England. Any local authority you like - go wild!\n\nAt the end of this you will have **x3 datasets** - one national and two local authority subsets\n\n### 3. Exploratory Data Analysis\n\n-   Choose:\n    a.  one attainment related dependent variable from your set\n    b.  one possible explanatory variable which you think might help explain levels of attainment in schools\n-   Create a series of graphs and statistical outputs to allow you to understand the structure and composition of your data. You should produce:\n    a.  A histogram for the dependent and independent variables you are analysing for **both your two subsets** and the **national dataset** (x6 histograms in total). You might want to augment your histograms to include:\n        i.  mean and median lines\n        ii. lines for the upper and lower quartiles\n        iii. indications of outliners\n        iv. a kernel density smoothing\n    b.  Alternative plots such as violin plots or box plots to compare with the histogram\n    c.  Point Maps of the locations of your schools in your two subsets - points sized by either of your variables\n    d.  A scatter plot of your dependent (attainment) and independent (disadvantage) variables (national and two local subsets) - on these plots you might want to include:\n        i.  a regression line of best fit\n        ii. the R-squared, slope and intercept parameters\n\n### 4. Explanatory Data Analysis - Attainment and Factors Affecting it in Different Parts of England\n\n-   Carry out a bi-variate regression analysis to explore the relationship between your chosen attainment dependent variable and your single explanatory independent variable, for your two subsets and the national dataset. **Questions to Answer**:\n    a.  After visualising the relationships using scatter plots, do you think you need to transform your variables (e.g. using a log transformation) before putting into a regression model? Maybe plot scatter plots of your logged variables too!\n    b.  What are the slope and intercept coefficients in each model? Are they statistically significant and how do vary between your two subsets? How do they compare with the national picture?\n    c.  How do the R-squared values differ and how reliable do you think they are (what do the degrees of freedom tell you)?\n    d.  What observations can you make about your two local authority study areas relative to the national picture?\n\n## Task 1 - Download your data and read into your software environment\n\n-   I've made this task easy for you by giving you the code to download all of this data. You'll need to download a selection of files - click on either tab to see the code for each.\n-   In order to run this code, you should first [download and unzip the data from Moodle](https://moodle.ucl.ac.uk/mod/folder/view.php?id=8199904), before editing the code below so that you can read the data from a local folder on your computer:\n\n::: callout-note\nI would encourage you **NOT** to just copy and paste this code blindly, but try to understand what it is doing at each stage.\n\nYou may notice that there are some bits that are redundant (some of the NA code that I was messing with before deciding on a single way of removing NAs).\n\nWhat am I doing to the URN field? Why might I be doing that?\n\nWhat is the regex function doing and why might I be searching out those characters? What might happen to the data if I didn't handle those first?\n:::\n\n::: qna\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport janitor\nfrom pathlib import Path\n\n# little function to define the file root on different machines\ndef find_qm_root(start_path: Path = Path.cwd(), anchor: str = \"QM_Fork\") -> Path:\n    \"\"\"\n    Traverse up from the start_path until the anchor folder (e.g., 'QM')      is found. Returns the path to the anchor folder.\n    \"\"\"\n    for parent in [start_path] + list(start_path.parents):\n        if parent.name == anchor:\n            return parent\n    raise FileNotFoundError(f\"Anchor folder '{anchor}' not found in path      hierarchy.\")\n  \nqm_root = find_qm_root()\n\n\n# Read CSV file\nedubase_schools = pd.read_csv(\n    \"https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1\",\n    encoding=\"cp1252\",\n    low_memory=False,\n    dtype={\"URN\": str}\n)\nedubase_schools = edubase_schools.clean_names()\n#py_edubase_schools.dtypes\n#dtype_df = py_edubase_schools.dtypes.reset_index()\n#dtype_df.columns = [\"column_name\", \"dtype\"]\n\n########################################\n# Define base path and common NA values - NOTE YOU WILL NEED TO DOWNLOAD THIS DATA AND PUT INTO YOUR OWN base_path LOCATION ON YOUR MACHINE - not this one as this is my path!!!\n#base_path = Path(\"E:/QM/sessions/L6_data/Performancetables_130242/2022-2023\")\nbase_path = qm_root / \"sessions\" / \"L6_data\" / \"Performancetables_130242\" / \"2022-2023\"\n##################################################\nna_values_common = [\"\", \"NA\", \"SUPP\", \"NP\", \"NE\"]\nna_values_extended = na_values_common + [\"SP\", \"SN\"]\nna_values_attainment = na_values_extended + [\"LOWCOV\", \"NEW\"]\nna_values_mats = na_values_common + [\"SUPPMAT\"]\nna_all = [\"\", \"NA\", \"SUPP\", \"NP\", \"NE\", \"SP\", \"SN\", \"LOWCOV\", \"NEW\", \"SUPPMAT\", \"NaN\"]\n\n# Absence data\nengland_abs = pd.read_csv(base_path / \"england_abs.csv\", na_values=na_all, dtype={\"URN\": str})\n\n# Census data\nengland_census = pd.read_csv(base_path / \"england_census.csv\", na_values=na_all, dtype={\"URN\": str})\nengland_census.iloc[:, 4:23] = england_census.iloc[:, 4:23].apply(lambda col: pd.to_numeric(col.astype(str).str.replace('%', '', regex=False), errors=\"coerce\"))\n\n# KS4 MATs performance data\nengland_ks4_mats_performance = pd.read_csv(base_path / \"england_ks4-mats-performance.csv\", na_values=na_all, encoding=\"cp1252\", low_memory=False, dtype={\"URN\": str})\nengland_ks4_mats_performance[\"TRUST_UID\"] = england_ks4_mats_performance[\"TRUST_UID\"].astype(str)\nengland_ks4_mats_performance[\"P8_BANDING\"] = england_ks4_mats_performance[\"P8_BANDING\"].astype(str)\nengland_ks4_mats_performance[\"INSTITUTIONS_INMAT\"] = england_ks4_mats_performance[\"INSTITUTIONS_INMAT\"].astype(str)\n\ncols_to_convert = england_ks4_mats_performance.columns[10:]\nexclude = [\"P8_BANDING\", \"INSTITUTIONS_INMAT\"]\ncols_to_convert = [col for col in cols_to_convert if col not in exclude]\nengland_ks4_mats_performance[cols_to_convert] = england_ks4_mats_performance[cols_to_convert].apply(lambda col: pd.to_numeric(col.astype(str).str.replace('%', '', regex=False), errors=\"coerce\"))\n\n# KS4 pupil destination data\nengland_ks4_pupdest = pd.read_csv(base_path / \"england_ks4-pupdest.csv\", na_values=na_all, dtype={\"URN\": str})\nengland_ks4_pupdest.iloc[:, 7:82] = england_ks4_pupdest.iloc[:, 7:82].apply(lambda col: pd.to_numeric(col.astype(str).str.replace('%', '', regex=False), errors=\"coerce\"))\n\n# KS4 final attainment data\nengland_ks4final = pd.read_csv(base_path / \"england_ks4final.csv\", na_values=na_all, dtype={\"URN\": str})\nstart_col = \"TOTPUPS\"\nend_col = \"PTOTENT_E_COVID_IMPACTED_PTQ_EE\"\ncols_range = england_ks4final.loc[:, start_col:end_col].columns\n# Strip % signs and convert to numeric\nengland_ks4final[cols_range] = england_ks4final[cols_range].apply(\n    lambda col: pd.to_numeric(col.astype(str).str.replace('%', '', regex=False), errors=\"coerce\")\n)\n\n# School information data\nengland_school_information = pd.read_csv(\n    base_path / \"england_school_information.csv\",\n    na_values=na_all,\n    dtype={\"URN\": str},\n    parse_dates=[\"OFSTEDLASTINSP\"],\n    dayfirst=True  # Adjust if needed\n)\n```\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(here)\n\n# Read CSV file\nedubase_schools <- read_csv(\"https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1\") %>% \n  clean_names() %>% \n  mutate(urn = as.character(urn))\n#str(r_edubase_schools)\n\n# Define base path and common NA values - NOTE YOU WILL NEED TO DOWNLOAD THIS DATA AND PUT INTO YOUR OWN base_path LOCATION ON YOUR MACHINE\nbase_path <- here(\"sessions\", \"L6_data\", \"Performancetables_130242\", \"2022-2023\")\nna_common <- c(\"\", \"NA\", \"SUPP\", \"NP\", \"NE\")\nna_extended <- c(na_common, \"SP\", \"SN\")\nna_attainment <- c(na_extended, \"LOWCOV\", \"NEW\")\nna_mats <- c(na_common, \"SUPPMAT\")\nna_all <- c(\"\", \"NA\", \"SUPP\", \"NP\", \"NE\", \"SP\", \"SN\", \"LOWCOV\", \"NEW\", \"SUPPMAT\")\n\n# Absence data\nengland_abs <- read_csv(file.path(base_path, \"england_abs.csv\"), na = na_all) |>\n  mutate(URN = as.character(URN))\n\n# Other School Census data\nengland_census <- read_csv(file.path(base_path, \"england_census.csv\"), na = na_all) |>\n  mutate(URN = as.character(URN)) |>\n  mutate(across(5:23, ~ parse_number(as.character(.))))\n\n# KS4 MATs performance data\n#First read\nengland_ks4_mats_performance <- read_csv(\n  file.path(base_path, \"england_ks4-mats-performance.csv\"),\n  na = na_all\n) |>\n  mutate(\n    TRUST_UID = as.character(TRUST_UID),\n    P8_BANDING = as.character(P8_BANDING),\n    INSTITUTIONS_INMAT = as.character(INSTITUTIONS_INMAT)\n  )\n\n# Then Identify columns to convert (excluding character columns)\ncols_to_convert <- england_ks4_mats_performance |>\n  select(-(1:10), -P8_BANDING, -INSTITUTIONS_INMAT) |>\n  names()\n\n# Then apply parse_number safely to convert characters to numbers\nengland_ks4_mats_performance <- england_ks4_mats_performance |>\n  mutate(across(all_of(cols_to_convert), ~ parse_number(as.character(.))))\n\n\n# KS4 pupil destination data\nengland_ks4_pupdest <- read_csv(file.path(base_path, \"england_ks4-pupdest.csv\"), na = na_all) |>\n  mutate(URN = as.character(URN)) |>\n  mutate(across(8:82, ~ parse_number(as.character(.))))\n\n# KS4 final attainment data\nengland_ks4final <- read_csv(file.path(base_path, \"england_ks4final.csv\"), na = na_all) |>\n  mutate(URN = as.character(URN)) |>\n  mutate(across(TOTPUPS:PTOTENT_E_COVID_IMPACTED_PTQ_EE, ~ parse_number(as.character(.))))\n\n# School information data\nengland_school_information <- read_csv(\n  file.path(base_path, \"england_school_information.csv\"),\n  na = na_all,\n  col_types = cols(\n    URN = col_character(),\n    OFSTEDLASTINSP = col_date(format = \"%d-%m-%Y\")\n  )\n)\n```\n:::\n\n:::\n\n## Task 2 - Data Munging\n\n-   Use Python or R to join all of the datasets you loaded in the previous step into a single master file that we will called `england_school_2022_23` using the unique school reference number (URN) as the key.\n-   Create the master file:\n\n::: qna\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Left join england_ks4final with england_abs, census, and school information\nengland_school_2022_23 = (\n    england_ks4final\n    .merge(england_abs, on=\"URN\", how=\"left\")\n    .merge(england_census, on=\"URN\", how=\"left\")\n    .merge(england_school_information, on=\"URN\", how=\"left\")\n    .merge(edubase_schools, left_on=\"URN\", right_on=\"urn\", how=\"left\")\n)\n```\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Left join england_ks4final with england_abs\nengland_school_2022_23 <- england_ks4final %>%\n  left_join(england_abs, by = \"URN\") %>%\n  left_join(england_census, by = \"URN\") %>%\n  left_join(england_school_information, by = \"URN\") %>%\n  left_join(edubase_schools, by = c(\"URN\" = \"urn\"))\n```\n:::\n\n:::\n\n#### Iterative Checking and initial exploratory analysis\n\n-   It's never too soon to start exploring your data - even (or indeed especially) in the data 'munging' (processing, cleaning etc.) stage.\n-   You will probably find yourself iteratively plotting, filtering, plotting again, switching variables, plotting again, over and over again until you are happy with the data you have. This is all part of the Exploratory Data Analysis Process and vital before applying any other methods to your data!\n-   If you recall from the lecture, the histogram of attainment 8 in the whole of England dataset looked a bit odd:\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Compute statistics\nmedian_value = england_school_2022_23[\"ATT8SCR\"].median(skipna=True)\nmean_value = england_school_2022_23[\"ATT8SCR\"].mean(skipna=True)\nsd_value = england_school_2022_23[\"ATT8SCR\"].std(skipna=True)\n\n# Set up the figure\nplt.figure(figsize=(10, 6))\n\n# Histogram with density\nsns.histplot(\n    data=england_school_2022_23,\n    x=\"ATT8SCR\",\n    stat=\"density\",\n    binwidth=1,\n    color=\"#4E3C56\",\n    alpha=0.4,\n    linewidth=0.5,\n    edgecolor=\"white\"\n)\n\n# Overlay normal distribution curve\nx_vals = np.linspace(\n    england_school_2022_23[\"ATT8SCR\"].min(),\n    england_school_2022_23[\"ATT8SCR\"].max(),\n    500\n)\nnormal_density = (\n    1 / (sd_value * np.sqrt(2 * np.pi))\n) * np.exp(-0.5 * ((x_vals - mean_value) / sd_value) ** 2)\nplt.plot(x_vals, normal_density, color=\"#2E6260\", linewidth=1)\n\n# Add vertical lines for median and mean\nplt.axvline(median_value, color=\"black\", linestyle=\"dotted\", linewidth=1)\nplt.axvline(mean_value, color=\"#F9DD73\", linestyle=\"solid\", linewidth=1)\n\n# Annotate median and mean\nplt.text(median_value, plt.gca().get_ylim()[1]*0.95, f\"Median = {median_value:.1f}\",\n         color=\"black\", ha=\"center\", va=\"top\", fontsize=10)\nplt.text(mean_value, plt.gca().get_ylim()[1]*0.85, f\"Mean = {mean_value:.1f}\",\n         color=\"#F9DD73\", ha=\"center\", va=\"top\", fontsize=10)\n\n# Customize labels and theme\nplt.title(\"Attainment 8 - All Schools England and Wales, 2022/23 Academic Year\")\nplt.xlabel(\"Attainment 8 Score\")\nplt.ylabel(\"Density\")\nsns.despine()\nplt.tight_layout()\n\n# Show the plot\n# plt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_value <- median(england_school_2022_23$ATT8SCR, na.rm = TRUE)\nmean_value   <- mean(england_school_2022_23$ATT8SCR, na.rm = TRUE)\nsd_value   <- sd(england_school_2022_23$ATT8SCR, na.rm = TRUE)\n\nggplot(england_school_2022_23, aes(x = ATT8SCR)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = \"#4E3C56\", alpha = 0.4) +\n  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value), color = \"#2E6260\", linewidth = 1) +\n  geom_vline(xintercept = median_value, color = \"black\", linetype = \"dotted\", size = 1) +\n  geom_vline(xintercept = mean_value, color = \"#F9DD73\", linetype = \"solid\", size = 1) +\n  annotate(\"text\",\n           x = median_value, y = Inf,\n           label = paste0(\"Median = \", round(median_value, 1)),\n           vjust = 1.3, color = \"black\", size = 3.5) +\n  annotate(\"text\",\n           x = mean_value, y = Inf,\n           label = paste0(\"Mean = \", round(mean_value, 1)),\n           vjust = 30.5, color = \"#F9DD73\", size = 3.5) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +\n  labs(\n    title = \"Attainment 8 - All Schools England and Wales, 2022/23 Academic Year\",\n    x = \"Attainment 8 Score\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n:::\n\n:::\n\n-   You may also recall from the lecture that this odd distribution was related to different types of school in the system. We can create a similar visualisation here:\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up the figure\nplt.figure(figsize=(12, 6))\n\n# Boxplot grouped by MINORGROUP\nsns.boxplot(\n    x='ATT8SCR',\n    data=england_school_2022_23,\n    color=\"#EDD971\",\n    fliersize=0,\n    linewidth=1\n)\n\n# Jittered points\nsns.stripplot(\n    x='ATT8SCR',\n    data=england_school_2022_23,\n    hue='MINORGROUP',\n    dodge=False,\n    jitter=True,\n    alpha=0.5,\n    size=4,\n    palette='turbo'\n)\n\n# Customize the plot\nplt.title(\"Attainment 8 - All Schools 2022/23 Academic Year\")\nplt.xlabel(\"Attainment 8 Score\")\nplt.ylabel(\"\")\nplt.legend(title=\"School Type\", loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=2)\nsns.despine()\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-9-1.png){width=1152}\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(england_school_2022_23, aes(x = ATT8SCR, y = \"\")) +\n  geom_boxplot(fill = \"#EDD971\", alpha = 0.3, outlier.shape = NA) +    \n  geom_jitter(aes(colour = MINORGROUP), height = 0.2, alpha = 0.3, size = 1) + \n  scale_colour_viridis_d(option = \"turbo\") +  # applies the default casaviz discrete palette\n  labs(\n    title = \"Attainment 8 - All Schools 2022/23 Academic Year\",\n    x = \"Attainment 8 Score\",\n    y = NULL,\n    colour = \"School Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.key.size = unit(10, \"mm\"),   # increase dot space\n    legend.text = element_text(size = 10)  # optional: larger legend labels\n  ) + guides(colour = guide_legend(override.aes = list(size = 4)))\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n:::\n\n:::\n\n#### Further filtering and subsetting\n\n-   Having joined all of these data frames into a single master dataframe for the whole of England and observed some issues, we will need to filter out some of the rows we don't want and the columns that we don't need\n-   In the two folders you downloaded from Moodle, one contains all of the data we need, the other contains all of the metadata which tells us what the variables are\n-   If you open up, for example `ks4_meta.xlsx` these are the variables related to attainment for each school.\n-   Similarly `abbreviations.xlsx` attaches some meaning to some of the codes associated with some of these variables, with other metadata files containing other info. Take some time to look at these so you understand the range of variables we'll be exploring.\n\n::: callout-note\nOne of the key filtering asks carried out in the code below is to filter the schools so that we only retain the maintained schools and the academies in the dataset\n:::\n\n## Task 2 (continued) - Filtering, Selecting and Subsetting\n\nThe Code Below already has the syntax to allow you to filter your dataset and drop special schools, independent schools and colleges.\n\nIt also contains the start of some code to allow you to select a smaller set of variables from the 700-odd in the main file.\n\n1.  Adapt this Python or R code so that it contains these variables plus the additional variables listed below (see the note about how you may need to adjust the names depending on whether using Python or R). - You should end up with 34 in total\n2.  Once you have created this filtered and reduced file called `england_filtered`, write it out as a csv (e.g. england_filtered.csv) and save to your local working directory - we will use it again in future practicals\n3.  Now you have your `england_filtered` file, use this code below as the template for creating two new data subsets:\n    a.  london_borough_sub\n    b.  england_lad_sub These two new files should create a subset which has filtered on the name or code of a London borough and another local authority in England\n    \n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n# Assuming england_filtered is a pandas DataFrame\nengland_filtered.to_csv(\"england_filtered.csv\", index=False)\n```\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nwrite_csv(england_filtered, \"england_filtered.csv\")\n```\n:::\n\n\n:::\n\n::: callout-note\nIf you are unsure of the names of the local authorities in England, use the `la_and_region_codes_meta.csv` file in your metadata folder to help you. You might want to name your \"sub\" files according to the names of the local authorities you choose.\n:::\n\n\n\n::: qna\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\n## check all of your variable names first as R and python will call them something different\ncolumn_df = pd.DataFrame({'Column Names': england_school_2022_23.columns.tolist()})\n\n# Define excluded MINORGROUP values\nexcluded_groups = [\"Special school\", \"Independent school\", \"College\", None]\n\n# Filter and select columns\nengland_filtered = (\n    england_school_2022_23[\n        ~england_school_2022_23[\"MINORGROUP\"].isin(excluded_groups) &\n        (england_school_2022_23[\"phaseofeducation_name_\"] == \"Secondary\") &\n        (england_school_2022_23[\"establishmentstatus_name_\"] == \"Open\")\n    ][[\"URN\", \"SCHNAME_x\", \"LEA\", \"TOWN_x\", \"TOTPUPS\", \"ATT8SCR\", \"OFSTEDRATING\", \"MINORGROUP\", \"PTFSM6CLA1A\"]]\n)\n```\n:::\n\n\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter data\nengland_filtered <- england_school_2022_23 %>%\n  filter(!MINORGROUP %in% c(\"Special school\", \"Independent school\", \"College\", NA)) %>%   filter(phase_of_education_name == \"Secondary\") %>% \n  filter(establishment_status_name == \"Open\") %>% \n  select(URN, SCHNAME.x, TOWN.x, TOTPUPS, ATT8SCR, OFSTEDRATING, MINORGROUP, PTFSM6CLA1A)\n```\n:::\n\n\n\n:::\n\n::: {.callout-important title=\"Other variables to include in your reduced dataset\"}\nOther variables to include in your reduced dataset:\n\n1.  Local Authority (LEA) Code and Name - **LEA** and **LANAME**\n2.  Total Number of Pupils on Roll (all ages) - **TOTPUPS**\n3.  Average Attainment 8 score per pupil for English element - **ATT8SCRENG**\n4.  Average Attainment 8 score per pupil for mathematics element - **ATT8SCRMAT**\n5.  Average Attainment 8 score per disadvantaged pupil - **ATT8SCR_FSM6CLA1A**\n6.  Average Attainment 8 score per non-disadvantaged pupil - **ATT8SCR_NFSM6CLA1A**\n7.  Average Attainment 8 score per boy - **ATT8SCR_BOYS**\n8.  Average Attainment 8 score per girl - **ATT8SCR_GIRLS**\n9.  Progress 8 measure after adjustment for extreme scores - **P8MEA**\n10. Adjusted Progress 8 measure - disadvantaged pupils - **P8MEA_FSM6CLA1A**\n11. Adjusted Progress 8 measure - non-disadvantaged pupils - **P8MEA_NFSM6CLA1A**\n12. \\% of pupils at the end of key stage 4 who are disadvantaged - **PTFSM6CLA1A**\n13. \\% of pupils at the end of key stage 4 who are not disadvantaged - **PTNOTFSM6CLA1A**\n14. \\% pupils where English not first language - **PNUMEAL**\n15. \\% pupils with English first language - **PNUMENGFL**\n16. \\% of pupils at the end of key stage 4 with low prior attainment at the end of key stage 2 - **PTPRIORLO**\n17. \\% of pupils at the end of key stage 4 with high prior attainment at the end of key stage 2 - **PTPRIORHI**\n18. Total boys on roll (including part-time pupils) - **NORB**\n19. Total girls on roll (including part-time pupils) - **NORG**\n20. Percentage of pupils eligible for FSM at any time during the past 6 years - **PNUMFSMEVER**\n21. Percentage of overall absence - **PERCTOT**\n22. Percentage of enrolments who are persistent absentees - **PPERSABS10**\n23. School Type e.g. Voluntary Aided school - **SCHOOLTYPE**\n24. Religious character - **RELCHAR**\n25. Admissions Policy - **ADMPOL**\n26. Admissions Policy new definition from 2019 - **ADMPOL_PT**\n27. Government Office Region Name - **gor_name**\n28. Mixed or Single Sex - **gender_name**\n:::\n\n::: callout-note\nWhen you joined your datasets earlier, you will find that some variables appear in more than one of the datasets. This may cause a problem now as R and Python will try and distinguish them in the join by appending a `.x` or `.y` (R) or `_x` or `_y` (Python) so you may get an error when joining, particularly with the **SCHOOLTYPE** and **ADMPOL** variables. For these try **SCHOOLTYPE.x** and **ADMPOL.y** (underscore for Python) instead.\n:::\n\n## Task 3 - Further Exploratory Analysis and Visual Modelling\n\n-   Using the example code below as a guide, select your own $Y$ variable related to attainment from your smaller filtered dataset. You have a range of attainment or progress related variables in there\n-   Select your own $X$ variable that might help explain attainment, this might relate to disadvantage, prior attainment, absence or anything else that is a **continuous** variable (DO NOT SELECT A CATEGORICAL VARIABLE AT THIS TIME).\n-   Once you have selected your two variables, plot them against each other on a scatter plot to see if there is an obvious relationship - again, use the code below as a guide.\n\n::: {.callout-tip title=\"AI tip\"}\nAI - yes, that dreaded thing that's invading everything. Well, it's hear to stay and while it's here, you might as well use it to help you with your coding. Of course, I don't want you to blindly copy things and not understand what's going on, but if you get stuck with syntax and getting plots to look how you want - feed your favourite AI with your code and get it to help you out.\n:::\n\n#### Exploratory Data Analysis Plots\n\n\n\n\n\n::: {.callout-tip title=\"Questions to Answer\"}\nJust from looking at your plots: - How do you expect the intercepts (baseline Y) to differ between your London and non-London local authority (if at all)? Which is higher and which is lower? - How do you think the slope values (influence of X on Y) are different (if at all)?\n:::\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Combine the datasets\ncombined_data = pd.concat([camden_sub, leeds_sub])\n\n# Create a function to make and customize the plot\ndef make_plot():\n    g = sns.FacetGrid(combined_data, col=\"LANAME\", height=5, aspect=1)\n    g.map_dataframe(\n        sns.scatterplot,\n        x=\"PTFSM6CLA1A\",\n        y=\"ATT8SCR\",\n        color=\"steelblue\",\n        alpha=0.7,\n        s=60\n    )\n    g.set_axis_labels(\"% KS4 Pupils Disadvantaged (PTFSM6CLA1A)\", \"Attainment 8 Score (ATT8SCR)\")\n    g.fig.suptitle(\"Attainment 8 vs % Disadvantaged by Local Authority\", fontsize=16)\n    g.fig.tight_layout()\n    g.fig.subplots_adjust(top=0.85)\n    plt.show()\n\n# Call the function once\nmake_plot()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Combine the datasets and label them\ncombined_data <- bind_rows(\n  camden_sub %>% mutate(Area = \"Camden\"),\n  leeds_sub %>% mutate(Area = \"Leeds\")\n)\n\n# Create faceted scatter plot\nggplot(combined_data, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +\n  geom_point(alpha = 0.7, size = 3, color = \"steelblue\") +\n  facet_wrap(~ Area) +\n  labs(\n    title = \"Attainment 8 vs % Disadvantaged by Local Authority\",\n    x = \"% KS4 Pupils Disadvantaged (PTFSM6CLA1A)\",\n    y = \"Attainment 8 Score (ATT8SCR)\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-21-3.png){width=672}\n:::\n:::\n\n:::\n\n#### Exploratory Data Analysis Maps\n\n-   Maps are great, they help you understand all kinds of things if your data have a spatial identifier. Try some maps out here!\n-   You might find R is a bit better at mapping things. Just saying.\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\nimport contextily as ctx\n\n# Reproject to Web Mercator for contextily\nsf_df_mercator = sf_df_bng.to_crs(epsg=3857)\n\n# Scale marker size\nmin_val = sf_df_mercator['TOTPUPS'].min()\nmax_val = sf_df_mercator['TOTPUPS'].max()\nsf_df_mercator['size_scale'] = 4 + (sf_df_mercator['TOTPUPS'] - min_val) * (12 - 4) / (max_val - min_val)\n\n# Color mapping\nnorm = Normalize(vmin=sf_df_mercator['ATT8SCR'].min(), vmax=sf_df_mercator['ATT8SCR'].max())\ncmap = plt.colormaps['magma']\nsf_df_mercator['color'] = sf_df_mercator['ATT8SCR'].apply(lambda x: cmap(norm(x)))\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 10))\nsf_df_mercator.plot(ax=ax, color=sf_df_mercator['color'], markersize=sf_df_mercator['size_scale']**2, alpha=0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Axes: >\n```\n\n\n:::\n\n```{.python .cell-code}\n# Add basemap\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n# Final touches\nax.set_title(\"Static Map of Schools Colored by ATT8SCR and Sized by TOTPUPS\", fontsize=14)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nText(0.5, 1.0, 'Static Map of Schools Colored by ATT8SCR and Sized by TOTPUPS')\n```\n\n\n:::\n\n```{.python .cell-code}\nax.set_axis_off()\n\n# Colorbar\nsm = cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\ncbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.03, pad=0.01)\ncbar.set_label('ATT8SCR', fontsize=12)\n\nplt.tight_layout()\nplt.savefig(\"static_school_map_with_basemap.png\", dpi=300)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(viridis)\n\n# Convert to sf object with EPSG:27700\nsf_df <- st_as_sf(leeds_sub, coords = c(\"easting\", \"northing\"), crs = 27700)\n\n# Transform to EPSG:4326\nsf_df <- st_transform(sf_df, crs = 4326)\n\n# Extract lat/lon for leaflet\nsf_df <- sf_df %>%\n  mutate(\n    lon = st_coordinates(.)[,1],\n    lat = st_coordinates(.)[,2]\n  )\n\n\n# Define size and color scales\nsize_scale <- rescale(sf_df$TOTPUPS, to = c(1, 15))  # radius from 4 to 12\n\n# Create a color palette based on ATT8SCR\npal <- colorNumeric(palette = \"magma\", domain = sf_df$ATT8SCR)\n\nleaflet(sf_df) %>%\n  addProviderTiles(\"CartoDB.Positron\") %>%\n  addCircleMarkers(\n    lng = ~lon,\n    lat = ~lat,\n    radius = size_scale,\n    stroke = FALSE,\n    fillOpacity = 0.8,\n    color = ~pal(ATT8SCR),\n    popup = ~paste0(\n      \"<strong>\", SCHNAME.x, \"</strong><br>\",\n      \"Pupils: \", TOTPUPS\n    )\n  ) %>%\n  addLegend(\n    \"bottomright\",\n    pal = pal,\n    values = ~ATT8SCR,\n    title = \"ATT8SCR\",\n    opacity = 0.8\n  )\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"leaflet html-widget html-fill-item\" id=\"htmlwidget-e71e32d7510f2193b081\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-e71e32d7510f2193b081\">{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addProviderTiles\",\"args\":[\"CartoDB.Positron\",null,null,{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]},{\"method\":\"addCircleMarkers\",\"args\":[[53.83394634055382,53.8409398309255,53.85034857386938,53.85712277299098,53.82017744375354,53.90039841956449,53.75656004662164,53.74558690458779,53.8387499079027,53.76566481366035,53.74785512138476,53.76006383630605,53.80288115128293,53.81070125174928,53.79721268666177,53.79235390646338,53.79514273396458,53.78344398433403,53.78683288703903,53.8713904925365,53.84229861871142,53.81544011630753,53.83589750626852,53.81758602453261,53.81551080341187,53.85370943046657,53.81414331569527,53.74218241498904,53.79386270979968,53.91268570640319,53.73733196122085,53.7988428504004,53.85674672875194,53.73060678993431,53.79097463962646,53.88125617657392,53.79654340248619,53.80000595654006,53.78823862185425,53.92960845400618,53.72987776155619],[-1.608423239084215,-1.524073025507307,-1.551662338143588,-1.68346318744024,-1.476550927178206,-1.36196893706364,-1.382153401796083,-1.620192354862328,-1.560250705548742,-1.537818960574212,-1.532085267275583,-1.558933938220971,-1.520950890789299,-1.687351169609469,-1.484791994789933,-1.656702836658199,-1.607676318817181,-1.619987392565149,-1.391088902611243,-1.717483226940705,-1.640838474112415,-1.434796568204613,-1.595108904930247,-1.558573543807086,-1.461906770359417,-1.544329442771106,-1.654444227262337,-1.606865522760626,-1.52688289356914,-1.693852141772329,-1.459842998496006,-1.662904935937967,-1.607800074111825,-1.500854894689133,-1.53533993921077,-1.727867413822788,-1.461329674570658,-1.514485461178211,-1.534398738002605,-1.380713206258969,-1.585532609441064],[13.08621667612025,13.49914917753829,12.61769710720363,12.17300056721497,5.820192853091322,10.72773681225184,11.07714123652864,10.02098695405559,7.193987521270562,9.504821327283041,3.231423709585933,10.02098695405559,6.749290981281906,8.575723199092455,6.923993193420307,8.790130459444129,6.94781622234827,11.74418604651163,15,10.9977311401021,12.32387975042541,4.978445830969938,11.53771979580261,7.233692569483835,8.115144639818491,1,11.0850822461713,11.2597844583097,7.40045377197958,12.95121951219512,7.932501418037436,10.53715258082813,8.186613726602381,12.99092456040839,10.24333522404991,9.663641520136132,10.30686330119115,3.699943278502552,3.287010777084515,5.502552467385138,14.57912648893931],null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":false,\"color\":[\"#FED597\",\"#C13B75\",\"#FCECAD\",\"#D5446D\",\"#4A1079\",\"#FEC98C\",\"#D8456C\",\"#FA7D5E\",\"#FECF92\",\"#CD4071\",\"#808080\",\"#F1605D\",\"#000004\",\"#E04C67\",\"#782281\",\"#BC3978\",\"#140E36\",\"#FB8560\",\"#FC8961\",\"#FE9D6C\",\"#FCF4B6\",\"#491078\",\"#B1357B\",\"#53137D\",\"#681C81\",\"#A9337D\",\"#7F2482\",\"#FCFDBF\",\"#F66D5C\",\"#FEC287\",\"#FC8D63\",\"#E44F64\",\"#B5367A\",\"#DA476A\",\"#F56B5C\",\"#FCEDAF\",\"#EA5661\",\"#808080\",\"#AF357B\",\"#F4695C\",\"#F4695C\"],\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":[\"#FED597\",\"#C13B75\",\"#FCECAD\",\"#D5446D\",\"#4A1079\",\"#FEC98C\",\"#D8456C\",\"#FA7D5E\",\"#FECF92\",\"#CD4071\",\"#808080\",\"#F1605D\",\"#000004\",\"#E04C67\",\"#782281\",\"#BC3978\",\"#140E36\",\"#FB8560\",\"#FC8961\",\"#FE9D6C\",\"#FCF4B6\",\"#491078\",\"#B1357B\",\"#53137D\",\"#681C81\",\"#A9337D\",\"#7F2482\",\"#FCFDBF\",\"#F66D5C\",\"#FEC287\",\"#FC8D63\",\"#E44F64\",\"#B5367A\",\"#DA476A\",\"#F56B5C\",\"#FCEDAF\",\"#EA5661\",\"#808080\",\"#AF357B\",\"#F4695C\",\"#F4695C\"],\"fillOpacity\":0.8},null,null,[\"<strong>Abbey Grange Church of England Academy<\\/strong><br>Pupils: 1659\",\"<strong>Allerton Grange School<\\/strong><br>Pupils: 1711\",\"<strong>Allerton High School<\\/strong><br>Pupils: 1600\",\"<strong>Benton Park School<\\/strong><br>Pupils: 1544\",\"<strong>Bishop Young Church of England Academy<\\/strong><br>Pupils: 744\",\"<strong>Boston Spa Academy<\\/strong><br>Pupils: 1362\",\"<strong>Brigshaw High School<\\/strong><br>Pupils: 1406\",\"<strong>Bruntcliffe Academy<\\/strong><br>Pupils: 1273\",\"<strong>Cardinal Heenan Catholic High School<\\/strong><br>Pupils: 917\",\"<strong>Cockburn John Charles Academy<\\/strong><br>Pupils: 1208\",\"<strong>Cockburn Laurence Calvert Academy<\\/strong><br>Pupils: 418\",\"<strong>Cockburn School<\\/strong><br>Pupils: 1273\",\"<strong>Co-op Academy Leeds<\\/strong><br>Pupils: 861\",\"<strong>Co-op Academy Priesthorpe<\\/strong><br>Pupils: 1091\",\"<strong>Corpus Christi Catholic College, A Voluntary Academy<\\/strong><br>Pupils: 883\",\"<strong>Crawshaw Academy<\\/strong><br>Pupils: 1118\",\"<strong>Dixons Unity Academy<\\/strong><br>Pupils: 886\",\"<strong>The Farnley Academy<\\/strong><br>Pupils: 1490\",\"<strong>Garforth Academy<\\/strong><br>Pupils: 1900\",\"<strong>Guiseley School<\\/strong><br>Pupils: 1396\",\"<strong>Horsforth School<\\/strong><br>Pupils: 1563\",\"<strong>John Smeaton Academy<\\/strong><br>Pupils: 638\",\"<strong>Lawnswood School<\\/strong><br>Pupils: 1464\",\"<strong>Leeds City Academy<\\/strong><br>Pupils: 922\",\"<strong>Leeds East Academy<\\/strong><br>Pupils: 1033\",\"<strong>Leeds Jewish Free School<\\/strong><br>Pupils: 137\",\"<strong>Leeds West Academy<\\/strong><br>Pupils: 1407\",\"<strong>The Morley Academy<\\/strong><br>Pupils: 1429\",\"<strong>Mount St Mary's Catholic High School<\\/strong><br>Pupils: 943\",\"<strong>Otley Prince Henry's Grammar School Specialist Language College<\\/strong><br>Pupils: 1642\",\"<strong>Oulton Academy<\\/strong><br>Pupils: 1010\",\"<strong>Pudsey Grammar School<\\/strong><br>Pupils: 1338\",\"<strong>Ralph Thoresby School<\\/strong><br>Pupils: 1042\",\"<strong>Rodillian Academy<\\/strong><br>Pupils: 1647\",\"<strong>The Ruth Gorse Academy<\\/strong><br>Pupils: 1301\",\"<strong>St. Mary's Menston, a Catholic Voluntary Academy<\\/strong><br>Pupils: 1228\",\"<strong>Temple Moor High School<\\/strong><br>Pupils: 1309\",\"<strong>Trinity Academy Leeds<\\/strong><br>Pupils: 477\",\"<strong>University Technical College Leeds<\\/strong><br>Pupils: 425\",\"<strong>Wetherby High School<\\/strong><br>Pupils: 704\",\"<strong>Woodkirk Academy<\\/strong><br>Pupils: 1847\"],null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addLegend\",\"args\":[{\"colors\":[\"#000004 , #0C0925 6.93069306930693%, #4A1079 23.4323432343234%, #8C2981 39.9339933993399%, #D0416F 56.4356435643564%, #FA7D5E 72.9372937293729%, #FECC8F 89.4389438943894%, #FCFDBF \"],\"labels\":[\"30\",\"35\",\"40\",\"45\",\"50\",\"55\"],\"na_color\":\"#808080\",\"na_label\":\"NA\",\"opacity\":0.8,\"position\":\"bottomright\",\"type\":\"numeric\",\"title\":\"ATT8SCR\",\"extra\":{\"p_1\":0.06930693069306934,\"p_n\":0.8943894389438943},\"layerId\":null,\"className\":\"info legend\",\"group\":null}]}],\"limits\":{\"lat\":[53.72987776155619,53.92960845400618],\"lng\":[-1.727867413822788,-1.36196893706364]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n:::\n\n#### Exploratory Data Analysis - Distributions and slicing and dicing\n\n-   You can also explore the scatter plot for your whole of England dataset.\n-   One really interesting thing to do is to slice and dice your data according to some of the categorical variables.\n-   In the R visualisation, you can see how we can start to slide and dice by regions, for example. Maybe try some of your other categorical variables.\n\n::: callout-note\nYou will notice that in the plots below I have used a log10() transformation, but you could also use the natural log() or a range of other possible transformations. If you want to understand a bit more about transformations for normalisng your data, you should explore Tukey's Ladder of Powers - <https://onlinestatbook.com/2/transformations/tukey.html> - in R (and probably in Python too) there are packages to apply appropriate Tukey transformations so your data gets a little closer to a normal distribution.\n:::\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Load your data into a DataFrame named england_filtered\n# england_filtered = pd.read_csv(\"your_data.csv\")\n\n# Apply log10 transformation (replace 0s to avoid log(0))\nengland_filtered['log_PTFSM6CLA1A'] = np.log10(england_filtered['PTFSM6CLA1A'].replace(0, np.nan))\nengland_filtered['log_ATT8SCR'] = np.log10(england_filtered['ATT8SCR'].replace(0, np.nan))\n\n# Drop rows with NaNs from log(0)\nengland_filtered = england_filtered.dropna(subset=['log_PTFSM6CLA1A', 'log_ATT8SCR'])\n\n# Set theme\nsns.set_theme(style=\"darkgrid\")\n\n# Create jointplot\ng = sns.jointplot(\n    x=\"log_PTFSM6CLA1A\",\n    y=\"log_ATT8SCR\",\n    data=england_filtered,\n    kind=\"reg\",\n    truncate=False,\n    color=\"m\",\n    height=7\n)\n\n# Label axes\ng.set_axis_labels(\"log10(PTFSM6CLA1A)\", \"log10(ATT8SCR)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<seaborn.axisgrid.JointGrid object at 0x000001EF27B79040>\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggside)\nlibrary(ggplot2)\n\nggplot(england_filtered, aes(x = log10(PTFSM6CLA1A), y = log10(ATT8SCR), colour = gor_name)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_xsidedensity(aes(y = after_stat(density)), alpha = 0.5, size = 1, position = \"stack\") +\n  geom_ysidedensity(aes(x = after_stat(density)), alpha = 0.5, size = 1, position = \"stack\") + \n  theme(ggside.panel.scale.x = 0.4,ggside.panel.scale.y = 0.4)\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-26-3.png){width=672}\n:::\n:::\n\n:::\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the theme\nsns.set_theme(style=\"darkgrid\")\n\n# Create side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Histogram for PTFSM6CLA1A\nsns.histplot(data=england_filtered, x=\"PTFSM6CLA1A\", ax=axes[0], kde=True, color=\"skyblue\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Axes: xlabel='PTFSM6CLA1A', ylabel='Count'>\n```\n\n\n:::\n\n```{.python .cell-code}\naxes[0].set_title(\"Histogram of PTFSM6CLA1A\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nText(0.5, 1.0, 'Histogram of PTFSM6CLA1A')\n```\n\n\n:::\n\n```{.python .cell-code}\naxes[0].set_xlabel(\"PTFSM6CLA1A\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nText(0.5, 0, 'PTFSM6CLA1A')\n```\n\n\n:::\n\n```{.python .cell-code}\naxes[0].set_ylabel(\"Frequency\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nText(0, 0.5, 'Frequency')\n```\n\n\n:::\n\n```{.python .cell-code}\n# Histogram for ATT8SCR\nsns.histplot(data=england_filtered, x=\"ATT8SCR\", ax=axes[1], kde=True, color=\"salmon\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Axes: xlabel='ATT8SCR', ylabel='Count'>\n```\n\n\n:::\n\n```{.python .cell-code}\naxes[1].set_title(\"Histogram of ATT8SCR\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nText(0.5, 1.0, 'Histogram of ATT8SCR')\n```\n\n\n:::\n\n```{.python .cell-code}\naxes[1].set_xlabel(\"ATT8SCR\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nText(0.5, 0, 'ATT8SCR')\n```\n\n\n:::\n\n```{.python .cell-code}\naxes[1].set_ylabel(\"Frequency\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nText(0, 0.5, 'Frequency')\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-27-1.png){width=1344}\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Histogram for PTFSM6CLA1A\nggplot(england_filtered, aes(x = PTFSM6CLA1A)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"white\", alpha = 0.5) +\n  geom_density(aes(y = ..count..), color = \"skyblue\", size = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of PTFSM6CLA1A\",\n    x = \"PTFSM6CLA1A\",\n    y = \"Frequency\"\n  )\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-28-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Histogram for ATT8SCR\nggplot(england_filtered, aes(x = ATT8SCR)) +\n  geom_histogram(binwidth = 1, fill = \"salmon\", color = \"white\", alpha = 0.5) +\n  geom_density(aes(y = ..count..), color = \"salmon\", size = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Histogram of ATT8SCR\",\n    x = \"ATT8SCR\",\n    y = \"Frequency\"\n  )\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-28-4.png){width=672}\n:::\n:::\n\n:::\n\n::: {.callout-tip title=\"Explore Different Visualisations\"}\nI've just given you a few very basic visualisation types here, but there are so many extensions and options available in packages like [ggplot2](https://exts.ggplot2.tidyverse.org/gallery/) in R and [Seaborn](https://seaborn.pydata.org/examples/index.html) in Python so you can get visualising in very creative ways - have a look at some of these gallery examples in the links and maybe try experimenting with a few different ones! There are no excuses these days for ðŸ’© data visualisations!\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(england_filtered, aes(x = PTFSM6CLA1A, y = ATT8SCR, fill = gor_name)) +\n  geom_violin(\n    position = position_dodge(width = 0.8),\n    alpha = 0.6,\n    draw_quantiles = c(0.25, 0.5, 0.75)\n  ) +\n  facet_wrap(~ gor_name) +\n  theme_minimal() +\n  labs(\n    title = \"Violin Plot of Attainment 8 Score with Quantiles by Region\",\n    x = \"% Disadvantaged\",\n    y = \"Attainment 8 Score\",\n    fill = \"Region\"\n  )\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggdist)\n\nggplot(england_filtered, aes(x = PTFSM6CLA1A, y = gor_name, fill = gor_name)) + \n  stat_slab(aes(thickness = after_stat(pdf*n)), scale = 0.7) +\n  stat_dotsinterval(side = \"bottom\", scale = 1, slab_linewidth = NA) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_ggdist() +\n  labs(\n    title = \"Rain Cloud Plot of % Disadvantage in Schools by Region\",\n    x = \"% Disadvantaged\",\n    y = \"Region\",\n    fill = \"Region\"\n  )\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n## Task 4 - Explanatory Analysis - Running Your Regression Model\n\n-   Now you have carried out some appropriate exploratory data analysis, you should run a simple bi-variate regression model for your two local authority areas and compare it with a similar analysis run on the national dataset\n\n::: callout-tip\n-   Depending on your variables and whether or not there appears to be a linear relationship or a log-log or level-log relationship, if you transform a variable for the national data or one of your subsets, you could also transform it for the others, otherwise it will be impossible to compare\n:::\n\n-   The Code below is an example using my variables, but you should use the variables you have selected.\n\n#### Running the Model for the first (London) local authority\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\n\n# Log-transform the variables\ncamden_sub['log_ATT8SCR'] = np.log(camden_sub['ATT8SCR'])\ncamden_sub['log_PTFSM6CLA1A'] = np.log(camden_sub['PTFSM6CLA1A'])\n\n# Define independent and dependent variables\nX = sm.add_constant(camden_sub['log_PTFSM6CLA1A'])  # adds intercept\ny = camden_sub['log_ATT8SCR']\n\n# Fit the model\ncamden_model1 = sm.OLS(y, X).fit()\n#camden_summary = extract_model_summary(camden_model1, 'Camden Model')\n\n# Print summary\nprint(camden_model1.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nC:\\Users\\Adam\\ONEDRI~1\\DOCUME~1\\VIRTUA~1\\R-RETI~1\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.576\nModel:                            OLS   Adj. R-squared:                  0.523\nMethod:                 Least Squares   F-statistic:                     10.88\nDate:                Wed, 22 Oct 2025   Prob (F-statistic):             0.0109\nTime:                        15:35:15   Log-Likelihood:                 9.4809\nNo. Observations:                  10   AIC:                            -14.96\nDf Residuals:                       8   BIC:                            -14.36\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P>|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               6.1206      0.678      9.023      0.000       4.556       7.685\nlog_PTFSM6CLA1A    -0.5784      0.175     -3.298      0.011      -0.983      -0.174\n==============================================================================\nOmnibus:                        2.491   Durbin-Watson:                   2.528\nProb(Omnibus):                  0.288   Jarque-Bera (JB):                0.897\nSkew:                           0.002   Prob(JB):                        0.639\nKurtosis:                       1.533   Cond. No.                         84.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear model and get predicted values\ncamden_model1 <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = camden_sub)\nsummary(camden_model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A), data = camden_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.12111 -0.10124  0.01187  0.06663  0.13871 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        6.1206     0.6783   9.023 1.82e-05 ***\nlog(PTFSM6CLA1A)  -0.5784     0.1753  -3.298   0.0109 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1048 on 8 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5233 \nF-statistic: 10.88 on 1 and 8 DF,  p-value: 0.01088\n```\n\n\n:::\n:::\n\n:::\n\n#### Running the Model for the second local authority\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\n\n\n# Log-transform safely: replace non-positive values with NaN\nleeds_sub['log_ATT8SCR'] = np.where(leeds_sub['ATT8SCR'] > 0, np.log(leeds_sub['ATT8SCR']), np.nan)\nleeds_sub['log_PTFSM6CLA1A'] = np.where(leeds_sub['PTFSM6CLA1A'] > 0, np.log(leeds_sub['PTFSM6CLA1A']), np.nan)\n\n# Drop rows with NaNs in either column\nleeds_clean = leeds_sub.dropna(subset=['log_ATT8SCR', 'log_PTFSM6CLA1A'])\n\n# Define independent and dependent variables\nX = sm.add_constant(leeds_clean['log_PTFSM6CLA1A'])  # adds intercept\ny = leeds_clean['log_ATT8SCR']\n\n# Fit the model\nleeds_model1 = sm.OLS(y, X).fit()\n#leeds_summary = extract_model_summary(leeds_model1, 'Leeds Model')\n\n# Print summary\nprint(leeds_model1.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.491\nModel:                            OLS   Adj. R-squared:                  0.477\nMethod:                 Least Squares   F-statistic:                     35.71\nDate:                Wed, 22 Oct 2025   Prob (F-statistic):           6.77e-07\nTime:                        15:35:15   Log-Likelihood:                 26.542\nNo. Observations:                  39   AIC:                            -49.08\nDf Residuals:                      37   BIC:                            -45.76\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P>|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               4.5473      0.123     36.940      0.000       4.298       4.797\nlog_PTFSM6CLA1A    -0.2201      0.037     -5.976      0.000      -0.295      -0.145\n==============================================================================\nOmnibus:                        0.518   Durbin-Watson:                   2.333\nProb(Omnibus):                  0.772   Jarque-Bera (JB):                0.654\nSkew:                          -0.200   Prob(JB):                        0.721\nKurtosis:                       2.508   Cond. No.                         22.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear model and get predicted values\nleeds_model1 <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = leeds_sub)\nsummary(leeds_model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A), data = leeds_sub)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.310438 -0.085062 -0.003004  0.116129  0.205687 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       4.54727    0.12310  36.940  < 2e-16 ***\nlog(PTFSM6CLA1A) -0.22006    0.03682  -5.976 6.77e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1258 on 37 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4911,\tAdjusted R-squared:  0.4774 \nF-statistic: 35.71 on 1 and 37 DF,  p-value: 6.767e-07\n```\n\n\n:::\n:::\n\n:::\n\n#### Running the Model for all Schools in England\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\n\n\n# Log-transform safely: replace non-positive values with NaN\nengland_filtered['log_ATT8SCR'] = np.where(england_filtered['ATT8SCR'] > 0, np.log(england_filtered['ATT8SCR']), np.nan)\nengland_filtered['log_PTFSM6CLA1A'] = np.where(england_filtered['PTFSM6CLA1A'] > 0, np.log(england_filtered['PTFSM6CLA1A']), np.nan)\n\n# Drop rows with NaNs in either column\nengland_clean = england_filtered.dropna(subset=['log_ATT8SCR', 'log_PTFSM6CLA1A'])\n\n# Define independent and dependent variables\nX = sm.add_constant(england_clean['log_PTFSM6CLA1A'])  # adds intercept\ny = england_clean['log_ATT8SCR']\n\n# Fit the model\nengland_model1 = sm.OLS(y, X).fit()\n#england_summary = extract_model_summary(england_model1, 'England Model')\n\n# Print summary\nprint(england_model1.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            log_ATT8SCR   R-squared:                       0.468\nModel:                            OLS   Adj. R-squared:                  0.468\nMethod:                 Least Squares   F-statistic:                     2611.\nDate:                Wed, 22 Oct 2025   Prob (F-statistic):               0.00\nTime:                        15:35:15   Log-Likelihood:                 1668.0\nNo. Observations:                2968   AIC:                            -3332.\nDf Residuals:                    2966   BIC:                            -3320.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P>|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               4.4778      0.013    349.542      0.000       4.453       4.503\nlog_PTFSM6CLA1A    -0.2071      0.004    -51.095      0.000      -0.215      -0.199\n==============================================================================\nOmnibus:                      105.763   Durbin-Watson:                   1.313\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              199.209\nSkew:                           0.267   Prob(JB):                     5.53e-44\nKurtosis:                       4.151   Cond. No.                         17.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear model and get predicted values\nengland_filtered_clean <- england_filtered[\n  !is.na(england_filtered$ATT8SCR) & \n  !is.na(england_filtered$PTFSM6CLA1A) &\n  england_filtered$ATT8SCR > 0 &\n  england_filtered$PTFSM6CLA1A > 0, \n]\n\nengland_model1 <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered_clean)\nsummary(england_model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65843 -0.08607 -0.00897  0.07800  0.55670 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       4.477823   0.012811  349.54   <2e-16 ***\nlog(PTFSM6CLA1A) -0.207090   0.004053  -51.09   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.138 on 2966 degrees of freedom\nMultiple R-squared:  0.4681,\tAdjusted R-squared:  0.468 \nF-statistic:  2611 on 1 and 2966 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n:::\n#### Combining your outputs for comparison\n\nDon't forget to run some basic regression modelling checks on all of your models:\n\n:::callout-note\nIf you are doing this in Python, it appears there isn't a nice little package with the code for diagnostic plots already created. However, the statsmodels pages do have some code we can borrow - https://www.statsmodels.org/dev/examples/notebooks/generated/linear_regression_diagnostics_plots.html - run this first before using the functions below\n:::\n\n\n\n\n### Linearity\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncls = LinearRegDiagnostic(england_model1)\n\ncls.residual_plot();\nplt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(performance)\n\ncheck_model(england_model1, \n            check = c(\"linearity\"))\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-39-3.png){width=672}\n:::\n:::\n\n:::\n\n### Homoscedasticity (Constant variance)?\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncls.scale_location_plot();\nplt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(england_model1, \n            check = c(\"homogeneity\"))\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-41-3.png){width=672}\n:::\n:::\n\n\n:::\n\n### Normality of Residuals\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncls.qq_plot()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Axes: title={'center': 'Normal Q-Q'}, xlabel='Theoretical Quantiles', ylabel='Standardized Residuals'>\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(england_model1, \n            check = c(\"qq\"))\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-43-3.png){width=672}\n:::\n:::\n\n:::\n\n### Influence of Outliers\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncls.leverage_plot(high_leverage_threshold=True, cooks_threshold=\"dof\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Axes: title={'center': 'Residuals vs Leverage'}, xlabel='Leverage', ylabel='Standardized Residuals'>\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(england_model1, \n            check = c(\"outliers\"))\n```\n\n::: {.cell-output-display}\n![](week6_practical_files/figure-html/unnamed-chunk-45-3.png){width=672}\n:::\n:::\n\n\n:::\n\n#### Combining your outputs for comparison\n\n::: callout-note\nYou might notice that R is a bit better at things like manipulating model outputs than Python!\n:::\n\n::: {.multicode}\n#### ![](L6_images/python-logo-only.png){width=\"30\"}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col \n\nresults_table = summary_col(\n    results=[camden_model1, leeds_model1, england_model1],\n    model_names=['Camden Model', 'Leeds Model', 'England Model'],\n    stars=True,\n    float_format=\"%0.3f\",\n    # You can customize what model statistics show up here (like R2, N, F-stat)\n    info_dict={'N':lambda x: \"{0:d}\".format(int(x.nobs))}\n)\n\n# \n# # Round for readability\nprint(results_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n======================================================\n                Camden Model Leeds Model England Model\n------------------------------------------------------\nconst           6.121***     4.547***    4.478***     \n                (0.678)      (0.123)     (0.013)      \nlog_PTFSM6CLA1A -0.578**     -0.220***   -0.207***    \n                (0.175)      (0.037)     (0.004)      \nR-squared       0.576        0.491       0.468        \nR-squared Adj.  0.523        0.477       0.468        \nN               10           39          2968         \n======================================================\nStandard errors in parentheses.\n* p<.1, ** p<.05, ***p<.01\n```\n\n\n:::\n:::\n\n\n#### ![](L6_images/Rlogo.png){width=\"41\" height=\"30\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(jtools)\nexport_summs(camden_model1, leeds_model1, england_model1, error_format = \"\", error_pos = \"same\", model.names = c(\"Camden Model\", \"Leeds Model\", \"England Model\"))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"huxtable\" data-quarto-disable-processing=\"true\"  style=\"margin-left: auto; margin-right: auto;\">\n<col><col><col><col><thead>\n<tr>\n<th class=\"huxtable-cell huxtable-header\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 0.8pt 0pt 0pt 0pt;      font-weight: normal;\"></th><th class=\"huxtable-cell huxtable-header\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 0.8pt 0pt 0.4pt 0pt;      font-weight: normal;\">Camden Model</th><th class=\"huxtable-cell huxtable-header\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 0.8pt 0pt 0.4pt 0pt;      font-weight: normal;\">Leeds Model</th><th class=\"huxtable-cell huxtable-header\" style=\"text-align: center;  border-style: solid solid solid solid; border-width: 0.8pt 0pt 0.4pt 0pt;      font-weight: normal;\">England Model</th></tr>\n</thead>\n<tbody>\n<tr>\n<th class=\"huxtable-cell huxtable-header\" style=\"border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;      font-weight: normal;\">(Intercept)</th><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;\">6.12 *** </td><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;\">4.55 *** </td><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;\">4.48 *** </td></tr>\n<tr>\n<th class=\"huxtable-cell huxtable-header\" style=\"border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;      font-weight: normal;\">log(PTFSM6CLA1A)</th><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;\">-0.58 * &nbsp;&nbsp;</td><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;\">-0.22 *** </td><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;\">-0.21 *** </td></tr>\n<tr>\n<th class=\"huxtable-cell huxtable-header\" style=\"border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;      font-weight: normal;\">N</th><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;\">10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;\">39&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;\">2968&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th class=\"huxtable-cell huxtable-header\" style=\"border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;      font-weight: normal;\">R2</th><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;\">0.58&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;\">0.49&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td class=\"huxtable-cell\" style=\"text-align: right;  border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;\">0.47&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th class=\"huxtable-cell huxtable-header\" colspan=\"4\" style=\"border-style: solid solid solid solid; border-width: 0.8pt 0pt 0pt 0pt;      font-weight: normal;\">*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.</th></tr>\n</tbody>\n</table>\n\n```\n\n:::\n:::\n\n:::\n\n::: {.callout-tip title=\"Questions to Answer\"}\nComparing your three regression models:\n\n1\\. How do the intercept (baseline) attainment values differ between your three models? **remember, if you (natural)logged your variables, you will need to take the exponential of the coefficient to interpret it back on the original scale.**\n\n2\\. What do the differences in the slope values tell you?\n\n3\\. Are your intercept and slope values statistically significant? If so, at what level?\n\n4\\. How do the R-squared values differ between your models?\n\n5\\. What can you say about your degrees of freedom and the reliability of your R-squared and other coefficients between your models?\n\n6\\. What is the overall story of the relationship between your attainment variable and your explanatory variable considering both your local examples and comparison with schools across the rest of England?\n:::\n\n### What does my model show?\n\n1.  Baseline attainment (intercept) is much higher in Camden than in Leeds or the whole of England. Exp(6.12) = 455.32. So the Attainment 8 Score in Camden would be 455 (an impossible score) when % of disadvantaged children in a school are 0. This compares to exp(4.547) = 94.35 in Leeds and exp(4.478) = 88.06 in England as a whole. All of these intercepts are statistically significant (i.e. observed values not a chance relationship). While unrealistic (potentially due to the low degrees of freedom), the Camden intercept shows us that after controlling for disadvantage, baseline attainment in Camden is better than in Leeds or the rest of England. \n\n2.  The slope value of -0.207 for England shows that across the country. As this is a log-log model, it is an **Elasticity**: A 1% increase in % of disadvantaged students in a school is associated with a 0.21% decrease in attainment. HOWEVER, as this is a log-log model, the effect is far stronger at one end of the distribution than the other. \n    -   What this means is that a change from 1% to 10% of disadvantaged students in a school there is a 37.9% decrease in attainment, however, change from 21% to 30% leads to just a 7.1% decrease in attainment. \n\n::: {.callout-tip title=\"AI can help here!\"}\nOne of the things that large language models are quite good at is interpreting regression coefficients in the context of real data. \n\nThey don't always get it right, so **USE WITH EXTREME CAUTION AND ALWAYS DOUBLE CHECK WITH YOUR OWN UNDERSTANDING**, however, they *can* help explain what YOUR model coefficients mean in relation to YOUR data in an incredibly helpful way. If you get stuck, try feeding an LLM (ChatGTP, Gemini etc.) with your regression outputs and a description of your data and see what it can tell you\n:::\n\nThis effect is similar for Leeds, but apparently far more severe in Camden where the slope is much steeper. However, Camden doesn't experience very low levels of disadvantage in its schools (unlike in Leeds and the rest of the county), with the lowest level around 35%. With low numbers of schools in the Borough, this observation is potentially unreliable.\n\n3.  The slopes and intercept coefficients in all models are all apparently statistically significant - but as we have seen with Camden, this does not necessarily mean our observations are generalisable! This is a good lesson in the difference between statistical significance (p-values are not a panacea!) and why you should always interrogate your model fully when interpreting outputs. \n\n4. Again, R-squared values might fool you into thinking the Camden model is 'better' than the other models, however, the R-squared is partially an artefact of the low degrees of freedom in the model. \n\n5. See above. \n\n6. Overall, we can make some crucial observations that encourage further research. While the model for England suggests a moderately strong log-log relationship between levels of disadvantage in a school and attainment, looking at this relationship for a London Borough (Camden) - and another Local Authority in England (Leeds) it's the case that this is unlikely to be a universal relationship. \n\nThe baseline of attainment in Camden, controlling for disadvantage, is much higher than in the rest of the country. Students get higher levels of attainment, even when controlling for disadvantage. The log log relationship is crucial as it shows that on average, at low levels of disadvantage small increases have a more severe impact on attainment level, whereas at higher levels, similar changes have much reduced effects. \n\n### Your Turn\n\nTo finish off the practical, see if you can answer the questions above for your example. \n\n",
    "supporting": [
      "week6_practical_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/leaflet-1.3.1/leaflet.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/leaflet-1.3.1/leaflet.js\"></script>\n<link href=\"../site_libs/leafletfix-1.0.0/leafletfix.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/proj4-2.6.2/proj4.min.js\"></script>\n<script src=\"../site_libs/Proj4Leaflet-1.0.1/proj4leaflet.js\"></script>\n<link href=\"../site_libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/leaflet-binding-2.2.3/leaflet.js\"></script>\n<script src=\"../site_libs/leaflet-providers-2.0.0/leaflet-providers_2.0.0.js\"></script>\n<script src=\"../site_libs/leaflet-providers-plugin-2.2.3/leaflet-providers-plugin.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}